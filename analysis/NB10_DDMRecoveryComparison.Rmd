---
title: "Experience vs. description based decision-making project: DDM recovery comparison"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: 'hide'
---

# Setup

Set up environment and load in data

```{r include=FALSE}
library(tidyverse)
library(here)
cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
theme_set(theme_classic())
sem <- function(x) {sd(x, na.rm=T) / sqrt(length(x))}
helpers_path = here('analysis/helpers/')

set.seed(38573)
```

```{r}
source(paste0(helpers_path,'ddmSims/fit_task.R'))
source(paste0(helpers_path,'ddmSims/sim_task.R'))
source(paste0(helpers_path,'ddmSims/fit_ddm_pta.R'))
test_trial_conditions = read.csv(paste0(helpers_path, 'ddmSims/test_data/test_trial_conditions.csv'))
```

Create empty list that will store the trial simulators for the forthcoming models.

```{r}
sim_trial_list = list()
fit_trial_list = list()
```

# Parameter recovery

## Model 1: Simplest

```{r}
source(paste0(helpers_path, 'ddmSims/r_ddm_models/ddm_model1.R'))
```

```{r}
sim_trial_list[['model1']] = sim_trial
fit_trial_list[['model1']] = fit_trial
```

### Simulate data

```{r}
d = .06
sigma = .08
rangeD = c(0.05, 0.06, 0.07)
rangeSigma = c(0.065, 0.08, 0.095)
trialsFileName=NA
trialsPerCondition=250 
```

```{r}
# Replicate same conditions n times
test_data = dplyr::bind_rows(replicate(trialsPerCondition, test_trial_conditions, simplify = FALSE))

# Simulate choice and RT for the replicated trial conditions
test_data = sim_task(test_data, model_name = "model1", d = d, sigma = sigma) %>%drop_na()
```

### Fit using PTA

```{r}
numModels = length(rangeD) * length(rangeSigma)
likelihoods = list()
models = c()
posteriors = list()

# Get likelihoods for all models and all artificial trials.
for (i in 1:length(rangeD)){
  curD = rangeD[i]
  for (j in 1:length(rangeSigma)){
    curSigma = rangeSigma[j]
    model = paste0(as.character(curD), ", ", as.character(curSigma))
    curFit = fit_task(test_data, model_name = "model1", pars_ = list(d=curD, sigma = curSigma))
    likelihoods[[model]] = curFit$likelihood
    models = c(models, model)
    posteriors[[model]] = 1/numModels
  }
}
```

```{r}
# Compute the posteriors.
for(t in 1:nrow(test_data)){
  denominator = 0
  for(m in 1:length(models)){
    model = models[m]
    denominator = denominator + (posteriors[[model]] * likelihoods[[model]][t]) #this indexing might be incorrect
    if(denominator == 0){
      next
    }
  }
  
  for(m in 1:length(models)){
    model = models[m]
    prior = posteriors[[model]]
    posteriors[[model]] = likelihoods[[model]][t] * prior /denominator
  }
}
```

Likelihood surface

```{r}
data.frame(m = models) %>%
  separate(m, c("d", "sigma"), sep= ",") %>%
  mutate(likelihoods = unlist(lapply(likelihoods, sum), use.names = FALSE)) %>%
  ggplot(aes(sigma, d, fill=likelihoods))+
  geom_tile()
```

What do the likelihoods for each trial look like for each model? Of comparable ranges, rising and falling similarly for each trial.

```{r eval=FALSE}
data.frame(likelihoods) %>%
  gather(key, value) %>%
  ggplot(aes(value))+
  geom_histogram(bins=30, alpha=.5, position="identity")+
  facet_grid(key~.)

data.frame(likelihoods) %>%
  gather(key, value) %>%
  group_by(key) %>%
  mutate(trial = 1:n()) %>%
  ggplot(aes(trial, value))+
  geom_line()+
  facet_grid(key~.)+
  xlim(0, 200)
```

Model posteriors

```{r}
data.frame(m = models) %>%
  separate(m, c("d", "sigma"), sep= ",") %>%
  mutate(posteriors = unlist(posteriors, use.names = FALSE)) %>%
  ggplot(aes(sigma, d, fill=posteriors))+
  geom_tile()
```

How do the posteriors change across trials for each parameter combination?

```{r}
posteriors_tbt = list()

for (i in 1:length(models)){
  model = models[i]
  posteriors_tbt[[model]] = c(1/numModels, rep(NA,length(likelihoods[[1]])-1))
}

```

```{r}
# Compute the posteriors.
for(t in 1:nrow(test_data)){
  denominator = 0
  for(m in 1:length(models)){
    model = models[m]
    denominator = denominator + (posteriors_tbt[[model]][t] * likelihoods[[model]][t]) 
    if(denominator == 0){
      next
    }
  }
  
  for(m in 1:length(models)){
    model = models[m]
    prior = posteriors_tbt[[model]][t]
    posteriors_tbt[[model]][t+1] = likelihoods[[model]][t] * prior /denominator
  }
}
```

```{r}
tmp = dplyr::bind_rows(replicate(nrow(test_data)+1, data.frame(m = models), simplify = FALSE)) %>%
  arrange(m) %>%
  mutate(posteriors = unlist(posteriors_tbt, use.names = FALSE)) %>%
  group_by(m) %>%
  mutate(trial = 1:n()) 
```

Posteriors maxes out in relatively few trials for the true model. 

Is that after that many iterations of the same trials or after iterating through all the trials?

```{r}
tmp %>%
  ggplot(aes(trial, posteriors, color=m))+
  geom_line()+
  theme(legend.position = "bottom")+
  xlim(0, 400)
```
Remember in this case there are 21 combinations repeated 250 times. Since fit_task takes in test_data which repeats these 21 trials in a single block the maxing out should be happening after all trials have been fit about 20 times (400 on the plot above). So you should be able to get to correct parameter combination with much fewer trials.

Confirm that you can do this.

#### Recovery with fewer trials

```{r}
trialsPerCondition=25 

# Replicate same conditions n times
test_data = dplyr::bind_rows(replicate(trialsPerCondition, test_trial_conditions, simplify = FALSE))

# Simulate choice and RT for the replicated trial conditions
test_data = sim_task(test_data, model_name = "model1", d = d, sigma = sigma) %>%drop_na()
```

```{r}
out = fit_ddm_pta(data_to_fit_ = test_data, model_name_ = "model1", rangeD_ = rangeD, rangeSigma_ = rangeSigma)
```

```{r}
data.frame(m = out$models) %>%
  separate(m, c("d", "sigma"), sep= ",") %>%
  mutate(posteriors = unlist(out$posteriors, use.names = FALSE)) %>%
  ggplot(aes(sigma, d, fill=posteriors))+
  geom_tile()
```

Ok, this was for the simplest model. Does it work for a more complicated model?

Initially considered trying Model 2b. This is the one with early integration for the probFractalDraw == 1 case and probability distortion for the fractals. This had captured several aspects of the true data in simulations. 

Initial efforts (no longer shown here) failed to recover the true parameters. There might be at least two reasons for this:
1. Grid search happened only for d and sigma and not for the other two parameters (delta and gamma), which were set to 1 both in simulating and fitting the task.  
2. The trial likelihood calculation using the state-space approach depends on the assumption that integration happens until the RT as a function of the value difference. For a model where early integration is allowed it is possible that a decision is reached before the stimuli are presented and the RT is then sampled from some log-normal distribution. In such cases the RT that would be looped until is not generated as a function of the value difference in the model. **So how would the state likelihood estimation should work when early integration is allowed???**  

So first check if a model with distortion but no early integration converges on the correct parameter combination. 

## Model 1a

This model expands the simplest model by adding probability distortion to the fractals but does not have early integration for the probFractalDraw == 1 case.

```{r}
source(paste0(helpers_path, 'ddmSims/r_ddm_models/ddm_model1a.R'))
```

```{r}
sim_trial_list[['model1a']] = sim_trial
fit_trial_list[['model1a']] = fit_trial
```

### Simulate data 

Setting only d and sigma and leaving delta and gamma at 1.

```{r}
trialsPerCondition=50 

# Replicate same conditions n times
test_data = dplyr::bind_rows(replicate(trialsPerCondition, test_trial_conditions, simplify = FALSE))

# Simulate choice and RT for the replicated trial conditions
test_data = sim_task(test_data, model_name = "model1a", d = d, sigma = sigma) %>%drop_na()
```

```{r}
out = fit_ddm_pta(data_to_fit_ = test_data, model_name_ = "model1a", search_space_= list(rangeD = rangeD, rangeSigma = rangeSigma))
```

Successful recovery even with 50 particles per trial condition.

```{r}
data.frame(m = out$models) %>%
  separate(m, c("d", "sigma"), sep= ",") %>%
  mutate(posteriors = unlist(out$posteriors, use.names = FALSE)) %>%
  ggplot(aes(sigma, d, fill=posteriors))+
  geom_tile()
```

What about with a different combination of delta and gamma but still only doing a grid search through delta and sigma?

```{r}
trialsPerCondition=50 

# Replicate same conditions n times
test_data = dplyr::bind_rows(replicate(trialsPerCondition, test_trial_conditions, simplify = FALSE))

test_data = sim_task(test_data, model_name = "model1a", d = d, sigma = sigma, delta=3, gamma=3) %>%drop_na()

out = fit_ddm_pta(data_to_fit_ = test_data, model_name_ = "model1a", rangeD_ = rangeD, rangeSigma_ = rangeSigma)
```

Failed recovery in the same way both with 50 and 150 particles particles.

```{r}
data.frame(m = out$models) %>%
  separate(m, c("d", "sigma"), sep= ",") %>%
  mutate(posteriors = unlist(out$posteriors, use.names = FALSE)) %>%
  ggplot(aes(sigma, d, fill=posteriors))+
  geom_tile()
```

How do the posteriors change trial by trial? Is the convergence on the wrong model fast?

```{r}

```

Can you recover the true combination if the grid search included all the parameters?

```{r}

```

Why would a model converge on the wrong combination?

1. If the trials consistently have higher likelihoods
  - Clarify how the likelihoods would necessarily be larger for different sampling distributions based on the parameters
2. ...

```{r}

```

How can you use the trial likelihoods to compute model posteriors with other optimization algorithms?

What do you need for the posterior?
  1. Likelihood for each trial.
  2. (Assuming equally likely models) number of models trials.

Can you get these using the optim function in R?
  1. Not sure
  2. I expect this would be the number of iterations
  
If I can get the parameter combinations used by optim in each iteration using something like as described here https://stackoverflow.com/questions/52552143/how-to-save-the-coefficients-for-each-optim-iteration
I should be able to generate trial likelihoods and 

Try this first with the simpler model model1.

Step 1: Does optim recover the true parameters? Or at least move in the right direction?

```{r}
# Previous code that worked with the built-in optim 
# optim(c(.04, .02), get_task_nll, data=stim1, par_names = c("d", "sigma"), model_name="model1", control = list(maxit=75))
```

How viable is this with Stan using hierarchical models? Can you use the trial likelihood function to code the bespoke model in Stan?

```{r}

```
