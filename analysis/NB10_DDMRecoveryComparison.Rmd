---
title: "Experience vs. description based decision-making project: DDM parameter recovery exercises"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: 'hide'
---

# Setup

Set up environment and load in data

```{r include=FALSE}
library(tidyverse)
library(here)
cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
theme_set(theme_classic())
sem <- function(x) {sd(x, na.rm=T) / sqrt(length(x))}
helpers_path = here('analysis/helpers/')

set.seed(38573)
```

```{r}
source(paste0(helpers_path,'ddmSims/fit_task.R'))
source(paste0(helpers_path,'ddmSims/sim_task.R'))
source(paste0(helpers_path,'ddmSims/fit_ddm_pta.R'))
test_trial_conditions = read.csv(paste0(helpers_path, 'ddmSims/test_data/test_trial_conditions.csv'))
```

Empty lists to store the trial simulators for the forthcoming models.

```{r}
sim_trial_list = list()
fit_trial_list = list()
```

# Parameter recovery

## Model 1: Simplest

```{r}
source(paste0(helpers_path, 'ddmSims/r_ddm_models/ddm_model1.R'))

sim_trial_list[['model1']] = sim_trial
fit_trial_list[['model1']] = fit_trial
```

### Simulate data

```{r}
d = .06
sigma = .08
rangeD = c(0.05, 0.06, 0.07)
rangeSigma = c(0.065, 0.08, 0.095)
trialsFileName=NA
trialsPerCondition=250 
```

```{r}
# Replicate same conditions n times
test_data = dplyr::bind_rows(replicate(trialsPerCondition, test_trial_conditions, simplify = FALSE))

# Simulate choice and RT for the replicated trial conditions
test_data = sim_task(test_data, model_name = "model1", d = d, sigma = sigma) %>%drop_na()
```

### Fit using PTA

```{r}
numModels = length(rangeD) * length(rangeSigma)
likelihoods = list()
models = c()
posteriors = list()

# Get likelihoods for all models and all artificial trials.
for (i in 1:length(rangeD)){
  curD = rangeD[i]
  for (j in 1:length(rangeSigma)){
    curSigma = rangeSigma[j]
    model = paste0(as.character(curD), ", ", as.character(curSigma))
    curFit = fit_task(test_data, model_name = "model1", pars_ = list(d=curD, sigma = curSigma))
    likelihoods[[model]] = curFit$likelihood
    models = c(models, model)
    posteriors[[model]] = 1/numModels
  }
}
```

```{r}
# Compute the posteriors.
for(t in 1:nrow(test_data)){
  denominator = 0
  for(m in 1:length(models)){
    model = models[m]
    denominator = denominator + (posteriors[[model]] * likelihoods[[model]][t]) 
    if(denominator == 0){
      next
    }
  }
  
  for(m in 1:length(models)){
    model = models[m]
    prior = posteriors[[model]]
    posteriors[[model]] = likelihoods[[model]][t] * prior /denominator
  }
}
```

Likelihood surface

```{r}
data.frame(m = models) %>%
  separate(m, c("d", "sigma"), sep= ",") %>%
  mutate(likelihoods = unlist(lapply(likelihoods, sum), use.names = FALSE)) %>%
  ggplot(aes(sigma, d, fill=likelihoods))+
  geom_tile()
```

```{r eval=FALSE}
# What do the likelihoods for each trial look like for each model? Of comparable ranges, rising and falling similarly for each trial.

data.frame(likelihoods) %>%
  gather(key, value) %>%
  ggplot(aes(value))+
  geom_histogram(bins=30, alpha=.5, position="identity")+
  facet_grid(key~.)

data.frame(likelihoods) %>%
  gather(key, value) %>%
  group_by(key) %>%
  mutate(trial = 1:n()) %>%
  ggplot(aes(trial, value))+
  geom_line()+
  facet_grid(key~.)+
  xlim(0, 200)
```

Model posteriors

```{r}
data.frame(m = models) %>%
  separate(m, c("d", "sigma"), sep= ",") %>%
  mutate(posteriors = unlist(posteriors, use.names = FALSE)) %>%
  ggplot(aes(sigma, d, fill=posteriors))+
  geom_tile()
```
How do the posteriors change across trials for each parameter combination?

```{r}
posteriors_tbt = list()

for (i in 1:length(models)){
  model = models[i]
  posteriors_tbt[[model]] = c(1/numModels, rep(NA,length(likelihoods[[1]])-1))
}

```

```{r}
# Compute the posteriors.
for(t in 1:nrow(test_data)){
  denominator = 0
  for(m in 1:length(models)){
    model = models[m]
    denominator = denominator + (posteriors_tbt[[model]][t] * likelihoods[[model]][t]) 
    if(denominator == 0){
      next
    }
  }
  
  for(m in 1:length(models)){
    model = models[m]
    prior = posteriors_tbt[[model]][t]
    posteriors_tbt[[model]][t+1] = likelihoods[[model]][t] * prior /denominator
  }
}
```

```{r}
tmp = dplyr::bind_rows(replicate(nrow(test_data)+1, data.frame(m = models), simplify = FALSE)) %>%
  arrange(m) %>%
  mutate(posteriors = unlist(posteriors_tbt, use.names = FALSE)) %>%
  group_by(m) %>%
  mutate(trial = 1:n()) 
```

Posteriors maxes out in relatively few trials for the true model.  

In this case there are 21 combinations repeated 250 times. Since `fit_task` takes in `test_data` which repeats these 21 trials in a single block the maxing out should be happening after all trials have been fit about 20 times (400 on the plot above). So you should be able to get to correct parameter combination with much fewer trials.

```{r}
tmp %>%
  ggplot(aes(trial, posteriors, color=m))+
  geom_line()+
  theme(legend.position = "bottom")+
  xlim(0, 500)
```
```{r}
rm(likelihoods, posteriors, posteriors_tbt, numModels, models)
```

Confirm that you can indeed recover the parameters with fewer repetitions.

#### Recovery with fewer trials

```{r}
trialsPerCondition=25 

# Replicate same conditions n times
test_data = dplyr::bind_rows(replicate(trialsPerCondition, test_trial_conditions, simplify = FALSE))

# Simulate choice and RT for the replicated trial conditions
test_data = sim_task(test_data, model_name = "model1", d = d, sigma = sigma) %>%drop_na()
```

```{r}
out = fit_ddm_pta(data_to_fit_ = test_data, model_name_ = "model1", search_space_ = list(rangeD = rangeD, rangeSigma = rangeSigma))
```

```{r}
data.frame(m = out$models) %>%
  separate(m, c("d", "sigma", "delta", "gamma"), sep= ",") %>%
  mutate(posteriors = unlist(out$posteriors, use.names = FALSE)) %>%
  ggplot(aes(sigma, d, fill=posteriors))+
  geom_tile()
```

Ok, this was for the simplest model. Does it work for a more complicated model?

Initially considered trying Model 2b. This is the one with early integration for the `probFractalDraw == 1` case and probability distortion for the fractals. This had captured several aspects of the true data in simulations. 

Initial efforts (no longer shown here) failed to recover the true parameters. There might be at least two reasons for this:
1. Grid search happened only for d and sigma and not for the other two parameters (delta and gamma), which were set to 1 both in simulating and fitting the task.  
2. The trial likelihood calculation using the state-space approach depends on the assumption that integration happens until the RT as a function of the value difference. For a model where early integration is allowed it is possible that a decision is reached before the stimuli are presented and the RT is then sampled from some log-normal distribution. In such cases the RT that would be looped until is not generated as a function of the value difference in the model. **So how would the state likelihood estimation should work when early integration is allowed???**  

So first check if a model with distortion but no early integration converges on the correct parameter combination. 

## Model 1a

This model expands the simplest model by adding probability distortion to the fractals but does not have early integration for the `probFractalDraw == 1` case.

```{r}
source(paste0(helpers_path, 'ddmSims/r_ddm_models/ddm_model1a.R'))
```

```{r}
sim_trial_list[['model1a']] = sim_trial
fit_trial_list[['model1a']] = fit_trial
```

### Simulate data 

Setting only d and sigma and leaving delta and gamma at 1.

```{r}
trialsPerCondition=50 

# Replicate same conditions n times
test_data = dplyr::bind_rows(replicate(trialsPerCondition, test_trial_conditions, simplify = FALSE))

# Simulate choice and RT for the replicated trial conditions
test_data = sim_task(test_data, model_name = "model1a", d = d, sigma = sigma) %>%drop_na()
```

```{r}
out = fit_ddm_pta(data_to_fit_ = test_data, model_name_ = "model1a", search_space_= list(rangeD = rangeD, rangeSigma = rangeSigma))
```

Successful recovery even with 50 particles per trial condition.

```{r}
data.frame(m = out$models) %>%
  separate(m, c("d", "sigma", "delta", "gamma"), sep= ",") %>%
  mutate(posteriors = unlist(out$posteriors, use.names = FALSE)) %>%
  ggplot(aes(sigma, d, fill=posteriors))+
  geom_tile()
```

What about with a different combination of delta and gamma but still only doing a grid search through delta and sigma?

```{r}
trialsPerCondition=50 

# Replicate same conditions n times
test_data = dplyr::bind_rows(replicate(trialsPerCondition, test_trial_conditions, simplify = FALSE))

test_data = sim_task(test_data, model_name = "model1a", d = d, sigma = sigma, delta=3, gamma=3) %>%drop_na()

out = fit_ddm_pta(data_to_fit_ = test_data, model_name_ = "model1a", search_space_ = list(rangeD = rangeD, rangeSigma = rangeSigma), posteriors_tbt_ = TRUE)
```

Have seen all possible cases (failed recovery, successful recovery and split posterior) with with 50 particles.

```{r}
data.frame(m = out$models) %>%
  separate(m, c("d", "sigma", "delta", "gamma"), sep= ",") %>%
  mutate(posteriors = unlist(out$posteriors, use.names = FALSE)) %>%
  ggplot(aes(sigma, d, fill=posteriors))+
  geom_tile()
```

How do the posteriors change trial by trial? Is the convergence fast or is there an in-between period with high uncertainty?

```{r}
tmp = dplyr::bind_rows(replicate(nrow(test_data)+1, data.frame(m = out$models), simplify = FALSE)) %>%
  arrange(m) %>%
  mutate(posteriors = unlist(out$posteriors_tbt, use.names = FALSE)) %>%
  group_by(m) %>%
  mutate(trial = 1:n()) 
```

```{r}
tmp %>%
  ggplot(aes(trial, posteriors, color=m))+
  geom_line()+
  theme(legend.position = "bottom")
```

What if you have more particles?

```{r}
trialsPerCondition=150 

# Replicate same conditions n times
test_data = dplyr::bind_rows(replicate(trialsPerCondition, test_trial_conditions, simplify = FALSE))

test_data = sim_task(test_data, model_name = "model1a", d = d, sigma = sigma, delta=3, gamma=3) %>%drop_na()

out = fit_ddm_pta(data_to_fit_ = test_data, model_name_ = "model1a", search_space_ = list(rangeD = rangeD, rangeSigma = rangeSigma), posteriors_tbt_ = TRUE)

data.frame(m = out$models) %>%
  separate(m, c("d", "sigma", "delta", "gamma"), sep= ",") %>%
  mutate(posteriors = unlist(out$posteriors, use.names = FALSE)) %>%
  ggplot(aes(sigma, d, fill=posteriors))+
  geom_tile()
```

```{r}
tmp = dplyr::bind_rows(replicate(nrow(test_data)+1, data.frame(m = out$models), simplify = FALSE)) %>%
  arrange(m) %>%
  mutate(posteriors = unlist(out$posteriors_tbt, use.names = FALSE)) %>%
  group_by(m) %>%
  mutate(trial = 1:n()) 
```

```{r}
tmp %>%
  ggplot(aes(trial, posteriors, color=m))+
  geom_line()+
  theme(legend.position = "bottom")
```

These managed to recover the true d and sigma but delta and gamma were incorrectly fixed at 1. What if you expand to search space to do a grid search across those as well?

Can you recover the true combination if the grid search included all the parameters?

```{r}
rangeDelta = c(1.5, 3, 4.5)
rangeGamma = c(1.5, 3, 4.5)
```

Reducing number of trials per condition because the search space will be much larger

```{r}
trialsPerCondition=30 

# Replicate same conditions n times
test_data = dplyr::bind_rows(replicate(trialsPerCondition, test_trial_conditions, simplify = FALSE))

test_data = sim_task(test_data, model_name = "model1a", d = d, sigma = sigma, delta=3, gamma=3) %>%drop_na()

out = fit_ddm_pta(data_to_fit_ = test_data, model_name_ = "model1a", search_space_ = list(rangeD = rangeD, rangeSigma = rangeSigma, rangeDelta = rangeDelta, rangeGamma =
                                                                                          rangeGamma), posteriors_tbt_ = TRUE)
```

I have seen both successful recovery and split posteriors between models but at least one of them is the correct one.

```{r}
data.frame(m = out$models) %>%
  separate(m, c("d", "sigma", "delta", "gamma"), sep= ",") %>%
  mutate(posteriors = unlist(out$posteriors, use.names = FALSE)) %>%
  mutate(d_sigma_comb = paste0("d= ", d, ", sigma= ", sigma),
         delta_gamma_comb = paste0("delta= ", delta, ", gamma= ", gamma)) %>%
  ggplot(aes(d_sigma_comb, delta_gamma_comb, fill=posteriors))+
  geom_tile()+
  labs(x="", y = "")+
  theme(axis.text.x = element_text(angle=45))
```
Note that, if you had tried to use the sum of trial likelihoods, you would not have been able to tell many models apart from each other.

```{r}
data.frame(m = out$models) %>%
  separate(m, c("d", "sigma", "delta", "gamma"), sep= ",") %>%
  mutate(d_sigma_comb = paste0("d= ", d, ", sigma= ", sigma),
         delta_gamma_comb = paste0("delta= ", delta, ", gamma= ", gamma)) %>%
  mutate(likelihoods = unlist(lapply(out$likelihoods, sum), use.names = FALSE)) %>%
  ggplot(aes(d_sigma_comb, delta_gamma_comb, fill=likelihoods))+
  geom_tile()+
  labs(x="", y = "")+
  theme(axis.text.x = element_text(angle=45))
```
How do the posteriors change over trials?

```{r}
tmp = dplyr::bind_rows(replicate(nrow(test_data)+1, data.frame(m = out$models), simplify = FALSE)) %>%
  arrange(m) %>%
  mutate(posteriors = unlist(out$posteriors_tbt, use.names = FALSE)) %>%
  group_by(m) %>%
  mutate(trial = 1:n()) 
```

```{r}
tmp %>%
  separate(m, c("d", "sigma", "delta", "gamma"), sep= ",") %>%
  ggplot(aes(trial, posteriors))+
  geom_line(aes(linetype=d, color=sigma))+
  theme(legend.position = "bottom")+
  facet_grid(delta ~ gamma)+
  scale_linetype_manual(values=c("twodash", "solid", "dotted"))
```

*Conclusion: Sum of trial likelihoods from the state-space approach does not seem viable to estimate parameters. Model posteriors based on trial likelihoods are able to recover parameters successfully.*

**Question: [Not immediately crucial but to understand how ML with what I understand to be the "standard" way of fitting with RT histograms] What is the intuition behind the likelihood computation from the RT histograms conditioned on choice?**

How can you use the trial likelihoods to compute model posteriors with other optimization algorithms?

## Optimizer

What do you need for the posterior?
  1. Likelihood for each trial.
  2. (Assuming equally likely models) number of models trials.

Can you get these using the `optim` function in R?
  1. I don't think so but if I can trace each parameter combination in iterations then I can compute them after the fact
  2. I expect this would be the number of iterations
  
If I can get the parameter combinations used by optim in each iteration (I can using jbryer/visualMLE package) I should be able to generate trial likelihoods and then compute posteriors for the parameters that were tried by the optimizer. Then I can also compare them to its "best-fitting" parameters as well.

Try this first with the simpler model model1. Does optim recover the true parameters? Or at least move in the right direction?

```{r}
trialsPerCondition=25 

# Replicate same conditions n times
test_data = dplyr::bind_rows(replicate(trialsPerCondition, test_trial_conditions, simplify = FALSE))

# Simulate choice and RT for the replicated trial conditions
test_data = sim_task(test_data, model_name = "model1", d = d, sigma = sigma) %>%drop_na()
```

True parameters

```{r}
d
sigma
```

Got pretty close to the true parameters without convergence due to `maxit` limit.

```{r}
optim_out = optim_save(c(.01, .01), get_task_nll, data=test_data, par_names = c("d", "sigma"), model_name="model1", control = list(maxit=100))
```

```{r}
optim_out$par
```

Does it converge and on the correct parameters if you remove the `maxit`? Doesn't get much closer than the 100 iterations.

```{r eval=FALSE}
optim_out = optim_save(c(.01, .01), get_task_nll, data=test_data, par_names = c("d", "sigma"), model_name="model1")
```

```{r}
optim_out$par
```

What about starting from another point?

```{r}
optim_out = optim_save(c(.1, .1), get_task_nll, data=test_data, par_names = c("d", "sigma"), model_name="model1", control = list(maxit=100))
```

```{r}
optim_out$par
```

Compute parameter combination trial by trial likelihoods for the optim search space

```{r}

```

How viable is this with Stan or with the Lombardi toolbox using hierarchical models? Can you use the trial likelihood function to code the bespoke model in Stan?

```{r}

```

Moving things over to remote cluster for subject estimation

```{r}

```

# Stop clusters

```{r}
parallel::stopCluster(cl = my.fit.cluster)
parallel::stopCluster(cl = my.sim.cluster)
```
