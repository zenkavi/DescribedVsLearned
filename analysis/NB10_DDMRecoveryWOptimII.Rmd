---
title: "Experience vs. description based decision-making project: DDM parameter recovery with the `optim` function"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: 'hide'
---

# Setup

Set up environment and load in data

```{r include=FALSE, message=FALSE}
library(tidyverse)
library(here)
library(gridExtra)
cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
theme_set(theme_classic())
sem <- function(x) {sd(x, na.rm=T) / sqrt(length(x))}
helpers_path = here('analysis/helpers/')

source(paste0(helpers_path, 'ddModels/ddm_par_recovery_report.R'))

set.seed(38573)
```

# Recovery with random starts on random datasets

Note: This took ~1 hour for a single subject's data running two jobs on each node for 25 compute nodes.

`docker run --rm -it -v ~/.aws:/root/.aws -v $(pwd):/cluster_scripts amazon/aws-cli s3 sync  s3://described-vs-experienced/ddModels/cluster_scripts/optim_out /cluster_scripts/optim_out`

Sample plots summarizing the simulations

```{r}
model_name = "model1a"
data_name = "sim_single_sub_data"
optim_out_path = paste0(helpers_path, 'ddModels/cluster_scripts/optim_out/')

ddm_par_recovery_report(model_ = model_name, data_ = data_name, optim_out_path_=optim_out_path)
```

```{r}
ddm_par_recovery_report(model_ = model_name, data_ = data_name, optim_out_path_=optim_out_path, diff_pct_plots_ = TRUE)
```

To get a sense of recovery success for different parameter combinations I created 20 random datasets using the following steps:   

- Sample `d` and `sigma` from a uniform distribution between 0 and 1 and `delta` and `gamma` from a uniform distribution between 1 and 8.   
- Simulate data for a single subject using the same conditions (QV and EV pairs) across parameter combinations.  
- Run `optim` (the built-in optimizer using the Nelder-Mead algorithm) with 1000 random starts on each of these 20 datasets.  
- The starting values were sampled from the same distributions described above.  

## Results

- [Scatter plots of start and convergend points in each iteration for all parameters](../outputs/fig/ddm_recovery_rand_datasets_scatter.pdf)  
  - Variance in recovered parameters is always higher for `delta` and `gamma` than for `d` and `sigma`  
  - There is almost invariably a correlation between start and end points for `delta` and `gamma` but not for `d` and `sigma`  
- [Median percentage of difference for each parameter across datasets](../outputs/fig/ddm_recovery_rand_datasets_diff_pct.pdf)
  - `Delta` and `gamma` are not always the worst recovered parameters. There are cases, albeit fewer, where `d` and `sigma` are not recovered well either (7 of 20 datasets). A quick look suggests these are cases with very small `sigma`.  
- Correlation between recovered parameters especially 
  - All iterations or best estimate?

```{r}
get_optim_out(model_ = model_name, data_=data_name, optim_out_path_ = optim_out_path)
```

- Relationship between true value and recovery success?  
  - In the current simulations?
  - Grid of recovery success for delta and gamma when d and sigma are fixed
    - Fix d = 0.06, sigma = 0.08 for delta and gamma = seq(1, 8, 1) 64 datasets, 
      - 4 heatmaps: for each combination delta and gamma what is the median percentage of difference for each parameter across the 1000 iterations (though focus is on the heatmaps of delta and gamma to see if there is a como)
      - True versus best of 1000 (ie. lowest nll) for delta and gamma. 2 scatter plots (one for delta and another for gamma) with 64 points or boxplots with 8 levels

```{r}

```

More data per optimization? (currently for single subject)
Hierarchical estimation?

Would you compute posterior among these different optimized values? Is there any point in doing that even when there is no iteration that has converged on the true values?

```{r}

```


