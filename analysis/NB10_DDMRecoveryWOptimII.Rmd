---
title: "Experience vs. description based decision-making project: DDM parameter recovery with the `optim` function"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: 'hide'
---

# Setup

Set up environment and load in data

```{r include=FALSE, message=FALSE}
library(tidyverse)
library(here)
library(gridExtra)
cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
theme_set(theme_classic())
sem <- function(x) {sd(x, na.rm=T) / sqrt(length(x))}
helpers_path = here('analysis/helpers/')

source(paste0(helpers_path, 'ddModels/ddm_par_recovery_report.R'))

set.seed(38573)
```

# Recovery with random starts on random datasets

Note: This took ~1 hour for a single subject's data running two jobs on each node for 25 compute nodes.

`docker run --rm -it -v ~/.aws:/root/.aws -v $(pwd):/cluster_scripts amazon/aws-cli s3 sync  s3://described-vs-experienced/ddModels/cluster_scripts/optim_out /cluster_scripts/optim_out`

Sample plots summarizing the simulations

```{r}
model_name = "model1a"
data_name = "sim_single_sub_data"
optim_out_path = paste0(helpers_path, 'ddModels/cluster_scripts/optim_out/')

ddm_par_recovery_report(model_ = model_name, data_ = data_name, optim_out_path_=optim_out_path)
```

```{r}
ddm_par_recovery_report(model_ = model_name, data_ = data_name, optim_out_path_=optim_out_path, diff_pct_plots_ = TRUE)
```

To get a sense of recovery success for different parameter combinations I created 20 random datasets using the following steps:   

- Sample `d` and `sigma` from a uniform distribution between 0 and 1 and `delta` and `gamma` from a uniform distribution between 1 and 8.   
- Simulate data for a single subject using the same conditions (QV and EV pairs) across parameter combinations.  
- Run `optim` (the built-in optimizer using the Nelder-Mead algorithm) with 1000 random starts on each of these 20 datasets.  
- The starting values were sampled from the same distributions described above.  

## Results

- [Scatter plots of start and convergend points in each iteration for all parameters](../outputs/fig/ddm_recovery_rand_datasets_scatter.pdf)  
  - Variance in recovered parameters is always higher for `delta` and `gamma` than for `d` and `sigma`  
  - There is almost invariably a correlation between start and end points for `delta` and `gamma` but not for `d` and `sigma`  
- [Median percentage of difference for each parameter across datasets](../outputs/fig/ddm_recovery_rand_datasets_diff_pct.pdf)
  - `Delta` and `gamma` are not always the worst recovered parameters. There are cases, albeit fewer, where `d` and `sigma` are not recovered well either (7 of 20 datasets). A quick look suggests these are cases with very small `sigma`.  
- Correlation between recovered parameters especially 
  - Distribution of correlations for all iterations in each dataset
  - Distribution of correlations for the best estimate across datasets

```{r}
model_name = "model1a"
data_prefix = "sim_single_sub_data"
optim_out_path = paste0(helpers_path, 'ddModels/cluster_scripts/optim_out/')
true_pars_path = paste0(helpers_path, 'ddModels/cluster_scripts/test_data/')
fig_out_path = paste0(here(), '/outputs/fig/')

out_iters = data.frame()

for(i in 1:20){
  cur_out = get_optim_out(model_ = model_name, data_=paste0(data_prefix, i), optim_out_path_ = optim_out_path, iters= TRUE)
  cur_out$dataset = i
  cur_out$true_pars = get_true_pars(data_=paste0(data_prefix, i), true_pars_path_ =  true_pars_path, return_str_ = TRUE)
  out_iters = rbind.all.columns(out_iters, cur_out)
}


out_iters = out_iters %>%
  rename(d = Param1, sigma = Param2, delta = Param3, gamma = Param4, nll = Result)
``` 

```{r}
# Add true parameters for each dataset and the correlation between delta and gamma to each facet of this plot
p_df = out_iters %>%
  filter(Iteration != 1) %>%
  filter(abs(delta) < 10 & delta > 0) %>%
  filter(abs(gamma) < 10 & gamma > 0) %>%
  group_by(dataset) %>%
  mutate(dg_cor = cor(delta, gamma)) %>%
  ungroup()

p_df_annot = p_df %>% 
  select(true_pars, dg_cor) %>%
  distinct() %>%
  mutate(dg_cor = paste0("r = ", round(dg_cor, 3)))
  
p = p_df %>% ggplot(aes(delta, gamma)) +
  geom_point(color="gray", size=.5)+
  facet_wrap(~true_pars, labeller = labeller(true_pars = label_wrap_gen(26) ) ) +
  geom_text(data = p_df_annot, aes(x = 2.5, y = 2.5, label = dg_cor), size=3)

ggsave(file=paste0(fig_out_path, 'ddm_recovery_rand_datasets_delta_gamma_cor.pdf'), p, height = 8, width=11, units="in")

```

Correlation between delta and gamma for the best estimate (the one with the lowest negative log likelihood) across datasets

```{r}
out_iters %>%
  filter(Iteration != 1) %>%
  group_by(dataset) %>%
  filter(nll == min(nll)) %>%
  ggplot(aes(delta, gamma))+
  geom_point()+
  xlim(1,10)+
  ylim(1, 10)
```

Given the correlation between delta and gamma maybe a try a parameterization with a single parameter for the distortion function? In fact, the logit slopes suggest that there isn't really any overweighting of probFractalDraw even for higher values at the group level (not to mention the large individual differences). So maybe what we need is not a sigmoid function but one that underweights almost all probFractalDraw values.

```{r}
delt = .1
gamm = 3

data.frame(prob = seq(0,1,.1)) %>%
  mutate(distorted_prob_1a = (delt * (prob)^gamm) / ( (delt * (prob)^gamm) + (1-prob)^gamm ),
         distorted_prob_1b = ((prob)^gamm) / ( ((prob)^gamm) + (1-prob)^gamm )  ,
         distorted_prob_1c = (delt * (prob)) / ( (delt * (prob)) + (1-prob)),) %>%
  gather(key, value, -prob) %>%
  ggplot(aes(prob, value, color=key))+
  geom_point()+
  geom_line()+
  geom_abline(aes(intercept=0, slope = 1), linetype="dashed")+
  theme(legend.position = "bottom")+
  labs(color="")
```

- Relationship between true value and recovery success?  
  - In the current simulations? Plot true value by median percentage of difference?
  - Grid of recovery success for delta and gamma when d and sigma are fixed
    - Fix d = 0.06, sigma = 0.08 for delta and gamma = seq(1, 8, 1) 64 datasets, 
      - 4 heatmaps: for each combination delta and gamma what is the median percentage of difference for each parameter across the 1000 iterations (though focus is on the heatmaps of delta and gamma to see if there is a como)
      - True versus best of 1000 (ie. lowest nll) for delta and gamma. 2 scatter plots (one for delta and another for gamma) with 64 points or boxplots with 8 levels

```{r}

```

More data per optimization? (currently for single subject)
Hierarchical estimation?

Would you compute posterior among these different optimized values? Is there any point in doing that even when there is no iteration that has converged on the true values?

```{r}

```


