---
title: "Experience vs. description based decision-making project: DDM parameter recovery multiple round optimization"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: 'hide'
---

# Setup

Set up environment and load in data

```{r include=FALSE, message=FALSE}
library(tidyverse)
library(here)
library(gridExtra)
cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
theme_set(theme_classic())
sem <- function(x) {sd(x, na.rm=T) / sqrt(length(x))}
helpers_path = here('analysis/helpers/')
cpueaters_path = '/Users/zeynepenkavi/CpuEaters/DescribedVsLearned_beh/analysis/helpers/'
source(paste0(helpers_path, 'ddModels/ddm_par_recovery_report.R'))

set.seed(385736)
```

```{r message=FALSE}
source(paste0(helpers_path, 'ddModels/fit_task.R'))
sim_trial_list = list()
fit_trial_list = list()

source(paste0(helpers_path, 'ddModels/get_true_pars.R'))
true_pars_path = paste0(helpers_path, 'ddModels/cluster_scripts/test_data/')

library(visualMLE)
```

# 3 parameter model

Recovery exercises both on random datasets, as well as, on a grid of delta and gamma showed a correlation between these parameters and therefore issues in identifiability. 

Motivated also by the logit analyses for the whole sample, which showed that while the weight of EVs decreased monotonically, weight of QVs followed more a step function/was possibly consistently underweighted I decided to switch the probability distortion function to a single parameter one.

In this notebook I look at how well the parameters of a now three parameter model (d, sigma, delta) are recovered using two different optimization methods.

To understand how well recovery works (especially for delta) using a three parameter model (d, sigma, delta) I systematically varied each parameter and generated 36 datasets to optimize over. The datasets were generated using the combination of the true values `true_ds = c(.001, .06 , .5)`, `true_sigmas = c(.001, .08, .3)`, `true_deltas = c(.1, .5 , 1, 3)`.

```{r}
optim_out_path = paste0(cpueaters_path, 'ddModels/cluster_scripts/optim_out/')
true_pars_path = paste0(helpers_path, 'ddModels/cluster_scripts/test_data/')
model = "model1c"
data_suffix = "sim_single_sub_data"

data_nums = c(46:81)
sim_types = c("sim3", "sim3b")

comp_data = data.frame()

for(i in 1:length(data_nums)){
  cur_data_num = data_nums[i]
  for(j in 1:length(sim_types)){
    cur_sim_type = sim_types[j]
    cur_out = ddm_par_recovery_report(model_ = model, data_ = paste0(data_suffix,cur_data_num), optim_out_path_= paste0(optim_out_path, cur_sim_type, '/'), true_pars_path_ = true_pars_path, diff_pct_plots_ = TRUE)$diff_pct_data
    cur_out$sim_type = cur_sim_type
    cur_out$data_num = as.character(cur_data_num)
    comp_data = rbind.all.columns(comp_data, cur_out)
  }
}
```

## Sim3: Single round optimization

How bad was delta recovery for each combination of d and sigma (3 by 3 heatmap) panels for each value of delta (4 panels)

```{r}
plot_heatmaps = function(x_par = "d", y_par = "sigma", fill_par = "delta"){
  
  comp_data = comp_data %>%
    mutate(true_val = as.factor(true_val))
  
  tmp1 = comp_data %>%
    filter(sim_type == "sim3") %>%
    filter(key == x_par) %>%
    select(key, true_val, data_num) %>%
    spread(key, true_val)
  
  tmp2 = comp_data %>%
    filter(sim_type == "sim3") %>%
    filter(key == y_par) %>%
    select(key, true_val, data_num) %>%
    spread(key, true_val)
  
  tmp3 = comp_data %>%
    filter(sim_type == "sim3") %>%
    filter(key == fill_par) %>%
    select(key, true_val, data_num) %>%
    spread(key, true_val) %>%
    rename(true_val = fill_par)
  
  tmp4 = comp_data %>%
    filter(sim_type == "sim3") %>%
    filter(key == fill_par) %>%
    select(key, median_diff, data_num) %>%
    spread(key, median_diff) %>%
    rename(median_diff = fill_par)
  
  tmp1 %>%
    left_join(tmp2, by="data_num") %>%
    left_join(tmp3, by="data_num") %>%
    left_join(tmp4, by="data_num") %>%
    mutate(median_diff_clipped = ifelse(median_diff>100, 101, median_diff)) %>%
    ggplot(aes_string(x=x_par, y=y_par))+
    geom_tile(aes(fill=median_diff_clipped))+
    facet_wrap(~true_val)+
    theme(legend.position = "bottom")+
    labs(x=x_par, y=y_par, fill=paste0("Median % difference between true and estimated ", fill_par))
}
```

```{r}
plot_heatmaps(x_par = "d", y_par = "sigma", fill_par = "delta")
```

How bad was d recovery for each combination of delta and sigma (3 by 4 heatmap) panels for each value of d (3 panels)

```{r}
plot_heatmaps(fill_par = "d", y_par = "sigma", x_par = "delta")
```

How bad was sigma recovery for each combination of d and delta (3 by 4 heatmap) panels for each value of sigma (3 panels)

```{r}

```

## Sim3b: Two round optimization 

How bad was delta recovery for each combination of d and sigma (3 by 3 heatmap) panels for each value of delta (4 panels)

```{r}

```

How bad was d recovery for each combination of delta and sigma (3 by 4 heatmap) panels for each value of d (3 panels)

```{r}

```

How bad was sigma recovery for each combination of d and delta (3 by 4 heatmap) panels for each value of sigma (3 panels)

```{r}

```

## Comparison   

Scatter plot of median percentage difference from true value for single round (x-axis) vs two round (y-axis) for each parameter

```{r}

```


Next:
- Joint choice and RT modeling
- Hierarchical estimation

Meeting note: 
- Make likelihood surface for one of the recovered delta-gamma correlation scatter plots

------------------------------------------------------------------------------------------------
More data per optimization? (currently for single subject)

Would you compute posterior among these different optimized values? Is there any point in doing that even when there is no iteration that has converged on the true values?