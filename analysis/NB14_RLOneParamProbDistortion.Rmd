---
title: 'Experience vs. description based decision-making project: RL with one parameter probability distortion'
output:
  html_document:
    toc: yes
    toc_float: yes
    code_folding: 'hide'
  pdf_document:
    toc: yes
---

# Set up

```{r include=FALSE}
library(tidyverse)
library(here)
cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
```

```{r}
helpers_path = here('analysis/helpers/')
```

# Read in data

```{r include=FALSE}
source(paste0(helpers_path,'01_clean_behavioral_data.R'))
source(paste0(helpers_path, 'facet_wrap_equal.R'))
source(paste0(helpers_path, 'rlModels/make_posterior_predictive_data.R'))
source(paste0(helpers_path, 'rlModels/identifiability_analysis.R'))

```

Set theme for plots

```{r}
theme_set(theme_bw())
```

# Define model

On each trial subjects process not only the two fractals but also their relevance to the trial's reward, as well as, the lottery information. So to describe the choice process in this task, including accounting for learning about fractals across trials, the computational model must include all of these factors.

In this model the probability of choosing the option on the left is described as a softmax function that depends on the value difference between the left and right pairs. The free parameter $\beta$ controls the extent to which the choice depends on this value difference instead of chance.

$$p(choice = left) = \frac{1}{1+e^{-\beta(V_{left} - V_{right})}}$$
The value of each pair is defined as a weighted average of both of its components. The two components of each pair, the lottery and the fractal, are weighted either proportional to their relevance for that trial

$$V_i = (1-w(pFrac))EV_i + w(pFrac)QV_i, \:i \in \{left, right\}$$
or asymmetrically such that only the relevance of the learned values is distorted while the relevance of the lotteries is not
$$V_i = (1-pFrac)EV_i + w(pFrac)QV_i, \:i \in \{left, right\}$$
Based on the behavioral analyses showing [a linear decrease on the effect of lottery value difference as this attribute's relevance decreases but a step-like decrease for the effect of fractal value difference](https://zenkavi.github.io/DescribedVsLearned_beh/outputs/NB3_psychometrics_logits.html) we modeled the probability distortion function as one that allows only for under- or over-weigthing (unlike the s-shape for the distortion of probabilities of lottery outcomes in the behavioral economic literature)

$$w(pFrac) = \frac{\delta*pFrac}{\delta*pFrac+(1- pFrac)}$$
such that $\delta < 1$ suggest an underweighting and $\delta >1$ suggest an overweighting of the relevance of the learned attribute. 

Note also that a model that included an additional parameter $\gamma$ for the probability distortion function suggested that [this parameter was close to 1.](https://zenkavi.github.io/DescribedVsLearned_beh/outputs/NB2_two_systems_model.html#Model_fit)

The expected value of each lottery is computed by multiplying the probability of winning with the reward amount.

$$EV_{i, t} = p_{i, t} V{i_t}$$
The Q-value of each fractal is adjusted in each trial by a reward prediction error weighted by a learning rate ($\alpha$).

$$QV_{i,t} = QV_{i, t-1} + \alpha(R_{i, t-1}- QV_{i, t-1})$$

# Model fit

```{r message=FALSE}
source(paste0(helpers_path, 'rlModels/fit_rl_hierarchical_oneParamProbDistortion.R'))
# Rename objects 
g_par_ests_sym = g_par_ests
par_ests_sym = par_ests
rm(g_par_ests, par_ests, fit)
source(paste0(helpers_path, 'rlModels/fit_rl_hierarchical_AsymmProbDistortion.R'))
rm(fit)
```

## Log likelihood distributions

Distribution of likelihood across iterations for each subject. Asymmetric and symmetric distortion of probability models are often indistinguishable. When they differ symmetric tends to have higher likelihoods.

```{r}
par_ests %>%
  filter(par == "alpha") %>% #filtering one parameter bc likelihood for combinations of parameters is the same. nothing special abt alpha here
  mutate(fit_type = "Asymmetric") %>% 
  rbind(par_ests_sym %>%
          filter(par == "alpha") %>%
          mutate(fit_type = "Symmetric"))%>%
  ggplot(aes(logLik, fill=fit_type)) +
  geom_histogram(alpha=.5, bins=30, position="identity")+
  facet_wrap(~subnum, scales="free_x")+
  xlab("Log Likelihood")+
  ylab("")+
  ggtitle("Distribution of log likelihoods across the samples for each subject")+
  theme(axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        panel.grid = element_blank(),
        legend.position = "bottom",
        legend.title = element_blank())+
  scale_fill_manual(values=c(cbbPalette[1], cbbPalette[2]))
```


## Parameter point estimates

### Group parameters

For both asymmetric and symmetric distortion group delta posterior means are < 1 in line with the logistic regression weights.

```{r}
g_par_ests %>%
  mutate(type='posterior') %>%
  rbind(data.frame(g_alpha = rbeta(16000,1,1),
                   g_delta = rgamma(16000,1,5),
                   g_beta = rgamma(16000,1,2)) %>%
          gather(key, value) %>%
          mutate(type="prior")) %>%
  mutate(type=factor(type,levels=c("prior","posterior")),
         distortion = "Asymmetric") %>%
  rbind(g_par_ests_sym %>%
           mutate(type='posterior') %>%
           rbind(data.frame(g_alpha = rbeta(16000,1,1),
                            g_delta = rgamma(16000,1,5),
                            g_beta = rgamma(16000,1,2)) %>%
                   gather(key, value) %>%
                   mutate(type="prior")) %>%
           mutate(type=factor(type,levels=c("prior","posterior")),
                  distortion = "Symmetric")) %>%
  ggplot(aes(value, fill=type))+
  geom_histogram(bins=30, alpha=.5, position="identity")+
  facet_grid(distortion~key, scales='free')+
  theme(legend.position = "bottom",
        legend.title = element_blank(),
        panel.grid = element_blank())+
  scale_fill_manual(values = c(cbbPalette[1], cbbPalette[2]))+
  xlab("")+
  ylab("")
```

### Individual parameters

Comparison of MLE and mean a posteriori estimates. 

**Note: posterior median estimates are from different iterations/samples whereas the estimates that have the highest likelihood are from the same sample.**

```{r}
tmp1 = par_ests_sym %>%
  group_by(subnum, par) %>%
  filter(logLik == max(logLik)) %>%
  mutate(distortion = "Symmetric") %>%
  rbind(par_ests %>%
  group_by(subnum, par) %>%
  filter(logLik == max(logLik)) %>%
  mutate(distortion = "Asymmetric"))

tmp2 = par_ests_sym %>%
  group_by(subnum, par) %>%
  summarise(est = mean(value), .groups='keep') %>%
  mutate(distortion = "Symmetric") %>%
  rbind(par_ests %>%
  group_by(subnum, par) %>%
  summarise(est = mean(value), .groups='keep') %>%
  mutate(distortion = "Asymmetric"))

tmp = tmp2 %>%
  left_join(tmp1, by=c("subnum", "par", "distortion")) 

tmp %>%
  ggplot(aes(est, value))+ #est is posterior mean, value is MLE
  geom_point()+
  geom_abline(slope=1, intercept = 0, linetype="dashed")+
  facet_wrap_equal(distortion~par, scales='free')+
  xlab("Posterior mean")+
  ylab("Maximum likelihood")
```

Comparison of the each subjects' MLE and mean a posteriori estimates between the two models.

```{r}
tmp %>%
  rename(post=est, mle=value) %>%
  select(-logLik) %>%
  gather(key, value, -subnum, -par, -distortion) %>%
  spread(distortion, value) %>%
  ggplot(aes(Symmetric, Asymmetric))+
  geom_point()+
  geom_abline(slope=1, intercept = 0, linetype="dashed")+
  facet_wrap_equal(key~par, scales='free')
```

## Covariance between parameters

Are there any dependencies between the parameters? Looking at correlations between samples from the **same** iteration.

Symmetric distortion model

```{r}
tmp = g_par_ests_sym %>%
  group_by(key) %>%
  mutate(iter = 1:n()) %>%
  group_by(iter) %>%
  spread(key, value) %>%
  ungroup() %>%
  select(-iter)

round(cor(tmp), 3)  
```

Asymmetric distortion model

```{r}
tmp = g_par_ests %>%
  group_by(key) %>%
  mutate(iter = 1:n()) %>%
  group_by(iter) %>%
  spread(key, value) %>%
  ungroup() %>%
  select(-iter)

round(cor(tmp), 3)  
```

```{r echo=FALSE}
rm(tmp, tmp1, tmp2)
```

# Posterior predictive checks

Do the subject parameters capture the behavioral patterns in the data?

Sample from each subject's posterior distributions 100 times and simulate data using these sampled parameters. Plot predicted versus actual choice as a function of the probability of a fractal draw.

Using the symmetric distortion model.

```{r}
modelName = 'fit_rl_hierarchical_oneParamProbDistortion'

if(file.exists(paste0(helpers_path, 'rlModels/pp_data_', modelName, '.RDS'))){
  pp_data = readRDS(paste0(helpers_path, 'rlModels/pp_data_', modelName, '.RDS'))
} else{
  pp_data = make_posterior_predictive_data(numDraws = 100, modelName = modelName) 
}
```

```{r  fig.height=8, fig.width=8}
tmp = clean_beh_data %>%
  group_by(subnum, probFractalDraw) %>%
  summarise(propLeft = mean(choiceLeft), .groups='keep') %>%
  mutate(data_type = "actual")

pp_data %>%
  group_by(subnum, sampleNum, probFractalDraw) %>%
  summarise(propLeft = mean(choiceLeft), .groups='keep') %>%
  mutate(data_type = "predicted") %>%
  rbind(tmp) %>%
  ggplot(aes(factor(probFractalDraw), propLeft, color=data_type, alpha=data_type, shape=data_type, size=data_type))+
  # geom_boxplot(position=position_dodge(width = 0))+
  geom_point()+
  facet_wrap(~subnum) + 
  theme(panel.grid = element_blank(), legend.position = "bottom", legend.title = element_blank())+
  xlab("Probability of a Fractal Draw")+
  ylab("Proportion of left choices")+
  ggtitle("Actuals versus predicted choice generated using parameters sampled from the posterior")+
  scale_color_manual(values=c(cbbPalette[2], "gray"))+
  scale_alpha_manual(values=c(1, .5))+
  scale_shape_manual(values=c(19, 1))+
  scale_size_manual(values=c(2, 1))
  
```

```{r echo=FALSE}
rm(modelName, tmp)
```

# Identifiability analysis

Simulate 25 subjects using the median posterior group parameters as true parameters

```{r}
trueGroupPars = g_par_ests %>%
  group_by(key) %>%
  summarise(median_value = round(median(value), 2)) %>%
  spread(key, median_value)

numSims = 25

set.seed(2349813)

truePars = data.frame(alpha=truncnorm::rtruncnorm(numSims, mean=trueGroupPars$g_alpha, a=0, b=1), 
                      beta=truncnorm::rtruncnorm(numSims, mean=trueGroupPars$g_beta, a=0, b=5), 
                      delta=truncnorm::rtruncnorm(numSims, mean=trueGroupPars$g_delta, a=0))

modelName = 'fit_rl_hierarchical_oneParamProbDistortion'

all_trials = clean_beh_data %>%
  select(-numRewardedTrials, -reactionTime, -responded, -choiceLeft, -reward, -totalReward) %>%
  rename(rightLotteryValue = referenceValue, rightLotteryProb = referenceProb, leftLotteryValue = lotteryValue, leftLotteryProb = lotteryProb) %>%
  mutate(subnum = ifelse(subnum == "27", "21", subnum))
```

```{r warning=FALSE}
id_par_ests = identifiability_analysis(truePars, modelName, all_trials = all_trials, subj_par_names = c("alpha", "beta", "delta"),group_par_names = c("g_alpha","g_delta", "g_beta"))
```

```{r}
id_par_ests$par_ests %>%
filter(par == "alpha") %>%
  ggplot(aes(logLik)) +
  geom_histogram(alpha=.5, bins=30)+
  facet_wrap(~subnum, scales="free_x")+
  xlab("Log Likelihood")+
  ylab("")+
  ggtitle("Distribution of log likelihoods across the samples for each subject")+
  theme(axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        panel.grid = element_blank())
```

Recovered group parameter posteriors. True values in blue.

```{r}
id_par_ests$g_par_ests %>%
  ggplot(aes(value))+
  geom_histogram(alpha=.5, bins = 30)+
  geom_vline(aes(xintercept = value), trueGroupPars %>% gather(key, value), color=cbbPalette[5])+
  facet_wrap(~key, scale="free")+
  theme(axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        panel.grid = element_blank(),
        strip.text.x = element_text(size=14),
        axis.text.x = element_text(size=14))+
  xlab("")+
  ylab("")
```

Recovered individual parameter posteriors. True values in blue.

```{r fig.height=8, fig.width=8}
id_par_ests$par_ests %>%
  ggplot(aes(value))+
  geom_histogram(alpha=.5, bins = 30)+
  facet_grid(subnum~par, scale="free")+
  geom_vline(aes(xintercept = value), truePars %>% mutate(subnum =1:n()) %>% gather(par, value, -subnum), color=cbbPalette[5])+
  theme(axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        panel.grid = element_blank(),
        strip.text.x = element_text(size=14),
        axis.text.x = element_text(size=14))+
  xlab("")+
  ylab("")
```

```{r echo=FALSE}
rm(id_par_ests, trueGroupPars, truePars, modelName, all_trials)
```

# Compare to previous RL fit

```{r}
g_par_ests_asym = g_par_ests
par_ests_asym = par_ests

source(paste0(helpers_path, 'rlModels/fit_rl_hierarchical.R'))
# Rename objects 
g_par_ests_old = g_par_ests
par_ests_old = par_ests
rm(g_par_ests, par_ests, fit)
```

## Log likelihood distributions

```{r}
par_ests_old %>%
  filter(par == "alpha") %>% #filtering one parameter bc likelihood for combinations of parameters is the same. nothing special abt alpha here
  mutate(fit_type = "Old") %>% 
  rbind(par_ests_sym %>%
          filter(par == "alpha") %>%
          mutate(fit_type = "Symmetric"))%>%
    rbind(par_ests_asym %>%
          filter(par == "alpha") %>%
          mutate(fit_type = "Asymmetric"))%>%
  ggplot(aes(logLik, fill=fit_type)) +
  geom_histogram(alpha=.5, bins=30, position="identity")+
  facet_wrap(~subnum, scales="free_x")+
  xlab("Log Likelihood")+
  ylab("")+
  ggtitle("Distribution of log likelihoods across the samples for each subject")+
  theme(axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        panel.grid = element_blank(),
        legend.position = "bottom",
        legend.title = element_blank())+
  scale_fill_manual(values=c(cbbPalette[1], cbbPalette[3], cbbPalette[2]))
```

## Parameter estimates

Compare alphas, betas, deltas

```{r}
tmp = par_ests_sym %>%
  group_by(subnum, par) %>%
  summarise(est = mean(value), .groups='keep') %>%
  mutate(distortion = "Symmetric") %>%
  rbind(par_ests_old %>%
          filter(par != "gamma") %>%
          group_by(subnum, par) %>%
          summarise(est = mean(value), .groups='keep') %>%
          mutate(distortion = "Old"))


tmp %>%
  spread(distortion, est) %>%
  ggplot(aes(Old, Symmetric))+ 
  geom_point()+
  geom_abline(slope=1, intercept = 0, linetype="dashed")+
  facet_wrap_equal(~par, scales='free')+
  xlab("Old (delta+gamma)")+
  ylab("Symmetric (delta)")
```

## QValues

Compare Q Values generated by previous hierarchical model and this one

```{r}
source(paste0(helpers_path,'add_inferred_pars.R'))
```

```{r}
clean_beh_data_old = add_inferred_pars(clean_beh_data, par_ests_old, model_name="original")
clean_beh_data_sym = add_inferred_pars(clean_beh_data, par_ests_sym, model_name="original")
```

```{r}
clean_beh_data_old %>%
  select(subnum, leftQValue, rightQValue) %>%
  mutate(data_type = "Old") %>%
  group_by(subnum) %>%
  mutate(trialNum=1:n()) %>%
  rbind(clean_beh_data_sym %>%
          select(subnum, leftQValue, rightQValue) %>%
          mutate(data_type = "Symmetric") %>%
          group_by(subnum) %>%
          mutate(trialNum=1:n())) %>%
  gather(key, value, -subnum, -data_type, -trialNum) %>%
  group_by(subnum, trialNum) %>%
  spread(data_type, value) %>%
  ggplot(aes(Old, Symmetric))+
  geom_point()+
  geom_abline(slope=1, intercept = 0, linetype="dashed")+
  facet_wrap_equal(~key, scales='free')+
  xlab("Old (delta+gamma)")+
  ylab("Symmetric (delta)")
```
