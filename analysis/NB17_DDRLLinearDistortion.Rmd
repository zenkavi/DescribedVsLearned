---
title: 'Experience vs. description based decision-making project: Hierarchical DDRL with linear distortion'
output:
  html_document:
    toc: yes
    toc_float: yes
    code_folding: 'hide'
  pdf_document:
    toc: yes
---

```{r include=FALSE}
library(tidyverse)
library(here)
library(broom)
cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

helpers_path = here('analysis/helpers/')
source(paste0(helpers_path, 'get_qvals.R'))
theme_set(theme_bw())
```

# Linear distortion

Motivation: the choice logit slopes showed choice depended on the fractal value difference a lot less than the lottery value difference.
Can a linear probability distortion parameter capture the choice effects?
Would you see logit slopes crossing appropriately at .5 if instead of a logit of choice ~ evdiff + qvdiff you had a logit of choice ~ evdiff + theta*qvdiff

**Model 1d:** 
d - drift rate scaler for value difference  
s - variance of evidence accumulation distribution  
alpha - learning rate  
theta - linear distortion  
With subject specific sigma added (by distorting drift rate and boundary separation as described in Wabersich and Vandekerckhove)

What are the parameter distributions?

```{r message=FALSE}
source(paste0(helpers_path, 'ddrlModels/fit_model1d.R'))
```

## Group posteriors

Note that `s` variance of evidence accumulation distribution is still centered around 1 even though it was allowed to vary.

```{r}
g_par_ests %>%
  ggplot(aes(value))+
  geom_histogram(bins=30, alpha=.5, position="identity")+
  facet_wrap(~key, scales='free')+
  theme(panel.grid = element_blank())+
  xlab("")+
  ylab("")

```

Covariance between parameters is low.

```{r}
tmp = g_par_ests %>%
  group_by(key) %>%
  mutate(iter = 1:n()) %>%
  group_by(iter) %>%
  spread(key, value) %>%
  ungroup() %>%
  select(-iter)

round(cor(tmp), 3)  
```

## Logit slope adjustment

Add parameter estimates to data, compute regressors with adjustments and run various logits.

```{r}
clean_beh_data = par_ests %>%
  group_by(subnum, par) %>%
  summarise(est = mean(value), .groups='keep') %>%
  spread(par, est) %>%
  left_join(clean_beh_data, by='subnum')

## Add Q values of fractals to each trial
clean_beh_data = clean_beh_data %>%
  group_by(subnum) %>%
  do(get_qvals(., model_name="original")) %>%
  ungroup()

#distorted values computed based on ddrlModels/fit_model1d.R
# opt_val[1] = ((1-trial_pFrac[i, t]) * ev_left[i, t]) + (w_pi * qv[1]);
clean_beh_data = clean_beh_data %>%
  mutate(rightLotteryEV = referenceProb * referenceValue,
         leftLotteryEV = lotteryValue * lotteryProb,
         lottery_ev_diff = leftLotteryEV - rightLotteryEV,
         fractal_qv_diff = leftQValue - rightQValue,
         fractal_prob_diff = fractalLeftProb - fractalRightProb,
         fractal_qv_diff_theta = theta*fractal_qv_diff, #note that these are subject specific thetas but the logits below model all data together
         fractal_prob_diff_theta = theta*fractal_prob_diff) 

# rm(fit, g_par_est, par_ests)

true_v_mod =  clean_beh_data %>%
  nest(data = -probFractalDraw) %>% 
  mutate(
    fit = map(data, ~ glm(choiceLeft ~ scale(lottery_ev_diff) + scale(fractal_prob_diff), data = .x, family=binomial(link="logit"))),
    tidied = map(fit, tidy)
  ) %>% 
  unnest(tidied) %>%
  filter(term != "(Intercept)") %>%
  select(probFractalDraw, term, estimate, std.error)

true_v_theta_mod = clean_beh_data %>%
  nest(data = -probFractalDraw) %>% 
  mutate(
    fit = map(data, ~ glm(choiceLeft ~ scale(lottery_ev_diff) + scale(fractal_prob_diff_theta), data = .x, family=binomial(link="logit"))),
    tidied = map(fit, tidy)
  ) %>% 
  unnest(tidied) %>%
  filter(term != "(Intercept)") %>%
  select(probFractalDraw, term, estimate, std.error)

qv_mod = clean_beh_data %>%
  nest(data = -probFractalDraw) %>% 
  mutate(
    fit = map(data, ~ glm(choiceLeft ~ scale(lottery_ev_diff) + scale(fractal_qv_diff), data = .x, family=binomial(link="logit"))),
    tidied = map(fit, tidy)
  ) %>% 
  unnest(tidied) %>%
  filter(term != "(Intercept)") %>%
  select(probFractalDraw, term, estimate, std.error)

qv_theta_mod = clean_beh_data %>%
  nest(data = -probFractalDraw) %>% 
  mutate(
    fit = map(data, ~ glm(choiceLeft ~ scale(lottery_ev_diff) + scale(fractal_qv_diff_theta), data = .x, family=binomial(link="logit"))),
    tidied = map(fit, tidy)
  ) %>% 
  unnest(tidied) %>%
  filter(term != "(Intercept)") %>%
  select(probFractalDraw, term, estimate, std.error)
```

True fractal probs regressor adjusted by subject specific theta's.

```{r}
true_v_theta_mod %>%
  rbind(true_v_mod %>% filter(term == "scale(fractal_prob_diff)")) %>%
  ggplot(aes(probFractalDraw, estimate, col=term, group=term))+
  geom_point()+
  geom_line()+
  geom_errorbar(aes(ymin = estimate - std.error, ymax = estimate +std.error), width=0.02)+
  geom_hline(aes(yintercept=0), linetype="dashed")+
  scale_color_manual(values = cbbPalette[3:1] )+
  theme(legend.position = "bottom")+
  labs(color="", y="Logit slope estimate", x="p(Fractal)")
```

What does the logit look like with the q values from this model

QVs have a larger effect than the true fractal values. But they still have less weight than lotteries at comparable relevance levels. Can the single linear distortion parameter correct for this?

```{r}
qv_mod %>%
  rbind(true_v_mod %>% filter(term == "scale(fractal_prob_diff)")) %>%
  ggplot(aes(probFractalDraw, estimate, col=term, group=term))+
  geom_point()+
  geom_line()+
  geom_errorbar(aes(ymin = estimate - std.error, ymax = estimate +std.error), width=0.02)+
  geom_hline(aes(yintercept=0), linetype="dashed")+
  scale_color_manual(values = cbbPalette[3:1] )+
  theme(legend.position = "bottom")+
  labs(color="", y="Logit slope estimate", x="p(Fractal)")
```
Only to a small degree. 

```{r}
qv_theta_mod %>%
  rbind(qv_mod %>% filter(term == "scale(fractal_qv_diff)")) %>%
  ggplot(aes(probFractalDraw, estimate, col=term, group=term))+
  geom_point()+
  geom_line()+
  geom_errorbar(aes(ymin = estimate - std.error, ymax = estimate +std.error), width=0.02)+
  geom_hline(aes(yintercept=0), linetype="dashed")+
  scale_color_manual(values = cbbPalette[3:1] )+
  theme(legend.position = "bottom")+
  labs(color="", y="Logit slope estimate", x="p(Fractal)")
```

### RT posterior predictive check

How about the RT patterns? Since this is an DDRL, can you get the inverse U and other RT patterns? No.

```{r}
clean_beh_data = clean_beh_data %>%
  mutate(wpFrac = theta*probFractalDraw,
         leftLotteryEV = lotteryValue*lotteryProb,
         rightLotteryEV = referenceValue*referenceProb,
         valLeftBundle = (1-probFractalDraw)*leftLotteryEV + wpFrac*leftQValue,
         valRightBundle = (1-probFractalDraw)*rightLotteryEV + wpFrac*rightQValue,
         trial_drift = d * (valLeftBundle - valRightBundle))

sim_data = data.frame()
library(RWiener)
for(i in 1:nrow(clean_beh_data)){
  
  # This will have to be for each trial bc the delta depends on the val difference in that trial
  # First generated the Q Values for all trials using the mean a posterior alpha for each subject
  
  sim_trial = rwiener(n =1, alpha = 2, tau = .1, beta = .5, delta = clean_beh_data$trial_drift[i])
  
  sim_trial$subnum = clean_beh_data$subnum[i]
  
  if(i == 1){
    sim_data = sim_trial
  } else{
    sim_data = rbind(sim_data, sim_trial)
  }
}
```

Doesn't recap the RT patterns.

**But there a lot of assumptions on the WFTP here with a fixed NDT, threshold and bias. Fitting those might do a much better job in capturing the RT patterns.**

```{r}
sim_data %>%
  cbind(clean_beh_data %>%
          select(probFractalDraw)) %>%
  mutate(logRt = log(q),
         probFractalDraw = as.factor(probFractalDraw)) %>%
  group_by(probFractalDraw) %>%
  summarise(meanLogRt = mean(logRt),
            semLogRt = sd(logRt)/sqrt(n())) %>%
  ggplot(aes(probFractalDraw, meanLogRt))+
  geom_point()+
  geom_errorbar(aes(ymin=meanLogRt - semLogRt, ymax=meanLogRt + semLogRt), width=.2)+
  labs(y="mean log RT", x="p(Fractal)")
```

## Theta sans DDM

How about estimating this linear distortion parameter $\theta$ from RL alone?

```{r}
par_ests_ddrl = par_ests
g_par_ests_ddrl = g_par_ests
source(paste0(helpers_path, 'rlModels/fit_rl_hierarchical_oneParamAsymmLinearProbDistortion_rpeBoth.R'))
par_ests_rl = par_ests
g_par_ests_rl = g_par_ests
```

Estimates overlapping for half of the subjects. When they don't overlap the RL only theta estimates are lower. What do lower theta's mean? Smaller theta --> more distortion (w(p) = p*theta). Theta closer to 1 is less distortion.

```{r}
par_ests_ddrl %>%
  mutate(fit_type = "ddrl") %>%
  select(-iter) %>%
  rbind(par_ests_rl %>% 
          mutate(fit_type = "rl") %>%
          select(-logLik)) %>%
  filter(par == "theta") %>%
  ggplot(aes(value, fill=fit_type))+
  geom_histogram(bins=30, alpha=.5, position="identity")+
  facet_wrap(~subnum)+
  theme(panel.grid = element_blank())

```

Do you get the same QV correction with the theta from RL only model? 

```{r}
rm(clean_beh_data)
source(paste0(helpers_path,'01_clean_behavioral_data.R'))

clean_beh_data = par_ests %>%
  group_by(subnum, par) %>%
  summarise(est = mean(value), .groups='keep') %>%
  spread(par, est) %>%
  left_join(clean_beh_data, by='subnum')

## Add Q values of fractals to each trial
clean_beh_data = clean_beh_data %>%
  group_by(subnum) %>%
  do(get_qvals(., model_name="original")) %>%
  ungroup()

#distorted values computed based on ddrlModels/fit_model1d.R
# opt_val[1] = ((1-trial_pFrac[i, t]) * ev_left[i, t]) + (w_pi * qv[1]);
clean_beh_data = clean_beh_data %>%
  mutate(rightLotteryEV = referenceProb * referenceValue,
         leftLotteryEV = lotteryValue * lotteryProb,
         lottery_ev_diff = leftLotteryEV - rightLotteryEV,
         fractal_qv_diff = leftQValue - rightQValue,
         fractal_prob_diff = fractalLeftProb - fractalRightProb,
         fractal_qv_diff_theta = theta*fractal_qv_diff, #note that these are subject specific thetas but the logits below model all data together
         fractal_prob_diff_theta = theta*fractal_prob_diff) 

# rm(fit, g_par_est, par_ests)

qv_mod = clean_beh_data %>%
  nest(data = -probFractalDraw) %>% 
  mutate(
    fit = map(data, ~ glm(choiceLeft ~ scale(lottery_ev_diff) + scale(fractal_qv_diff), data = .x, family=binomial(link="logit"))),
    tidied = map(fit, tidy)
  ) %>% 
  unnest(tidied) %>%
  filter(term != "(Intercept)") %>%
  select(probFractalDraw, term, estimate, std.error)

qv_theta_mod = clean_beh_data %>%
  nest(data = -probFractalDraw) %>% 
  mutate(
    fit = map(data, ~ glm(choiceLeft ~ scale(lottery_ev_diff) + scale(fractal_qv_diff_theta), data = .x, family=binomial(link="logit"))),
    tidied = map(fit, tidy)
  ) %>% 
  unnest(tidied) %>%
  filter(term != "(Intercept)") %>%
  select(probFractalDraw, term, estimate, std.error)
```

The correction is larger than the DDRL estimates because the estimated theta's from RL alone suggest more distortion of the QValues.

```{r}
qv_theta_mod %>%
  rbind(qv_mod %>% filter(term == "scale(fractal_qv_diff)")) %>%
  ggplot(aes(probFractalDraw, estimate, col=term, group=term))+
  geom_point()+
  geom_line()+
  geom_errorbar(aes(ymin = estimate - std.error, ymax = estimate +std.error), width=0.02)+
  geom_hline(aes(yintercept=0), linetype="dashed")+
  scale_color_manual(values = cbbPalette[3:1] )+
  theme(legend.position = "bottom")+
  labs(color="", y="Logit slope estimate", x="p(Fractal)")
```

### Symmmetric distortion

In the models above $\theta$ was parameterized asymmetrically only distortion the relevance of the fractal QValues. This might be handicapping both the recovery of the parameter and as a result its ability to capture the group level distortions in the logits. We can parameterize the model more symmetrically by allowing it to distort the relevance of both attributes with or without normalizing (i.e. making sure the sum of the relevances adds up 1). 

What I call double symmetry here is parameterized as follows to ensure normalization of attribute relevance.

$$w(pFrac) = \theta * pFrac$$
$$w(pLott) = (1-\theta) * (1-pFrac)$$
$$V_i = w(pLott)EV_{i} + w(pFrac)QV_{i}$$

Below is a comparison of the recovered theta distributions for each subjects for three different models
1. ddrl with asymmetric distortion  
2. rl with symmetric distortion but no normalization  
3. rl with symmetric distortion and normalization  

The estimates are vastly differently for each subject. Of course, one can get some sort of correction on the logit slopes using any one of these estimates. But the recapitulation of the logit pattern is not an indication of fit to data.

*Ultimately, the question of whether a single parameter can capture the discrepancy between the logit slopes of the attributes should come down to determining the best fitting model for all behavior (RT and choice) and then checking whether logits of value estimates distorted by (subject-specific) parameters from this model can result in the same pattern*

```{r}
source(paste0(helpers_path, 'rlModels/fit_rl_hierarchical_oneParamDoubleSymmLinearProbDistortion_rpeBoth.R'))
par_ests_rl_double = par_ests
g_par_ests_rl_double = g_par_ests

source(paste0(helpers_path, 'rlModels/fit_rl_hierarchical_oneParamSymmLinearProbDistortion_rpeBoth.R'))
par_ests_rl = par_ests
g_par_ests_rl = g_par_ests


par_ests_rl_double %>%
  mutate(fit_type = "rl double") %>%
  rbind(par_ests_rl %>% 
          mutate(fit_type = "rl")) %>%
  rbind(par_ests_ddrl %>%
          mutate(fit_type = "ddrl") %>%
          rename(logLik = iter)) %>%
  filter(par == "theta") %>%
  ggplot(aes(value, fill=fit_type))+
  geom_histogram(bins=30, alpha=.5, position="identity")+
  facet_wrap(~subnum)+
  theme(panel.grid = element_blank())


```

```{r}
rm(clean_beh_data)
source(paste0(helpers_path,'01_clean_behavioral_data.R'))

clean_beh_data = par_ests_rl_double %>%
  group_by(subnum, par) %>%
  summarise(est = mean(value), .groups='keep') %>%
  spread(par, est) %>%
  left_join(clean_beh_data, by='subnum')

## Add Q values of fractals to each trial
clean_beh_data = clean_beh_data %>%
  group_by(subnum) %>%
  do(get_qvals(., model_name="original")) %>%
  ungroup()

#distorted values computed based on ddrlModels/fit_model1d.R
clean_beh_data = clean_beh_data %>%
  mutate(rightLotteryEV = referenceProb * referenceValue,
         leftLotteryEV = lotteryValue * lotteryProb,
         lottery_ev_diff = leftLotteryEV - rightLotteryEV,
         fractal_qv_diff = leftQValue - rightQValue,
         fractal_qv_diff_theta = theta*fractal_qv_diff, #note that these are subject specific thetas but the logits below model all data together
         lottery_ev_diff_theta = (1-theta)*lottery_ev_diff) 

# rm(fit, g_par_est, par_ests)

qv_mod = clean_beh_data %>%
  nest(data = -probFractalDraw) %>% 
  mutate(
    fit = map(data, ~ glm(choiceLeft ~ scale(lottery_ev_diff) + scale(fractal_qv_diff), data = .x, family=binomial(link="logit"))),
    tidied = map(fit, tidy)
  ) %>% 
  unnest(tidied) %>%
  filter(term != "(Intercept)") %>%
  select(probFractalDraw, term, estimate, std.error)

qv_theta_mod = clean_beh_data %>%
  nest(data = -probFractalDraw) %>% 
  mutate(
    fit = map(data, ~ glm(choiceLeft ~ scale(lottery_ev_diff_theta) + scale(fractal_qv_diff_theta), data = .x, family=binomial(link="logit"))),
    tidied = map(fit, tidy)
  ) %>% 
  unnest(tidied) %>%
  filter(term != "(Intercept)") %>%
  select(probFractalDraw, term, estimate, std.error)
```

```{r}
qv_theta_mod %>%
  # rbind(qv_mod %>% filter(term == "scale(fractal_qv_diff)")) %>%
  rbind(qv_mod) %>%
  ggplot(aes(probFractalDraw, estimate, col=term, group=term))+
  geom_point()+
  geom_line()+
  geom_errorbar(aes(ymin = estimate - std.error, ymax = estimate +std.error), width=0.02)+
  geom_hline(aes(yintercept=0), linetype="dashed")+
  scale_color_manual(values = cbbPalette[4:1] )+
  theme(legend.position = "bottom")+
  labs(color="", y="Logit slope estimate", x="p(Fractal)")
```

### One parameter non-linear distortion

Above we saw that a single linear relevance distortion parameter from various models can correct for the difference in the logit slopes to different degrees.  

Here we check if this specific to a linear distortion or whether a single non linear distortion parameter can produce qualitatively similar effects.  

So the question is: Do you get a similar QV slope correction with the d from RL only model? You can get some sort of correction but these qualitative comparisons are not conclusive.  

Still, this is not an indication of the fit of this model to data. This only suggests that the correction/best fitting models does not need to be limited to a linear distortion.   

```{r}
source(paste0(helpers_path, 'rlModels/fit_rl_hierarchical_oneParamAsymmNonLinearProbDistortion_rpeBoth.R'))
```

```{r}
rm(clean_beh_data)
source(paste0(helpers_path,'01_clean_behavioral_data.R'))
# Add mean posterior estimates to clean_beh_data
clean_beh_data = par_ests %>%
  group_by(subnum, par) %>%
  summarise(est = mean(value), .groups='keep') %>%
  spread(par, est) %>%
  left_join(clean_beh_data, by='subnum')

## Add Q values of fractals to each trial
clean_beh_data = clean_beh_data %>%
  group_by(subnum) %>%
  do(get_qvals(., model_name="original")) %>%
  ungroup()
```

```{r}
rm(clean_beh_data)
source(paste0(helpers_path,'01_clean_behavioral_data.R'))

clean_beh_data = par_ests %>%
  group_by(subnum, par) %>%
  summarise(est = mean(value), .groups='keep') %>%
  spread(par, est) %>%
  left_join(clean_beh_data, by='subnum')

## Add Q values of fractals to each trial
clean_beh_data = clean_beh_data %>%
  group_by(subnum) %>%
  do(get_qvals(., model_name="original")) %>%
  ungroup()

#distorted values computed based on ddrlModels/fit_model1d.R
clean_beh_data = clean_beh_data %>%
  mutate(rightLotteryEV = referenceProb * referenceValue,
         leftLotteryEV = lotteryValue * lotteryProb,
         lottery_ev_diff = leftLotteryEV - rightLotteryEV,
         fractal_qv_diff = leftQValue - rightQValue,
         fractal_qv_diff_delta = delta*fractal_qv_diff) #note that these are subject specific thetas but the logits below model all data together

# rm(fit, g_par_est, par_ests)

qv_mod = clean_beh_data %>%
  nest(data = -probFractalDraw) %>% 
  mutate(
    fit = map(data, ~ glm(choiceLeft ~ scale(lottery_ev_diff) + scale(fractal_qv_diff), data = .x, family=binomial(link="logit"))),
    tidied = map(fit, tidy)
  ) %>% 
  unnest(tidied) %>%
  filter(term != "(Intercept)") %>%
  select(probFractalDraw, term, estimate, std.error)

qv_delta_mod = clean_beh_data %>%
  nest(data = -probFractalDraw) %>% 
  mutate(
    fit = map(data, ~ glm(choiceLeft ~ scale(lottery_ev_diff) + scale(fractal_qv_diff_delta), data = .x, family=binomial(link="logit"))),
    tidied = map(fit, tidy)
  ) %>% 
  unnest(tidied) %>%
  filter(term != "(Intercept)") %>%
  select(probFractalDraw, term, estimate, std.error)
```


```{r}
qv_delta_mod %>%
  rbind(qv_mod %>% filter(term == "scale(fractal_qv_diff)")) %>%
  # rbind(qv_mod) %>%
  ggplot(aes(probFractalDraw, estimate, col=term, group=term))+
  geom_point()+
  geom_line()+
  geom_errorbar(aes(ymin = estimate - std.error, ymax = estimate +std.error), width=0.02)+
  geom_hline(aes(yintercept=0), linetype="dashed")+
  scale_color_manual(values = cbbPalette[3:1] )+
  theme(legend.position = "bottom")+
  labs(color="", y="Logit slope estimate", x="p(Fractal)")
```

**Unbounded distortion** - is examined as part of RL only models in NB19_RevisitingRLModels with two Parameter linear probability distortion where the slope parameter is allowed to be >1.
