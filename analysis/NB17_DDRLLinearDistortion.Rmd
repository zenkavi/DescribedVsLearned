---
title: 'Experience vs. description based decision-making project: Hierarchical DDRL with linear distortion'
output:
  html_document:
    toc: yes
    toc_float: yes
    code_folding: 'hide'
  pdf_document:
    toc: yes
---

# Set up

```{r include=FALSE}
library(tidyverse)
library(here)
library(broom)
cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

helpers_path = here('analysis/helpers/')
theme_set(theme_bw())
```

# Linear distortion

Motivation: the choice logit slopes showed choice depended on the fractal value difference a lot less than the lottery value difference.
Can a linear probability distortion parameter capture the choice effects?
Would you see logit slopes crossing appropriately at .5 if instead of a logit of choice ~ evdiff + qvdiff you had a logit of choice ~ evdiff + theta*qvdiff

**Model 1d:** 
d - drift rate scaler for value difference  
s - variance of evidence accumulation distribution  
alpha - learning rate  
theta - linear distortion  
With subject specific sigma added (by distorting drift rate and boundary separation as described in Wabersich and Vandekerckhove)

What are the parameter distributions?

```{r message=FALSE}
source(paste0(helpers_path, 'ddrlModels/fit_model1d.R'))
```

## Group posteriors

Note that `s` variance of evidence accumulation distribution is still centered around 1.

```{r}
g_par_ests %>%
  ggplot(aes(value))+
  geom_histogram(bins=30, alpha=.5, position="identity")+
  facet_wrap(~key, scales='free')+
  theme(panel.grid = element_blank())+
  xlab("")+
  ylab("")

```

Covariance between parameters

```{r}
tmp = g_par_ests %>%
  group_by(key) %>%
  mutate(iter = 1:n()) %>%
  group_by(iter) %>%
  spread(key, value) %>%
  ungroup() %>%
  select(-iter)

round(cor(tmp), 3)  
```

```{r}
tmp = g_par_ests %>%
  filter(key == "g_theta") %>%
  summarise(mean_theta  = mean(value))
mean_theta = tmp$mean_theta
```

## Logit slope adjustment

Can the group theta bring the QV slope up sufficiently? Gets close but...

```{r}
tmp = clean_beh_data %>%
  mutate(EVRight = referenceProb * referenceValue,
         EVLeft = lotteryValue * lotteryProb) %>%
  select(EVLeft, EVRight, fractalLeftProb, fractalRightProb, probFractalDraw, choiceLeft) %>%
  mutate(EVDiff = scale(EVLeft - EVRight), 
         fractalDiff = scale(fractalLeftProb - fractalRightProb)) %>%
  nest(data = -probFractalDraw) %>% 
  mutate(
    fit = map(data, ~ glm(choiceLeft ~ EVDiff + fractalDiff, data = .x, family=binomial(link="logit"))),
    tidied = map(fit, tidy)
  ) %>% 
  unnest(tidied) %>%
  filter(term != "(Intercept)") %>%
  select(probFractalDraw, term, estimate, std.error)
```

... still doesn't cross at .5 or reach the same weight at comparable relevance levels.

These models use the true values as the IVs though and the subjects do not know these. The believed values (QValues) might have a bigger influence on choice.

```{r}
clean_beh_data %>%
  mutate(EVRight = referenceProb * referenceValue,
         EVLeft = lotteryValue * lotteryProb) %>%
  select(EVLeft, EVRight, fractalLeftProb, fractalRightProb, probFractalDraw, choiceLeft) %>%
  mutate(EVDiff = scale(EVLeft - EVRight), 
         fractalDiffXtheta = scale(fractalLeftProb - fractalRightProb)*(mean_theta) ) %>%
  nest(data = -probFractalDraw) %>% 
  mutate(
    fit = map(data, ~ glm(choiceLeft ~ EVDiff + fractalDiffXtheta, data = .x, family=binomial(link="logit"))),
    tidied = map(fit, tidy)
  ) %>% 
  unnest(tidied) %>%
  filter(term != "(Intercept)") %>%
  select(probFractalDraw, term, estimate, std.error) %>%
  rbind(tmp %>% filter(term == "fractalDiff")) %>%
  ggplot(aes(probFractalDraw, estimate, col=term, group=term))+
  geom_point()+
  geom_line()+
  geom_errorbar(aes(ymin = estimate - std.error, ymax = estimate +std.error), width=0.02)+
  geom_hline(aes(yintercept=0), linetype="dashed")+
  scale_color_manual(values = c(cbbPalette[2], cbbPalette[1], cbbPalette[3]) )+
  theme(legend.position = "bottom")+
  labs(color="", y="Logit slope estimate", x="p(Fractal)")
```

What does the logit look like with the q values from this model

```{r}
source(paste0(helpers_path, 'get_qvals.R'))

# Add mean posterior estimates to clean_beh_data
clean_beh_data = par_ests %>%
  group_by(subnum, par) %>%
  summarise(est = mean(value), .groups='keep') %>%
  spread(par, est) %>%
  left_join(clean_beh_data, by='subnum')

## Add Q values of fractals to each trial
clean_beh_data = clean_beh_data %>%
  group_by(subnum) %>%
  do(get_qvals(., model_name="original")) %>%
  ungroup()
```

Indeed the QVs have a larger effect than the true fractal values. But they still have less weight than lotteries at comparable relevance levels. Can the single linear distortion parameter correct for this?

```{r}
clean_beh_data %>%
  mutate(EVRight = referenceProb * referenceValue,
         EVLeft = lotteryValue * lotteryProb) %>%
  select(EVLeft, EVRight, leftQValue, rightQValue, probFractalDraw, choiceLeft) %>%
  mutate(EVDiff = scale(EVLeft - EVRight), 
         QVDiff = scale(leftQValue - rightQValue)) %>%
  nest(data = -probFractalDraw) %>% 
  mutate(
    fit = map(data, ~ glm(choiceLeft ~ EVDiff + QVDiff, data = .x, family=binomial(link="logit"))),
    tidied = map(fit, tidy)
  ) %>% 
  unnest(tidied) %>%
  filter(term != "(Intercept)") %>%
  select(probFractalDraw, term, estimate, std.error) %>%
  rbind(tmp %>% filter(term == "fractalDiff")) %>%
  ggplot(aes(probFractalDraw, estimate, col=term, group=term))+
  geom_point()+
  geom_line()+
  geom_errorbar(aes(ymin = estimate - std.error, ymax = estimate +std.error), width=0.02)+
  geom_hline(aes(yintercept=0), linetype="dashed")+
  scale_color_manual(values = c(cbbPalette[2], cbbPalette[1], cbbPalette[3]) )+
  theme(legend.position = "bottom")+
  labs(color="", y="Logit slope estimate", x="p(Fractal)")
```

It can to a large degree. Now for cases where fractals are more relevant the fractal weights are similar to littery weights at comparable levels of relevance. Still, the switch does not happen at .5.

```{r}
clean_beh_data %>%
  mutate(EVRight = referenceProb * referenceValue,
         EVLeft = lotteryValue * lotteryProb) %>%
  select(EVLeft, EVRight, leftQValue, rightQValue, probFractalDraw, choiceLeft) %>%
  mutate(EVDiff = scale(EVLeft - EVRight), 
         QVDiff_theta = scale(leftQValue - rightQValue)*(mean_theta)) %>%
  nest(data = -probFractalDraw) %>% 
  mutate(
    fit = map(data, ~ glm(choiceLeft ~ EVDiff + QVDiff_theta, data = .x, family=binomial(link="logit"))),
    tidied = map(fit, tidy)
  ) %>% 
  unnest(tidied) %>%
  filter(term != "(Intercept)") %>%
  select(probFractalDraw, term, estimate, std.error) %>%
  rbind(tmp %>% filter(term == "fractalDiff")) %>%
  ggplot(aes(probFractalDraw, estimate, col=term, group=term))+
  geom_point()+
  geom_line()+
  geom_errorbar(aes(ymin = estimate - std.error, ymax = estimate +std.error), width=0.02)+
  geom_hline(aes(yintercept=0), linetype="dashed")+
  scale_color_manual(values = c(cbbPalette[2], cbbPalette[1], cbbPalette[3]) )+
  theme(legend.position = "bottom")+
  labs(color="", y="Logit slope estimate", x="p(Fractal)")
```
Flip this for easier visual comparison

```{r}
clean_beh_data %>%
  mutate(EVRight = referenceProb * referenceValue,
         EVLeft = lotteryValue * lotteryProb) %>%
  select(EVLeft, EVRight, leftQValue, rightQValue, probFractalDraw, choiceLeft) %>%
  mutate(EVDiff = scale(EVLeft - EVRight), 
         QVDiff_theta = scale(leftQValue - rightQValue)*(mean_theta)) %>%
  nest(data = -probFractalDraw) %>% 
  mutate(
    fit = map(data, ~ glm(choiceLeft ~ EVDiff + QVDiff_theta, data = .x, family=binomial(link="logit"))),
    tidied = map(fit, tidy)
  ) %>% 
  unnest(tidied) %>%
  filter(term != "(Intercept)") %>%
  select(probFractalDraw, term, estimate, std.error) %>%
  rbind(tmp %>% filter(term == "fractalDiff")) %>%
  mutate(probFractalDraw = as.numeric(as.character(probFractalDraw)),
         attributeRelevance = ifelse(term == "EVDiff", 1-probFractalDraw, probFractalDraw),
         attributeRelevance = as.factor(attributeRelevance)) %>%
  ggplot(aes(attributeRelevance, estimate, col=term, group=term))+
  geom_point()+
  geom_line()+
  geom_errorbar(aes(ymin = estimate - std.error, ymax = estimate +std.error), width=0.2)+
  geom_hline(aes(yintercept=0), linetype="dashed")+
  scale_color_manual(values = c(cbbPalette[2], cbbPalette[1], cbbPalette[3]) )+
  theme(legend.position = "bottom")+
  labs(color="", y="Logit slope estimate", x="Attribute relevance")
```
## RT posterior predictive check

How about the RT patterns? Since this is an DDRL, can you get the inverse U and other RT patterns? No.

```{r}
clean_beh_data = clean_beh_data %>%
  mutate(wpFrac = theta*probFractalDraw,
         leftLotteryEV = lotteryValue*lotteryProb,
         rightLotteryEV = referenceValue*referenceProb,
         valLeftBundle = (1-probFractalDraw)*leftLotteryEV + wpFrac*leftQValue,
         valRightBundle = (1-probFractalDraw)*rightLotteryEV + wpFrac*rightQValue,
         trial_drift = d * (valLeftBundle - valRightBundle))

sim_data = data.frame()
library(RWiener)
for(i in 1:nrow(clean_beh_data)){
  
  # This will have to be for each trial bc the delta depends on the val difference in that trial
  # First generated the Q Values for all trials using the mean a posterior alpha for each subject
  
  sim_trial = rwiener(n =1, alpha = 2, tau = .1, beta = .5, delta = clean_beh_data$trial_drift[i])
  
  sim_trial$subnum = clean_beh_data$subnum[i]
  
  if(i == 1){
    sim_data = sim_trial
  } else{
    sim_data = rbind(sim_data, sim_trial)
  }
}
```

Doesn't recap the RT patterns.

```{r}
sim_data %>%
  cbind(clean_beh_data %>%
          select(probFractalDraw)) %>%
  mutate(logRt = log(q),
         probFractalDraw = as.factor(probFractalDraw)) %>%
  group_by(probFractalDraw) %>%
  summarise(meanLogRt = mean(logRt),
            semLogRt = sd(logRt)/sqrt(n())) %>%
  ggplot(aes(probFractalDraw, meanLogRt))+
  geom_point()+
  geom_errorbar(aes(ymin=meanLogRt - semLogRt, ymax=meanLogRt + semLogRt), width=.2)+
  labs(y="mean log RT", x="p(Fractal)")
```

## Theta estimate sans DDM

How about estimating this linear distortion parameter from RL alone?

```{r}
par_ests_ddrl = par_ests
g_par_ests_ddrl = g_par_ests
source(paste0(helpers_path, 'rlModels/fit_rl_hierarchical_oneParamAsymmLinearProbDistortion_rpeBoth.R'))
par_ests_rl = par_ests
g_par_ests_rl = g_par_ests
```

Estimates overlapping for half of the subjects. I'll stick to estimating this parameter from RL alone since DDRL is a more complicated model that fails the posterior predictive check for half the data it's modeling (RTs).

```{r}
par_ests_ddrl %>%
  mutate(fit_type = "ddrl") %>%
  select(-iter) %>%
  rbind(par_ests_rl %>% 
          mutate(fit_type = "rl") %>%
          select(-logLik)) %>%
  filter(par == "theta") %>%
  ggplot(aes(value, fill=fit_type))+
  geom_histogram(bins=30, alpha=.5, position="identity")+
  facet_wrap(~subnum)+
  theme(panel.grid = element_blank())

```

Do you get the same QV correction with the theta from RL only model? 

```{r}
rm(clean_beh_data)
source(paste0(helpers_path,'01_clean_behavioral_data.R'))
# Add mean posterior estimates to clean_beh_data
clean_beh_data = par_ests %>%
  group_by(subnum, par) %>%
  summarise(est = mean(value), .groups='keep') %>%
  spread(par, est) %>%
  left_join(clean_beh_data, by='subnum')

## Add Q values of fractals to each trial
clean_beh_data = clean_beh_data %>%
  group_by(subnum) %>%
  do(get_qvals(., model_name="original")) %>%
  ungroup()
```

Yes, to an even larger degree.

```{r}
tmp2 = g_par_ests %>%
  filter(key == "g_theta") %>%
  summarise(mean_theta  = mean(value))
mean_theta = tmp2$mean_theta
rm(tmp2)

clean_beh_data %>%
  mutate(EVRight = referenceProb * referenceValue,
         EVLeft = lotteryValue * lotteryProb) %>%
  select(EVLeft, EVRight, leftQValue, rightQValue, probFractalDraw, choiceLeft) %>%
  mutate(EVDiff = scale(EVLeft - EVRight), 
         QVDiffXtheta = scale(leftQValue - rightQValue)*(mean_theta)) %>%
  nest(data = -probFractalDraw) %>% 
  mutate(
    fit = map(data, ~ glm(choiceLeft ~ EVDiff + QVDiffXtheta, data = .x, family=binomial(link="logit"))),
    tidied = map(fit, tidy)
  ) %>% 
  unnest(tidied) %>%
  filter(term != "(Intercept)") %>%
  select(probFractalDraw, term, estimate, std.error) %>%
  rbind(tmp %>% filter(term == "fractalDiff")) %>%
  mutate(probFractalDraw = as.numeric(as.character(probFractalDraw)),
         attributeRelevance = ifelse(term == "EVDiff", 1-probFractalDraw, probFractalDraw),
         attributeRelevance = as.factor(attributeRelevance)) %>%
  ggplot(aes(attributeRelevance, estimate, col=term, group=term))+
  geom_point()+
  geom_line()+
  geom_errorbar(aes(ymin = estimate - std.error, ymax = estimate +std.error), width=0.2)+
  geom_hline(aes(yintercept=0), linetype="dashed")+
  scale_color_manual(values = c(cbbPalette[2], cbbPalette[1], cbbPalette[3]) )+
  theme(legend.position = "bottom")+
  labs(color="", y="Logit slope estimate", x="Attribute relevance")
```

## Theta with double symmetry

$$w(pFrac) = \theta * pFrac$$
$$w(pLott) = (1-\theta) * (1-pFrac)$$
$$V_i = w(pLott)EV_{i} + w(pFrac)QV_{i}$$

```{r}
source(paste0(helpers_path, 'rlModels/fit_rl_hierarchical_oneParamDoubleSymmLinearProbDistortion_rpeBoth.R'))
par_ests_rl_double = par_ests
g_par_ests_rl_double = g_par_ests

source(paste0(helpers_path, 'rlModels/fit_rl_hierarchical_oneParamSymmLinearProbDistortion_rpeBoth.R'))
par_ests_rl = par_ests
g_par_ests_rl = g_par_ests


par_ests_rl_double %>%
  mutate(fit_type = "rl double") %>%
  rbind(par_ests_rl %>% 
          mutate(fit_type = "rl")) %>%
  filter(par == "theta") %>%
  ggplot(aes(value, fill=fit_type))+
  geom_histogram(bins=30, alpha=.5, position="identity")+
  facet_wrap(~subnum)+
  theme(panel.grid = element_blank())


```

```{r}
rm(clean_beh_data)
source(paste0(helpers_path,'01_clean_behavioral_data.R'))
# Add mean posterior estimates to clean_beh_data
clean_beh_data = par_ests_rl_double %>%
  group_by(subnum, par) %>%
  summarise(est = mean(value), .groups='keep') %>%
  spread(par, est) %>%
  left_join(clean_beh_data, by='subnum')

## Add Q values of fractals to each trial
clean_beh_data = clean_beh_data %>%
  group_by(subnum) %>%
  do(get_qvals(., model_name="original")) %>%
  ungroup()
```

```{r}
clean_beh_data %>%
  mutate(EVRight = referenceProb * referenceValue,
         EVLeft = lotteryValue * lotteryProb) %>%
  # select(EVLeft, EVRight, leftQValue, rightQValue, probFractalDraw, choiceLeft, theta) %>%
  mutate(EVDiff_theta = scale(EVLeft - EVRight)*(1-theta), 
         QVDiff_theta = scale(leftQValue - rightQValue)*(theta)) %>%
  nest(data = -probFractalDraw) %>% 
  mutate(
    fit = map(data, ~ glm(choiceLeft ~ EVDiff_theta + QVDiff_theta, data = .x, family=binomial(link="logit"))),
    tidied = map(fit, tidy)
  ) %>% 
  unnest(tidied) %>%
  filter(term != "(Intercept)") %>%
  select(probFractalDraw, term, estimate, std.error) %>%
  # rbind(tmp %>% filter(term == "fractalDiff")) %>%
 mutate(probFractalDraw = as.numeric(as.character(probFractalDraw)),
         attributeRelevance = ifelse(term == "EVDiff_theta", 1-probFractalDraw, probFractalDraw),
         attributeRelevance = as.factor(attributeRelevance)) %>%
  ggplot(aes(attributeRelevance, estimate, col=term, group=term))+
   # ggplot(aes(probFractalDraw, estimate, col=term, group=term))+
  geom_point()+
  geom_line()+
    geom_errorbar(aes(ymin = estimate - std.error, ymax = estimate +std.error), width=0.2)+
  # geom_errorbar(aes(ymin = estimate - std.error, ymax = estimate +std.error), width=0.02)+
  geom_hline(aes(yintercept=0), linetype="dashed")+
  scale_color_manual(values = c(cbbPalette[2], cbbPalette[1], cbbPalette[3]) )+
  theme(legend.position = "bottom")+
  labs(color="", y="Logit slope estimate", x="Attribute relevance")
```


## One parameter non-linear distortion

Do you get a similar QV slope correction with the d from RL only model?

```{r}
source(paste0(helpers_path, 'rlModels/fit_rl_hierarchical_oneParamAsymmNonLinearProbDistortion_rpeBoth.R'))
```

```{r}
rm(clean_beh_data)
source(paste0(helpers_path,'01_clean_behavioral_data.R'))
# Add mean posterior estimates to clean_beh_data
clean_beh_data = par_ests %>%
  group_by(subnum, par) %>%
  summarise(est = mean(value), .groups='keep') %>%
  spread(par, est) %>%
  left_join(clean_beh_data, by='subnum')

## Add Q values of fractals to each trial
clean_beh_data = clean_beh_data %>%
  group_by(subnum) %>%
  do(get_qvals(., model_name="original")) %>%
  ungroup()
```

```{r}
tmp2 = g_par_ests %>%
  filter(key == "g_delta") %>%
  summarise(mean_delta  = mean(value))
mean_delta = tmp2$mean_delta
rm(tmp2)

clean_beh_data %>%
  mutate(EVRight = referenceProb * referenceValue,
         EVLeft = lotteryValue * lotteryProb) %>%
  select(EVLeft, EVRight, leftQValue, rightQValue, probFractalDraw, choiceLeft) %>%
  mutate(EVDiff = scale(EVLeft - EVRight), 
         QVDiffXd = scale(leftQValue - rightQValue)*(mean_delta)) %>%
  nest(data = -probFractalDraw) %>% 
  mutate(
    fit = map(data, ~ glm(choiceLeft ~ EVDiff + QVDiffXd, data = .x, family=binomial(link="logit"))),
    tidied = map(fit, tidy)
  ) %>% 
  unnest(tidied) %>%
  filter(term != "(Intercept)") %>%
  select(probFractalDraw, term, estimate, std.error) %>%
  rbind(tmp %>% filter(term == "fractalDiff")) %>%
  mutate(probFractalDraw = as.numeric(as.character(probFractalDraw)),
         attributeRelevance = ifelse(term == "EVDiff", 1-probFractalDraw, probFractalDraw),
         attributeRelevance = as.factor(attributeRelevance)) %>%
  ggplot(aes(attributeRelevance, estimate, col=term, group=term))+
  geom_point()+
  geom_line()+
  geom_errorbar(aes(ymin = estimate - std.error, ymax = estimate +std.error), width=0.2)+
  geom_hline(aes(yintercept=0), linetype="dashed")+
  scale_color_manual(values = c(cbbPalette[2], cbbPalette[1], cbbPalette[3]) )+
  theme(legend.position = "bottom")+
  labs(color="", y="Logit slope estimate", x="Attribute relevance")
```

**Unbounded distortion** - is examined as part of RL only models in NB19_RevisitingRLModels with two Parameter linear probability distortion where the slope parameter is allowed to be >1.
