---
title: 'Experience vs. description based decision-making project: Revisiting RL models'
output:
  html_document:
    toc: yes
    toc_float: yes
    code_folding: 'hide'
  pdf_document:
    toc: yes
---

Set up

```{r include=FALSE}
library(broom)
library(tidyverse)
theme_set(theme_bw())
library(here)
cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
helpers_path = here('analysis/helpers/')
source(paste0(helpers_path,'01_clean_behavioral_data.R'))
set.seed(2038573)
```

# Modeling approach

Motivation for modeling RL and DDM separately:

- Recovering learning rates and distortion parameters with hierarchical DDRL (Stan) has been problematic (see [here](https://zenkavi.github.io/DescribedVsLearned_beh/outputs/NB12_DDMRLRecovery.html)). Recovery of parameters that have smaller effects on RT distributions seems to be problematic in the literature as well (see learning rate recovery in Figure 6 of Pedersen, Frank, Biele, 2016 and Table 2 of Shahar et al. 2019)  
*To conclude this in our dataset need to do a proper alpha and prob distortion parameter recoverability exercise comparing RL to DDRL*  

- Hierarchical DDRL's (Stan) have not done a good job in capturing RL patterns (see [here](https://zenkavi.github.io/DescribedVsLearned_beh/outputs/NB15_DDRLStan.html#Posterior_predictive_checks13)) but this might be due to strict assumptions imposed on the one DDRl model tried so far (fixed boundary separation, ndt and bias). Relaxing these and possibly even making them trial-dependent (getting closer to a full DDM) might improve their ability to replicate the observed patterns.

- Non-hierarchical (optim) models estimated very similar parameter for RL-->DDM and DDRL (see [here](https://zenkavi.github.io/DescribedVsLearned_beh/outputs/NB13_DDMAndDDMRLSubjectFitsWOptim.html#Comparison_of_ddrl_alpha%E2%80%99s_to_hierarchical_rl_alpha%E2%80%99s)) for both drift rates and sigmas as well as alpha's (which for RL->DDMs were from hierarchical estimates). Note, that this tested only one (and not the best fitting) prob distortion.*Still, if you can't find a flexible way to model hDDMs then you could fit non-hierarchical DDMs*

- Simulating different DDM's is a lot easier and faster when not hierarchical BUT *read Annis, Miller and Palmeri (2016) and Ahn, Haines, Zhang (2017) for inspiration on how to implement more complicated hDDMs in Stan.*  

- Testing different RL's is a lot easier without DDMs

**Preferred modeling approach: Hierarchical RL (Stan) followed by either 1.non-hierarchical DDMs or 2. hierarchical DDMs (maybe in Stan) with QValues (mapping onto trial-wise drift rates) from prior fits of RL**

# RL model comparison

Which model fits best? For group and for individuals?  

[25 models - 2*2*2*3 + 1]  
- one vs two parameter probability/relevance distortion  
- asymmetric (distort only the relevance of fractals) vs symmetric probability/relevance distortion  
- linear vs non-linear probability/relevance distortion  
- Update QV for both fractals on each trial (rpeBoth) vs update QV of the fractal of the chosen bundle (rpeChosenFractal) vs update QVs for both fractals only when the trial reward depended on fractals (rpeWhenFractalRewarded)  
- Normalized single parameter distortion

Probability distortion functions:

Two parameter non-linear  

$$w(pFrac) = \frac{\delta*pFrac^{\gamma}}{\delta*pFrac^{\gamma}+(1- pFrac)^{\gamma}}$$  

One parameter non-linear  

$$w(pFrac) = \frac{\delta*pFrac}{\delta*pFrac+(1- pFrac)}$$  

One parameter linear bounded  

$$w(pFrac) = \theta * pFrac$$  

where $0 < \theta < 1$  

Two parameter non-linear unbounded  

$$w(pFrac) = c + m * pFrac$$  

Normalized single parameter distortion  

$$w(pLott) = (1-\theta) * (1-pFrac)$$  
$$V_i = w(pLott)EV_{i} + w(pFrac)QV_{i}$$  

- how to capture best fit for group? 
1. rank models by log_likelihoods sampled from the post burn-in fits 100 times
- count which model is best for each subject how many times
2. posterior predictive data for whole sample with succinct summary 
3. replicate logit pattern


```{r message=FALSE, warning=FALSE}
numpar_names = c("oneParam", "twoParams")
symm_names = c("Asymm", "Symm")
shape_names = c("LinearProbDistortion", "NonLinearProbDistortion")
rpe_names = c("_rpeBoth", "_rpeChosenBundleFractal", "_rpeWhenFractalRewarded")

model_names = c("oneParamDoubleSymmLinearProbDistortion_rpeBoth") #start with the name that doesn't fit in the rest
model_info = data.frame(model_name = "oneParamDoubleSymmLinearProbDistortion_rpeBoth", numpar=  "oneParam", symm = "Symm", shape = "LinearProbDistortion", rpe="_rpeBoth", norm=1)

for(i in 1:length(numpar_names)){
  for(j in 1:length(symm_names)){
    for(k in 1:length(shape_names)){
      for(l in 1:length(rpe_names)){
        model_name = paste0(numpar_names[i], symm_names[j], shape_names[k], rpe_names[l])
        model_names = c(model_names, model_name)
        cur_row = c(model_name = model_name,numpar = numpar_names[i],symm = symm_names[j],shape = shape_names[k], rpe = rpe_names[l], norm=0)
        model_info=  rbind(model_info, cur_row)
      }
    }
  }
}

rm(i, j, k, l, model_name, numpar_names, rpe_names, shape_names, symm_names, cur_row)
```

Read in or compute the fit statistics.

```{r}
if(file.exists(paste0(helpers_path, 'rlModels/elpds.RDS')) & !exists("elpds")){
  print("Reading in previously saved elpds...")
  elpds = readRDS(paste0(helpers_path, 'rlModels/elpds.RDS'))
  
} else{
  for(i in 1:length(model_names)){
    
    cur_model = model_names[i]
    
    if(i == 1){
      elpds = data.frame(elpd_loo_cv = NA, elpd_loo_cv_se = NA, waic = NA, waic_se = NA, model_name = NA, numpar=  NA, symm = NA, shape = NA, rpe=NA, norm=NA)
    }
    
    source(paste0(helpers_path, 'rlModels/fit_rl_hierarchical_', cur_model, '.R'))
    
    logLik = extract(fit, "logLikelihood")$logLikelihood
    cur_loo_cv = data.frame(loo::loo(logLik)$estimates)[1,] #loo_cv elpd estimate
    cur_waic = data.frame(loo::waic(logLik)$estimates)[3,] #waic estimate
    
    cur_row = model_info %>% filter(model_name == cur_model)
    cur_row$elpd_loo_cv = cur_loo_cv$Estimate
    cur_row$elpd_loo_cv_se = cur_loo_cv$SE
    cur_row$waic = cur_waic$Estimate
    cur_row$waic_se = cur_waic$SE
    
    elpds = rbind(elpds, cur_row)
    
    rbind(elpds)
    
    rm(logLik, cur_loo_cv, cur_waic, cur_row, fit, g_par_ests, par_ests)
  }
  rm(i)
}


if(file.exists(paste0(helpers_path, 'rlModels/sampled_logLiks.RDS')) & !exists("sampled_logLiks")){
  sampled_logLiks = readRDS(paste0(helpers_path, 'rlModels/sampled_logLiks.RDS'))
  print("Reading in previously saved samples of log likelihoods...")
} else{
  for(i in 1:length(model_names)){
    
    cur_model = model_names[i]
    
    if(i == 1){
      sampled_logLiks = data.frame(subnum = NA, logLik = NA, model_name=NA)
    }
    
    source(paste0(helpers_path, 'rlModels/fit_rl_hierarchical_', cur_model, '.R'))
    
    tmp = par_ests %>%
      filter(par == "alpha") %>% #parameter name doesn't matter. Just get the likelihoods for the model, which are the same for the sampled parameters. This just happens to be a parameter that is common across all models.
      group_by(subnum) %>% 
      mutate(iter = 1:n()) %>%
      filter(iter > 2000) %>% #sample only from post burn-in samples
      sample_n(100) %>%
      select(subnum, logLik) %>%
      mutate(model_name = cur_model)
    
    sampled_logLiks = rbind(sampled_logLiks, tmp) 
    
    rm(tmp, fit, g_par_ests, par_ests)
  }
  rm(i)
}



elpds = elpds %>% drop_na()
sampled_logLiks = sampled_logLiks %>% drop_na()

if(!file.exists(paste0(helpers_path, 'rlModels/elpds.RDS'))){
  saveRDS(elpds, paste0(helpers_path, 'rlModels/elpds.RDS'))
}

if(!file.exists(paste0(helpers_path, 'rlModels/sampled_logLiks.RDS'))){
  saveRDS(sampled_logLiks, paste0(helpers_path, 'rlModels/sampled_logLiks.RDS'))
}

```

```{r}
plot_opts = list(theme(axis.text.x = element_blank(),
                        legend.position = "bottom",
                        panel.grid = element_blank(),
                        legend.box = "vertical",
                        legend.spacing.y = unit(-0.25, "cm")),
                  scale_alpha_manual(values=c(.6, 1), name="Distortion shape",
                                     breaks = c("LinearProbDistortion", "NonLinearProbDistortion"),
                                     labels = c("Linear", "NonLinear")),
                  scale_shape_manual(values = c(16, 17), name="Distorted attribute",
                                     breaks = c("Asymm", "Symm"),
                                     labels = c("Fractal Only", "Both")),
                  scale_color_manual(breaks=c("_rpeBoth", "_rpeChosenBundleFractal", "_rpeWhenFractalRewarded"),
                                     labels=c("Both", "Chosen", "Rewarded"),
                                     name="Learning",
                                     values=c(cbbPalette[5], cbbPalette[3], cbbPalette[7])),
                  scale_linetype_manual(values = c("solid", "dashed"), name="# distortion params",
                                        breaks = c("oneParam", "twoParams"),
                                        labels = c("1", "2")))
```

```{r}
tmp = sampled_logLiks %>%
  left_join(model_info, by="model_name") %>%
  group_by(model_name) %>%
  summarise(sem_logLik = sd(logLik)/sqrt(n()),
            mean_logLik = mean(logLik),
            numpar = unique(numpar),
            symm = unique(symm),
            shape = unique(shape),
            rpe = unique(rpe)) %>%
  mutate(model_name = as.factor(model_name),
         model_name = reorder(model_name, mean_logLik)) 

tmp %>%
  ggplot(aes(model_name, mean_logLik, col=rpe, shape = symm, alpha = shape))+
  geom_point()+
  geom_errorbar(aes(ymin=mean_logLik - sem_logLik, ymax=mean_logLik + sem_logLik, linetype=numpar))+
  labs(x="Model", y="Mean Log likelihood")+
  plot_opts

```

Note that this difference isn't as large as the plot above suggests. The expected log prob densities for loo-cv show a similar trend but are indistinguishable across the models.

```{r}
elpds %>%
  mutate(model_name = reorder(model_name, elpd_loo_cv)) %>%
  ggplot(aes(model_name, elpd_loo_cv, col=rpe, shape = symm, alpha = shape))+
  geom_point()+
  geom_errorbar(aes(ymin=elpd_loo_cv-elpd_loo_cv_se, ymax=elpd_loo_cv+elpd_loo_cv_se, linetype=numpar), width=.2)+
  labs(x="Model", y="LOO-CV elpd")+
  plot_opts
```

Do the model rankings agree across different statistics? Largely but not entirely.

```{r}
rankings = elpds %>%
  arrange(-elpd_loo_cv) %>%
  mutate(elpd_rank = 1:n()) %>%
  select(elpd_rank, model_name) %>%
  left_join(tmp %>%
              arrange(-mean_logLik) %>%
              mutate(mean_logLik_rank = 1:n()) %>%
              select(model_name, mean_logLik_rank), by="model_name") %>%
  left_join(elpds %>%
              arrange(waic) %>%
              mutate(waic_rank = 1:n()) %>%
              select(waic_rank, model_name), by="model_name") %>%
  select(model_name, elpd_rank, waic_rank, mean_logLik_rank)

rankings
```

Which is the best model for each subject based on the 100 sampled log liks?

```{r}
tmp = sampled_logLiks %>%
  left_join(model_info, by="model_name") %>%
  group_by(subnum, model_name) %>%
  summarise(sem_logLik = sd(logLik)/sqrt(n()),
            mean_logLik = mean(logLik),
            numpar = unique(numpar),
            symm = unique(symm),
            shape = unique(shape),
            rpe = unique(rpe), .groups="keep")

tmp %>%
  ggplot(aes(model_name, mean_logLik, col=rpe, shape = symm, alpha = shape))+
  geom_point()+
  geom_errorbar(aes(ymin=mean_logLik - sem_logLik, ymax=mean_logLik + sem_logLik, linetype=numpar))+
  labs(x="Model", y="Mean Log likelihood")+
  plot_opts+
  theme(legend.position = "none")+
  facet_wrap(~subnum)

```

# Estimate comparison

From top and bottom three models

## Implied learning rates

```{r}
tmp = rankings %>%
  arrange(mean_logLik_rank)

comp_models = c(head(tmp$model_name, 3), tail(tmp$model_name, 3))

rm(tmp)
```

```{r}
learning_rates = data.frame()
qvs = data.frame()

for(i in 1:length(comp_models)){
  
  cur_model = comp_models[i]
  cur_model_rpe = strsplit(cur_model, "_")[[1]][2]
  source(paste0(helpers_path, 'rlModels/fit_rl_hierarchical_', cur_model, '.R'))
  
  cur_alphas = par_ests %>%
    filter(par == "alpha") %>%
    group_by(subnum) %>%
    mutate(iter = 1:n()) %>%
    filter(iter>2000) %>%
    summarise(map_alpha = mean(value)) %>%
    mutate(model_name = cur_model)
  
  learning_rates = rbind(learning_rates, cur_alphas)
  
  cur_qvs = par_ests %>%
    group_by(subnum, par) %>%
    summarise(est = mean(value), .groups='keep') %>%
    spread(par, est) %>%
    left_join(clean_beh_data, by='subnum') %>%
    ungroup() %>% group_by(subnum) %>%
    do(get_qvals(., model_name= cur_model_rpe)) %>%
    mutate(trial = 1:n()) %>%
    ungroup() %>%
    select(subnum, trial, leftQValue, rightQValue) %>%
    mutate(model_name = cur_model)
  
  qvs = rbind(qvs, cur_qvs)
}
rm(par_ests, g_par_ests, fit, cur_model, cur_model_rpe, cur_alphas, cur_qvs)
```

```{r}
tmp = learning_rates %>%
  group_by(subnum) %>%
  spread(model_name, map_alpha) %>%
  ungroup() %>%
  select(-subnum)

p = GGally::ggpairs(tmp, 
                    diag = list("continuous"="blank"),
                    columnLabels = gsub("_","\n",names(tmp)),
                    labeller = label_wrap_gen(width=10))+
  theme(panel.grid = element_blank())

p

```
## Implied QVs

```{r}
tmp = qvs %>%
  select(-rightQValue) %>%
  group_by(subnum, trial) %>%
  spread(model_name, leftQValue) %>%
  ungroup() %>%
  select(-subnum, -trial) 

data.frame(cor(tmp))
```

# Evidence for learning

Does the preferred model's QVs account for choice data better than fractalprobdiff? Yes, it has a lower AIC.

```{r message=FALSE}
model_name = 'fit_rl_hierarchical_oneParamSymmNonLinearProbDistortion_rpeBoth.R'
source(paste0(helpers_path, 'rlModels/', model_name))
```

```{r}
# Add mean posterior estimates to clean_beh_data
clean_beh_data = par_ests %>%
  group_by(subnum, par) %>%
  summarise(est = mean(value), .groups='keep') %>%
  spread(par, est) %>%
  left_join(clean_beh_data, by='subnum')

## Add Q values of fractals to each trial
clean_beh_data = clean_beh_data %>%
  group_by(subnum) %>%
  do(get_qvals(., model_name="original")) %>%
  ungroup()
```

```{r}
clean_beh_data = clean_beh_data %>%
  mutate(EVRight = referenceProb * referenceValue,
         EVLeft = lotteryValue * lotteryProb) %>%
  mutate(EVDiff = scale(EVLeft - EVRight),
         QVDiff = scale(leftQValue - rightQValue),
         fractalDiff = scale(fractalLeftProb - fractalRightProb))

```


```{r}
m1 = glm(choiceLeft ~ (EVDiff + fractalDiff) * probFractalDraw, data = clean_beh_data, family=binomial(link="logit"))
m2 = glm(choiceLeft ~ (EVDiff + QVDiff) * probFractalDraw, data = clean_beh_data, family=binomial(link="logit"))
```

```{r}
summary(m1)
```

```{r}
summary(m2)
```


# Non-parameteric probability distortion

```{r}
model_name = 'fit_rl_hierarchical_nonParamSymmProbDistortion_rpeBoth.R'
source(paste0(helpers_path, 'rlModels/', model_name))
```

How does this model compare to the others in fit? It's much worse.

```{r}
sampled_logLiks_nonParam  = par_ests %>%
  filter(par == "alpha") %>%
  group_by(subnum) %>% 
  mutate(iter = 1:n()) %>%
  filter(iter > 2000) %>%
  sample_n(100) %>%
  select(subnum, logLik) %>%
  mutate(model_name = model_name)
```

```{r}
sampled_logLiks_nonParam %>%
  group_by(model_name) %>%
  summarise(sem_logLik = sd(logLik)/sqrt(n()),
            mean_logLik = mean(logLik),
            numpar = "11",
            symm = "Symm",
            shape = "none",
            rpe = "_rpeBoth") %>%
  rbind(tmp) %>%
  mutate(model_name = as.factor(model_name),
         model_name = reorder(model_name, mean_logLik), 
         red = ifelse(numpar == "11", "new", "old")) %>%
  ggplot(aes(model_name, mean_logLik, col=red))+
  geom_point()+
  geom_errorbar(aes(ymin=mean_logLik - sem_logLik, ymax=mean_logLik + sem_logLik))+
  theme(axis.text.x = element_blank(),
        legend.position = "none",
        panel.grid = element_blank())+
  labs(x="Model", "Mean Log likelihood")
```

```{r}
g_par_ests %>%
  group_by(key) %>%
  # mutate(iter = 1:n()) %>%
  # filter(iter > 2000) %>%
  ggplot(aes(value))+
  geom_histogram(bins=30, alpha=.5, position="identity")+
  facet_wrap(~key, scales='free')+
  theme(panel.grid = element_blank())+
  xlab("")+
  ylab("")
```

Why are weights for pFrac < .5 so close to .5?

Are the weights for true fractal probabilities closer to pFractalDraw (are the qvalue weights so high because they reflect subjects' beliefs, even if distorted)? Fit model without learning and the weights (especially for pFractalDraw < .6) was basically the same.

```{r}
g_par_ests %>%
  group_by(key) %>%
  summarise(mean_par = mean(value),
            sem_par = sd(value)/sqrt(n())) %>%
  filter(key %in% c("g_alpha", "g_beta") == FALSE) %>%
  mutate(key = factor(key, levels = c("g_w0", "g_w1", "g_w2", "g_w3", "g_w4", "g_w5", "g_w6", "g_w7", "g_w8", "g_w9", "g_w10"))) %>%
  arrange(key) %>%
  mutate(pFractal = seq(0, 1, .1)) %>%
  ggplot(aes(pFractal, mean_par))+
  geom_point()+
  geom_errorbar(aes(ymin=mean_par-sem_par, ymax=mean_par+sem_par), width=.02)+
  geom_abline(aes(intercept=0, slope=1))
# ylim(0,1)
```

```{r}
par_ests %>%
  select(-logLik) %>%
  group_by(par, subnum) %>%
  mutate(iter = 1:n()) %>%
  filter(iter>2000) %>%
  summarise(mean_par = mean(value),
            sem_par = sd(value)/sqrt(n()), .groups="keep") %>%
  filter(par %in% c("alpha", "beta") == FALSE) %>%
  mutate(par = factor(par, levels = c("w0", "w1", "w2", "w3", "w4", "w5", "w6", "w7", "w8", "w9", "w10"))) %>%
  arrange(subnum, par) %>%
  ungroup() %>%
  mutate(pFractal = rep(seq(0, 1, .1), 25)) %>%
  ggplot(aes(pFractal, mean_par))+
  # geom_point()+
  geom_errorbar(aes(ymin=mean_par-sem_par, ymax=mean_par+sem_par), width=.02)+
  geom_abline(aes(intercept=0, slope=1)) +
  facet_wrap(~subnum)
```

