---
title: 'Experience vs. description based decision-making project: Revisiting RL models'
output:
  html_document:
    toc: yes
    toc_float: yes
    code_folding: 'hide'
  pdf_document:
    toc: yes
---

# Set up

```{r include=FALSE}
library(broom)
library(tidyverse)
theme_set(theme_bw())
library(here)
cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
helpers_path = here('analysis/helpers/')
source(paste0(helpers_path,'01_clean_behavioral_data.R'))
```

Which model fits best? For group and for individuals?  

[24 models - 2*2*2*3]  
- one vs two parameter probability/relevance distortion  
- asymmetric (distort only the relevance of fractals) vs symmetric probability/relevance distortion  
- linear vs non-linear probability/relevance distortion  
- Update QV for both fractals on each trial (rpeBoth) vs update QV of the fractal of the chosen bundle (rpeChosenFractal) vs update QVs for both fractals only when the trial reward depended on fractals (rpeWhenFractalRewarded)  

update both to same degree vs. update fractal in chosen bundle vs. update rewarded bundle vs. update more for surprising events (4 conditions)
- how to capture best fit for group? 
1. stack all likelihoods for all subjects? stack mean likelihoods (25 points for each model)
  - sample from likelihoods 100 times and count which model is best for each subject how many times
2. posterior predictive data for whole sample with succinct summary 
3. replicate logit pattern
[eventually do this kind of comparison for ddms and possibly bayesian model averaging though unlikely for the poster]

```{r}
numpar_names = c("oneParam", "twoParams")
symm_names = c("Asymm", "Symm")
shape_names = c("LinearProbDistortion", "NonLinearProbDistortion")
rpe_names = c("_rpeBoth", "_rpeChosenBundleFractal", "_rpeWhenFractalRewarded")
prefix = "fit_rl_hierarchical_"

for(i in 1:length(numpar_names)){
  for(j in 1:length(symm_names)){
    for(k in 1:length(shape_names)){
      for(l in 1:length(rpe_names)){
        model_name = paste0(prefix, numpar_names[i], symm_names[j], shape_names[k], rpe_names[l], ".R")
        print(paste0(model_name, " = ", file.exists(paste0(helpers_path, 'rlModels/', model_name))))
      }
    }
  }
}


```

[revisit the linear prob distortion nb to see if another single prob distortion model parameter shifts the fractal prob diff logit slopes the same way]

Does the best fitting model's QVs account for choice data better than fractalprobdiff?
(anova of logits with QV diff instead of fractalprobdiff)
On the poster plot overlay the logit plot with the logit slopes of the qv model as well with a lower alpha for the worse fitting model.

```{r}

```

Is there a value difference effect on (true) RTs for QVs? There isn't one for true fractalprobdiffs.

```{r}

```

Does modeling RL with DD as the choice rule buy anything in this dataset? Compare alpha recoverability for RL vs DDRL [might move to another nb]

```{r}

```
