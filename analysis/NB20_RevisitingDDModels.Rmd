---
title: 'Experience vs. description based decision-making project: Revisiting DD models'
output:
  html_document:
    toc: yes
    toc_float: yes
    code_folding: 'hide'
  pdf_document:
    toc: yes
---


```{r include=FALSE}
library(tidyverse)
theme_set(theme_bw())
library(here)
cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
helpers_path = here('analysis/helpers/')

source(paste0(helpers_path,'ddModels/sim_task.R'))
source(paste0(helpers_path,'ddModels/fit_task.R'))
source(paste0(helpers_path,'01_clean_behavioral_data.R'))
source(paste0(helpers_path, 'get_qvals.R'))
source(paste0(helpers_path,'optimPostProcess/sim_sanity_checks.R'))


set.seed(38573)
```

Add distorted value estimates using the hierarchical RL fit.

```{r message=FALSE}

source(paste0(helpers_path, 'rlModels/fit_rl_hierarchical_oneParamDoubleSymmLinearProbDistortion_rpeBoth.R'))

clean_beh_data = par_ests %>%
  group_by(subnum, par) %>%
  summarise(est = mean(value), .groups='keep') %>%
  spread(par, est) %>%
  left_join(clean_beh_data, by='subnum')

## Add Q values of fractals to each trial
clean_beh_data = clean_beh_data %>%
  group_by(subnum) %>%
  do(get_qvals(., model_name="original")) %>%
  ungroup()

clean_beh_data = clean_beh_data %>%
  mutate(rightLotteryEV = referenceProb * referenceValue,
         leftLotteryEV = lotteryValue * lotteryProb,
         lottery_ev_diff = leftLotteryEV - rightLotteryEV,
         fractal_qv_diff = leftQValue - rightQValue,
         distorted_ev_diff = (1-theta)*(1-probFractalDraw)*lottery_ev_diff,
         distorted_qv_diff = theta*probFractalDraw*fractal_qv_diff)
```

**Potential problem** for 3 integrator model: The model would predict slower decisions because the difference in the absolute value of each attribute integrator RDV would be small.

**BUT** by design there aren't trials that provide strong (distorted) evidence for a single side so for the current set of stimuli this should not be a concern.

```{r}
clean_beh_data %>%
  ggplot(aes(distorted_ev_diff, distorted_qv_diff)) +
  geom_point()+
  geom_abline(aes(slope=1, intercept=0))
```

Simulate data with a 3 integrator model.

Select half the data to use as input stimuli for simulated data.

```{r}
sub_data = clean_beh_data %>%
  filter(subnum  %in% c("01", "03", "05","07", "09", "11", "13", "15", "17", "19")) %>%
  select(leftLotteryEV, rightLotteryEV, leftQValue, rightQValue, probFractalDraw, reactionTime, choiceLeft, subnum, distorted_ev_diff, distorted_qv_diff) %>%
  rename(EVLeft = leftLotteryEV, EVRight = rightLotteryEV, QVLeft = leftQValue, QVRight = rightQValue, distortedEVDiff = distorted_ev_diff, distortedQVDiff = distorted_qv_diff)
```

Source trial simulating and fitting function three integrator model. The model only had the ddm parameters as variables; choice parameters (learning rate and probability distortion) are fitted elsewhere and used to compute the distorted value differences fed into this model.

```{r}
sim_trial_list = list()
source(paste0(helpers_path, 'ddModels/r_ddm_models/ddm_model4b_separatedProbDistortion.R'))
sim_trial_list[['model4b']] = sim_trial
```

Test if trial simulating function works.

```{r}
sim_trial(dLott=0.03, dFrac=0.04, dArb=0.04, sigmaLott = 0.03, sigmaFrac = 0.03, distortedEVDiff = sub_data$distortedEVDiff[100], distortedQVDiff = sub_data$distortedQVDiff[100], probFractalDraw = sub_data$probFractalDraw[100], barrierDecay = 0, EVLeft = sub_data$EVLeft[100], EVRight = sub_data$EVRight[100], QVLeft = sub_data$QVLeft[100], QVRight = sub_data$QVRight[100])
```

Simulate a dataset using half the stimuli filtered earlier and this three integrator model.

```{r}
m4b_1 = sim_task(sub_data, model_name = "model4b", dLott=0.03, dFrac=0.06, dArb=0.05, sigmaLott = 0.03, sigmaFrac = 0.03)
```

```{r}
sim_sanity_checks(m4b_1, checks=c(1,3,4,5,6,7,8), compare_logits = TRUE)
```

Are the predicted response times close to the true response times? Not really. Despite the apparent correspondence when looking at aggregate data grouped by probFractalDraw level the individual trial estimates are not correlated at all.

Does a well-established model do better in such an absolute check of model fit?

```{r}
m4b_1 %>%
  left_join(sub_data, by=c("EVLeft", "EVRight", "QVLeft", "QVRight", "distortedEVDiff", "distortedQVDiff", "probFractalDraw")) %>%
  ggplot(aes(reactionTime.y, reactionTime.x))+
  geom_point(color="light gray")+
  geom_smooth(formula="y~x", method="lm")+
  geom_abline(aes(slope=1, intercept=0), linetype="dashed")+
  xlab("True RT")+
  ylab("Predicted RT")
```

Check the fitting function

```{r}
fit_trial_list = list()
fit_trial_list[['model4b']] = fit_trial
```

```{r}
i = 100

fit_trial(dLott=0.03, dFrac=0.06, dArb=0.06, sigmaLott = 0.03, sigmaFrac = 0.03, barrierDecay=0, distortedEVDiff = m4b_1$distortedEVDiff[i],
    distortedQVDiff = m4b_1$distortedQVDiff[i], choice = m4b_1$choice[i], reactionTime = m4b_1$reactionTime[i], probFractalDraw = m4b_1$probFractalDraw[i], debug=TRUE)
```

What are the true parameters?

```{r}
m4b_1 %>%
  select(dLott, dFrac, dArb, sigmaLott, sigmaFrac, sigmaArb, bias) %>%
  distinct()
```

What is the true negative log likelihood?

```{r}
get_task_nll(data = m4b_1[1:200,], par_ = c(.03, .06, .05, .03, .03), par_names = c("dArb", "dLott", "dFrac", "sigmaLott", "sigmaFrac"), model_name_="model4b", fix_pars_ = list(bias = 0) )
```

What is the negative log likelihood from the starting point. This shouldn't be lower than the true parameter combination. If it is then the algorithm cannot converge on the true parameters. 

**FIND THE ERROR IN THE LIKELIHOOD COMPUTATION**

```{r}
get_task_nll(data = m4b_1[1:200,], par_ = c(.05, .05, .05, .05, .05), par_names = c("dArb", "dLott", "dFrac", "sigmaLott", "sigmaFrac"), model_name_="model4b", fix_pars_ = list(bias = 0) )
```

Still pretty slow but at least it ends and does not produce errors. Whether it's any good in recovering true parameters I'm not sure yet.

```{r}
# dLott=0.03, dFrac=0.06, dArb=0.05, sigmaLott = 0.03, sigmaFrac = 0.03
# optim_out = visualMLE::optim_save(c(.05, .05, .05, .05, .05), get_task_nll, data=m4b_1[1:200,], par_names = c("dArb", "dLott", "dFrac", "sigmaLott", "sigmaFrac"), model_name_="model4b", fix_pars_ = list(bias = 0.1), control = list(maxit=100))

```

```{r}
optim_out
```

Interactive figure updating state probabilities

```{r}

```

```{r}
data.frame(x=rnorm(10000, mean = muLott, sd=sigmaLott)) %>%
  ggplot(aes(x))+
  geom_density()+
  scale_y_continuous(breaks=c(seq(0,15,1)))
```

Since I can simulate things is there another metric I can use to minimize for fitting?
RT given choice does this look different for left/right (i.e. the only choice category that is measured)
No.
Differences exist for probFractalDraw levels
But if I minimize distance between simulated and true RT for probFractalDraw levels then I wouldn't be checking if the model is doing a good job in capturing choice behavior too

```{r}
clean_beh_data %>%
  ggplot(aes(reactionTime, color=as.factor(choiceLeft)))+
  geom_density(position="identity")

```
```{r}
data.frame(prStatesArb) %>%
  mutate(state = 1:nrow(prStatesArb)) %>%
  gather(key, value, -state) %>%
  ggplot(aes(key, state))+
  geom_tile(aes(fill=log(value+1e-100)))
```


```{r}
data.frame(prStatesLott) %>%
  mutate(state = 1:nrow(prStatesLott)) %>%
  gather(key, value, -state) %>%
  ggplot(aes(key, state))+
  geom_tile(aes(fill=log(value+1e-100)))
```

```{r}
data.frame(prStatesFrac) %>%
  mutate(state = 1:nrow(prStatesFrac)) %>%
  gather(key, value, -state) %>%
  ggplot(aes(key, state))+
  geom_tile(aes(fill=log(value+1e-10)))
```