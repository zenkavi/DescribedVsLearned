---
title: 'Experience vs. description based decision-making project: Revisiting DD models'
output:
  html_document:
    toc: yes
    toc_float: yes
    code_folding: 'hide'
  pdf_document:
    toc: yes
---


```{r include=FALSE}
library(tidyverse)
theme_set(theme_bw())
library(here)
cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
helpers_path = here('analysis/helpers/')

source(paste0(helpers_path,'ddModels/sim_task.R'))
source(paste0(helpers_path,'ddModels/fit_task.R'))
source(paste0(helpers_path,'01_clean_behavioral_data.R'))
source(paste0(helpers_path, 'get_qvals.R'))
source(paste0(helpers_path,'optimPostProcess/sim_sanity_checks.R'))


set.seed(38573)
```

Add distorted value estimates using the hierarchical RL fit.

```{r message=FALSE}

source(paste0(helpers_path, 'rlModels/fit_rl_hierarchical_oneParamDoubleSymmLinearProbDistortion_rpeBoth.R'))

clean_beh_data = par_ests %>%
  group_by(subnum, par) %>%
  summarise(est = mean(value), .groups='keep') %>%
  spread(par, est) %>%
  left_join(clean_beh_data, by='subnum')

## Add Q values of fractals to each trial
clean_beh_data = clean_beh_data %>%
  group_by(subnum) %>%
  do(get_qvals(., model_name="original")) %>%
  ungroup()

clean_beh_data = clean_beh_data %>%
  mutate(rightLotteryEV = referenceProb * referenceValue,
         leftLotteryEV = lotteryValue * lotteryProb,
         lottery_ev_diff = leftLotteryEV - rightLotteryEV,
         fractal_qv_diff = leftQValue - rightQValue,
         distorted_ev_diff = (1-theta)*(1-probFractalDraw)*lottery_ev_diff,
         distorted_qv_diff = theta*probFractalDraw*fractal_qv_diff)
```

**Potential problem** for 3 integrator model: The model would predict slower decisions because the difference in the absolute value of each attribute integrator RDV would be small.

**BUT** by design there aren't trials that provide strong (distorted) evidence for a single side so for the current set of stimuli this should not be a concern.

```{r}
clean_beh_data %>%
  ggplot(aes(distorted_ev_diff, distorted_qv_diff)) +
  geom_point()+
  geom_abline(aes(slope=1, intercept=0))
```

Simulate data with a 3 integrator model.

Select half the data to use as input stimuli for simulated data.

```{r}
sub_data = clean_beh_data %>%
  filter(subnum  %in% c("01", "03", "05","07", "09", "11", "13", "15", "17", "19")) %>%
  select(leftLotteryEV, rightLotteryEV, leftQValue, rightQValue, probFractalDraw, reactionTime, choiceLeft, subnum, distorted_ev_diff, distorted_qv_diff) %>%
  rename(EVLeft = leftLotteryEV, EVRight = rightLotteryEV, QVLeft = leftQValue, QVRight = rightQValue, distortedEVDiff = distorted_ev_diff, distortedQVDiff = distorted_qv_diff)
```

Source trial simulating and fitting function three integrator model. The model only had the ddm parameters as variables; choice parameters (learning rate and probability distortion) are fitted elsewhere and used to compute the distorted value differences fed into this model.

```{r}
sim_trial_list = list()
source(paste0(helpers_path, 'ddModels/r_ddm_models/ddm_model4b_separatedProbDistortion.R'))
sim_trial_list[['model4b']] = sim_trial
```

Test if trial simulating function works.

```{r}
sim_trial(dLott=0.03, dFrac=0.04, dArb=0.04, sigmaLott = 0.03, sigmaFrac = 0.03, distortedEVDiff = sub_data$distortedEVDiff[100], distortedQVDiff = sub_data$distortedQVDiff[100], probFractalDraw = sub_data$probFractalDraw[100], barrierDecay = 0, EVLeft = sub_data$EVLeft[100], EVRight = sub_data$EVRight[100], QVLeft = sub_data$QVLeft[100], QVRight = sub_data$QVRight[100])
```

Simulate a dataset using half the stimuli filtered earlier and this three integrator model.

```{r}
m4b_1 = sim_task(sub_data, model_name = "model4b", dLott=0.03, dFrac=0.06, dArb=0.05, sigmaLott = 0.03, sigmaFrac = 0.03)
```

```{r}
sim_sanity_checks(m4b_1, checks=c(1,3,4,5,6,7,8), compare_logits = TRUE)
```

Are the predicted response times close to the true response times? Not really. Despite the apparent correspondence when looking at aggregate data grouped by probFractalDraw level the individual trial estimates are not correlated at all.

Does a well-established model do better in such an absolute check of model fit?

```{r}
m4b_1 %>%
  left_join(sub_data, by=c("EVLeft", "EVRight", "QVLeft", "QVRight", "distortedEVDiff", "distortedQVDiff", "probFractalDraw")) %>%
  ggplot(aes(reactionTime.y, reactionTime.x))+
  geom_point(color="light gray")+
  geom_smooth(formula="y~x", method="lm")+
  geom_abline(aes(slope=1, intercept=0), linetype="dashed")+
  xlab("True RT")+
  ylab("Predicted RT")
```

Check the fitting function

```{r}
fit_trial_list = list()
fit_trial_list[['model4b']] = fit_trial
```

```{r}
i = 100

fit_trial(dLott=0.03, dFrac=0.06, dArb=0.06, sigmaLott = 0.03, sigmaFrac = 0.03, barrierDecay=0, distortedEVDiff = m4b_1$distortedEVDiff[i],
          distortedQVDiff = m4b_1$distortedQVDiff[i], choice = m4b_1$choice[i], reactionTime = m4b_1$reactionTime[i], probFractalDraw = m4b_1$probFractalDraw[i], debug=TRUE)
```

What are the true parameters?

```{r}
m4b_1 %>%
  select(dLott, dFrac, dArb, sigmaLott, sigmaFrac, sigmaArb, bias) %>%
  distinct()
```

What is the true negative log likelihood?

```{r}
get_task_nll(data = m4b_1[1:200,], par_ = c(.03, .06, .05, .03, .03), par_names = c("dArb", "dLott", "dFrac", "sigmaLott", "sigmaFrac"), model_name_="model4b", fix_pars_ = list(bias = 0) )
```

What is the negative log likelihood from the starting point. This shouldn't be lower than the true parameter combination. If it is then the algorithm cannot converge on the true parameters. 

**FIND THE ERROR IN THE LIKELIHOOD COMPUTATION**

```{r}
get_task_nll(data = m4b_1[1:200,], par_ = c(.05, .05, .05, .05, .05), par_names = c("dArb", "dLott", "dFrac", "sigmaLott", "sigmaFrac"), model_name_="model4b", fix_pars_ = list(bias = 0) )
```

Still pretty slow but at least it ends and does not produce errors. Whether it's any good in recovering true parameters I'm not sure yet.

```{r}
# dLott=0.03, dFrac=0.06, dArb=0.05, sigmaLott = 0.03, sigmaFrac = 0.03
# optim_out = visualMLE::optim_save(c(.05, .05, .05, .05, .05), get_task_nll, data=m4b_1[1:200,], par_names = c("dArb", "dLott", "dFrac", "sigmaLott", "sigmaFrac"), model_name_="model4b", fix_pars_ = list(bias = 0.1), control = list(maxit=100))

```

```{r}
# optim_out
```

Interactive figure updating state probabilities

Assume for a single integrator drift rate etc parameters
Simulate n particles
Then using the sum of independent variables logic to make a heat map of the probability of a particle in each state at each time step

```{r}
run_lott_integrator = function(dLott, sigmaLott, distortedEVDiff, nonDecisionTime=0, bias=0, barrierDecay = 0, timeStep=10, barrier = 1, maxIter = 400, debug=TRUE){
  
  if(debug){
    debug_df = data.frame(time=0, lotteryRDV=0)
  }
  
  lotteryRDV = bias
  time = 1
  elapsedNDT = 0
  choice = 0
  RT = NA
  nonDecIters = nonDecisionTime / timeStep
  initialBarrier = barrier
  barrier = rep(initialBarrier, maxIter)
  
  # The values of the barriers can change over time
  for(t in seq(1, maxIter, 1)){
    barrier[t] = initialBarrier / (1 + barrierDecay * t)
  }
  
  lottery_mu = dLott * distortedEVDiff
  
  while (time<maxIter){
    
    # If the arbitrator RDV hits one of the barriers make decision
    if (lotteryRDV >= barrier[time] | lotteryRDV <= -barrier[time]){
      
      # Convert ms back to secs
      RT = (time * timeStep)/1000 
      
      if (lotteryRDV >= barrier[time]){
        choice = "left"
      } else if (lotteryRDV <= -barrier[time]){
        choice = "right"
      }
      break
    } 
    
    # Otherwise continue sampling evidence
    if (elapsedNDT < nonDecIters){
      elapsedNDT = elapsedNDT + 1
    } else{
      lotteryRDV = lotteryRDV + rnorm(1, lottery_mu, sigmaLott)
    }
    
    if (debug){
      debug_row = data.frame(time = time, lotteryRDV = round(lotteryRDV, 3))
      debug_df = rbind(debug_df, debug_row)
    }
    
    # Increment sampling iteration
    time = time + 1
  }
  
  out = data.frame(distortedEVDiff = distortedEVDiff, choice=choice, reactionTime = RT, dLott=dLott, sigmaLott=sigmaLott, barrier=barrier[time], nonDecisionTime=nonDecisionTime, bias=bias, timeStep=timeStep, maxIter=maxIter)
  
  if(debug){
    out = list(out=out, debug_df=debug_df)
  }
  
  return(out)
}

```

```{r}
initialBarrier = 1
approxStateStep = .1
halfNumStateBins = round(initialBarrier / approxStateStep)
stateStep = initialBarrier / (halfNumStateBins + 0.5)

# The vertical axis is divided into states.
states = seq(-1*(initialBarrier) + (stateStep / 2), initialBarrier - (stateStep / 2), stateStep)
```

```{r}
n_particles = 250
sim_data = data.frame()

for(i in 1:n_particles){
  tmp = run_lott_integrator(dLott = 0.03, sigmaLott = 0.05, distortedEVDiff = 0.4)$debug_df
  tmp$iter = i
  sim_data = rbind(sim_data, tmp)
}
```

```{r}
sim_data %>%
  ggplot(aes(time, lotteryRDV, group=iter))+
  geom_line(color="gray")+
  scale_x_continuous(breaks = seq(0, 400, 1))+
  scale_y_continuous(breaks = round(states,3))+
  geom_hline(aes(yintercept=0), linetype="dashed")+
  geom_hline(aes(yintercept=1))+
  geom_hline(aes(yintercept=-1))+
  theme(panel.grid.minor = element_blank(), 
        axis.text = element_blank())
```

What proportion of particles is in each state in each time step?

```{r}
for(i in 1:nrow(sim_data)){
  sim_data$state_bin[i] = which.min(abs(states - sim_data$lotteryRDV[i]))
}
```

For the 10 time steps (in facets).

```{r}
sim_data %>%
  # filter(time< 11) %>%
  filter(time>20 &time<31) %>%
  ggplot(aes(state_bin))+
  geom_histogram(bins=21)+
  coord_flip()+
  facet_grid(.~time)+
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank())+
  ylab("")+
  xlab("# particles / state")+
  scale_x_continuous(breaks=c(1:21)) +
  geom_vline(aes(xintercept=11), linetype="dashed")+
  geom_vline(aes(xintercept=21))+
  geom_vline(aes(xintercept=1))
```

```{r}
dLott = 0.03
distortedEVDiff = .4
sigmaLott = .05

est_data = data.frame()

for(i in 0:max(sim_data$time)){
 tmp=data.frame(time = i, 
            states = states, 
            pdf_states = dnorm(states, mean=dLott*distortedEVDiff*i, sd=sqrt(sigmaLott^2*i)))
 est_data = rbind(est_data, tmp)
}

est_data %>%
  # filter(time<11) %>%
    filter(time>20 &time<31) %>%
  ggplot(aes(states, pdf_states))+
  geom_bar(stat="identity")+
  coord_flip()+
  facet_grid(.~time)+
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank())+
  ylab("")+
  xlab("pdf / state")+
  scale_x_continuous(breaks=states) +
  geom_vline(aes(xintercept=0), linetype="dashed")+
  geom_vline(aes(xintercept=1))+
  geom_vline(aes(xintercept=-1))
```
For this single integrator is the density (/likelihood) at time step == true rt always higher for the true parameter combination compared to a wrong parameter combo?

Or is the density always larger for something stupid like the size of the mean?

```{r}
true_liks = sim_data %>%
  group_by(iter) %>%
  filter(time == max(time)) %>%
  left_join(est_data %>%
  filter(states>.95), by="time")

true_liks
```
Having larger scaling factor doesn't translate higher pdfs

```{r}
false_dLott = 0.08
distortedEVDiff = .4
false_sigmaLott = .08

false_est_data = data.frame()

for(i in 0:max(sim_data$time)){
 tmp=data.frame(time = i, 
            states = states, 
            pdf_states = dnorm(states, mean=false_dLott*distortedEVDiff*i, sd=sqrt(false_sigmaLott^2*i)))
 false_est_data = rbind(false_est_data, tmp)
}

false_liks = sim_data %>%
  group_by(iter) %>%
  filter(time == max(time)) %>%
  left_join(false_est_data %>%
  filter(states>.95), by="time")

true_liks %>%
  mutate(lik_type = "true") %>%
  rbind(false_liks %>% mutate(lik_type="false")) %>%
  select(iter, pdf_states, lik_type) %>%
  spread(lik_type, pdf_states) %>%
  ggplot(aes(true, false))+
  geom_point()+
  geom_abline(aes(intercept=0, slope=1))
```

Even with false parameters that are relatively close to the true ones the true pdf's are higher for most particles.

```{r}
false_dLott = 0.04
distortedEVDiff = .4
false_sigmaLott = .06

false_est_data = data.frame()

for(i in 0:max(sim_data$time)){
 tmp=data.frame(time = i, 
            states = states, 
            pdf_states = dnorm(states, mean=false_dLott*distortedEVDiff*i, sd=sqrt(false_sigmaLott^2*i)))
 false_est_data = rbind(false_est_data, tmp)
}

false_liks = sim_data %>%
  group_by(iter) %>%
  filter(time == max(time)) %>%
  left_join(false_est_data %>%
  filter(states>.95), by="time")

true_liks %>%
  mutate(lik_type = "true") %>%
  rbind(false_liks %>% mutate(lik_type="false")) %>%
  select(iter, pdf_states, lik_type) %>%
  spread(lik_type, pdf_states) %>%
  ggplot(aes(true, false))+
  geom_point()+
  geom_abline(aes(intercept=0, slope=1))
```

Ok so try it for a full dataset

```{r}
n_trials = 300
sim_subj = data.frame()
true_dLott = 0.03
true_sigmaLott = 0.05

for(i in 1:n_trials){
  tmp = run_lott_integrator(dLott = true_dLott, sigmaLott = true_sigmaLott, distortedEVDiff = clean_beh_data$distorted_ev_diff[i])$debug_df
  tmp$iter = i
  sim_subj = rbind(sim_subj, tmp)
}
```

```{r}
sim_subj %>%
  filter(iter<50)%>%
  ggplot(aes(time, lotteryRDV, group=iter))+
  geom_line(color="gray")+
  scale_x_continuous(breaks = seq(0, 400, 1))+
  scale_y_continuous(breaks = round(states,3))+
  geom_hline(aes(yintercept=0), linetype="dashed")+
  geom_hline(aes(yintercept=1))+
  geom_hline(aes(yintercept=-1))+
  theme(panel.grid.minor = element_blank(), 
        axis.text = element_blank())
```



```{r}
sim_subj_rts = sim_subj %>%
  group_by(iter) %>%
  filter(time == max(time))

subj_liks = data.frame()

for(i in 1:nrow(sim_subj_rts)){
  trial_time = sim_subj_rts$time[i]
  trial_rdv = sim_subj_rts$lotteryRDV[i]
  trial_evDiff = clean_beh_data$distorted_ev_diff[i]
 tmp=data.frame(iter = sim_subj_rts$iter[i],
                time = trial_time,
                rdv = trial_rdv,
                pdf_rdv = dnorm(trial_rdv, mean=true_dLott*trial_evDiff*trial_time, sd=sqrt(true_sigmaLott^2*trial_time)))
 subj_liks = rbind(subj_liks, tmp)
}
```

```{r}
sum(subj_liks$pdf_rdv)
```

```{r}
false_dLott = 0.08
false_sigmaLott = 0.08
false_subj_liks = data.frame()

for(i in 1:nrow(sim_subj_rts)){
  trial_time = sim_subj_rts$time[i]
  trial_rdv = sim_subj_rts$lotteryRDV[i]
  trial_evDiff = clean_beh_data$distorted_ev_diff[i]
 tmp=data.frame(iter = sim_subj_rts$iter[i],
                time = trial_time,
                rdv = trial_rdv,
                pdf_rdv = dnorm(trial_rdv, mean=false_dLott*trial_evDiff*trial_time, sd=sqrt(false_sigmaLott^2*trial_time)))
 false_subj_liks = rbind(false_subj_liks, tmp)
}
```

```{r}
sum(false_subj_liks$pdf_rdv)
```

Ok so try it with a model of the full task (integrating on both attributes instead of just on lotteries)

```{r}

```

