---
title: 'Experience vs. description based decision-making project: Revisiting DD models'
output:
  html_document:
    toc: yes
    toc_float: yes
    code_folding: 'hide'
  pdf_document:
    toc: yes
---

```{r include=FALSE}
library(tidyverse)
theme_set(theme_bw())
library(here)
cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
helpers_path = here('analysis/helpers/')

source(paste0(helpers_path,'ddModels/sim_task.R'))
source(paste0(helpers_path,'ddModels/fit_task.R'))
source(paste0(helpers_path,'01_clean_behavioral_data.R'))
source(paste0(helpers_path, 'get_qvals.R'))
source(paste0(helpers_path,'optimPostProcess/sim_sanity_checks.R'))


set.seed(38573)
```

Add distorted value estimates using the hierarchical RL fit.

```{r message=FALSE, warning=FALSE}

source(paste0(helpers_path, 'rlModels/fit_rl_hierarchical_oneParamDoubleSymmLinearProbDistortion_rpeBoth.R'))

clean_beh_data = par_ests %>%
  group_by(subnum, par) %>%
  summarise(est = mean(value), .groups='keep') %>%
  spread(par, est) %>%
  left_join(clean_beh_data, by='subnum')

## Add Q values of fractals to each trial
clean_beh_data = clean_beh_data %>%
  group_by(subnum) %>%
  do(get_qvals(., model_name="original")) %>%
  ungroup()

clean_beh_data = clean_beh_data %>%
  mutate(rightLotteryEV = referenceProb * referenceValue,
         leftLotteryEV = lotteryValue * lotteryProb,
         lottery_ev_diff = leftLotteryEV - rightLotteryEV,
         fractal_qv_diff = leftQValue - rightQValue,
         distorted_ev_diff = (1-theta)*(1-probFractalDraw)*lottery_ev_diff,
         distorted_qv_diff = theta*probFractalDraw*fractal_qv_diff)

rm(fit, g_par_est, par_ests)
```

**Potential problem** for 3 integrator model: The model would predict slower decisions because the difference in the absolute value of each attribute integrator RDV would be small.

**BUT** by design there aren't trials that provide strong (distorted) evidence for a single side so for the current set of stimuli this should not be a concern.

```{r}
clean_beh_data %>%
  ggplot(aes(distorted_ev_diff, distorted_qv_diff)) +
  geom_point()+
  geom_abline(aes(slope=1, intercept=0))
```

# The 3 integrator model

Source trial simulating and fitting function three integrator model. The model only had the ddm parameters as variables; choice parameters (learning rate and probability distortion) are fitted elsewhere and used to compute the distorted value differences fed into this model.

```{r}
sim_trial_list = list()
source(paste0(helpers_path, 'ddModels/r_ddm_models/ddm_model4b_separatedProbDistortion.R'))
sim_trial_list[['model4b']] = sim_trial
```

Test if trial simulating function works.

```{r}
sub_data = clean_beh_data %>%
  filter(subnum  %in% c("01", "03", "05","07", "09", "11", "13", "15", "17", "19")) %>%
  select(leftLotteryEV, rightLotteryEV, leftQValue, rightQValue, probFractalDraw, reactionTime, choiceLeft, subnum, distorted_ev_diff, distorted_qv_diff) %>%
  rename(EVLeft = leftLotteryEV, EVRight = rightLotteryEV, QVLeft = leftQValue, QVRight = rightQValue, distortedEVDiff = distorted_ev_diff, distortedQVDiff = distorted_qv_diff)
```

```{r}
sim_trial(dLott=0.03, dFrac=0.04, dArb=0.04, sigmaLott = 0.03, sigmaFrac = 0.03, distortedEVDiff = sub_data$distortedEVDiff[100], distortedQVDiff = sub_data$distortedQVDiff[100], probFractalDraw = sub_data$probFractalDraw[100], barrierDecay = 0, EVLeft = sub_data$EVLeft[100], EVRight = sub_data$EVRight[100], QVLeft = sub_data$QVLeft[100], QVRight = sub_data$QVRight[100])
```

## Aggregate checks 

Simulate data with a 3 integrator model.

Simulate a dataset using half the stimuli filtered earlier and this three integrator model.

**Note: this selection of parameters provides a nice qualitative fit to data. It assumes integration is slower for lotteries than for fractals. This is one way of capturing the difference in processing lottery values, which can only be done after the stimulus presentation, as apposed to fractal values which are learned about trialwise. There are alternative ways of modeling this: e.g. integration for the fractal integrator might start before the stimulus presentation screen; ndt for the lottery integrator (or the arbitrator when the lottery is more relevant) might be longer etc. The way to determine which of these hypotheses is most supported by data would be a model comparison.**

```{r}
m4b_1 = sim_task(sub_data, model_name = "model4b", dLott=0.03, dFrac=0.06, dArb=0.05, sigmaLott = 0.03, sigmaFrac = 0.03)
# m4b_1 = sim_task(sub_data, model_name = "model4b", dLott=0.01, dFrac=0.02, dArb=0.015, sigmaLott = 0.01, sigmaFrac = 0.01)
```

```{r}
sim_sanity_checks(m4b_1, checks=c(1,3,4,5,6,7,8), compare_logits = TRUE)
```


## Question: Absolute check

Are the predicted response times close to the true response times? Not really. Despite the apparent correspondence when looking at aggregate data grouped by probFractalDraw level the individual trial estimates are not correlated at all.

Does a well-established model do better in such an absolute check of model fit?

```{r}
m4b_1 %>%
  left_join(sub_data, by=c("EVLeft", "EVRight", "QVLeft", "QVRight", "distortedEVDiff", "distortedQVDiff", "probFractalDraw")) %>%
  ggplot(aes(reactionTime.y, reactionTime.x))+
  geom_point(color="light gray")+
  geom_smooth(formula="y~x", method="lm")+
  geom_abline(aes(slope=1, intercept=0), linetype="dashed")+
  xlab("True RT")+
  ylab("Predicted RT")
```

# Sum of random variables approach in discrete time and space

Given the $choice = C$ and $RT = T$ for a trial  

Assuming time step $dt$ timestep of choice is
$$t = \lceil\frac{T}{dt}\rceil$$
Assuming the relative decision variable $RDV$ changes according to
$$\frac{\triangle RDV}{dt} \sim N(d\triangle V, \sigma)$$  

The distribution of the RDV from a starting point $z$ at timestep of choice would be 
$$RDV (t) \sim N({d\triangle Vt+z}, \sqrt{\sigma^2t})$$  
Then the probability of crossing the a boundary at timestep $t$ would be the AUC above (1-CDF) or below (CDF) the top or the bottom boundary respectively.

**Why might this not have been used previously? Some ideas:**  
**The boundary in traditional DDM is not fixed at 1 and -1. Instead it is a parameter. It's not immediately apparent to me how to use the sum of random variables approach without assuming the boundaries at 1 and -1.**
**The traditional DDM often models the whole of a response time distribution for correct vs incorrect.**

## Demonstration

Assuming drift rate and noise parameters for a single integrator we simulate n particles for the same trial (i.e. same value difference) and make a histogram of a particles in each state at each time step.

```{r}
run_lott_integrator = function(dLott, sigmaLott, distortedEVDiff, nonDecisionTime=0, bias=0, barrierDecay = 0, timeStep=10, barrier = 1, maxIter = 400, debug=TRUE){
  
  if(debug){
    debug_df = data.frame(time=0, lotteryRDV=0)
  }
  
  lotteryRDV = bias
  time = 1
  elapsedNDT = 0
  choice = 0
  RT = NA
  nonDecIters = nonDecisionTime / timeStep
  initialBarrier = barrier
  barrier = rep(initialBarrier, maxIter)
  
  # The values of the barriers can change over time
  for(t in seq(1, maxIter, 1)){
    barrier[t] = initialBarrier / (1 + barrierDecay * t)
  }
  
  lottery_mu = dLott * distortedEVDiff
  
  while (time<maxIter){
    
    # If the arbitrator RDV hits one of the barriers make decision
    if (lotteryRDV >= barrier[time] | lotteryRDV <= -barrier[time]){
      
      # Convert ms back to secs
      RT = (time * timeStep)/1000 
      
      if (lotteryRDV >= barrier[time]){
        choice = "left"
      } else if (lotteryRDV <= -barrier[time]){
        choice = "right"
      }
      break
    } 
    
    # Otherwise continue sampling evidence
    if (elapsedNDT < nonDecIters){
      elapsedNDT = elapsedNDT + 1
    } else{
      lotteryRDV = lotteryRDV + rnorm(1, lottery_mu, sigmaLott)
    }
    
    if (debug){
      debug_row = data.frame(time = time, lotteryRDV = round(lotteryRDV, 3))
      debug_df = rbind(debug_df, debug_row)
    }
    
    # Increment sampling iteration
    time = time + 1
  }
  
  out = data.frame(distortedEVDiff = distortedEVDiff, choice=choice, reactionTime = RT, dLott=dLott, sigmaLott=sigmaLott, barrier=barrier[time], nonDecisionTime=nonDecisionTime, bias=bias, timeStep=timeStep, maxIter=maxIter)
  
  if(debug){
    out = list(out=out, debug_df=debug_df)
  }
  
  return(out)
}

```


```{r}
dLott = 0.03
distortedEVDiff = .4
sigmaLott = .05
n_particles = 250
sim_data = data.frame()

for(i in 1:n_particles){
  tmp = run_lott_integrator(dLott = dLott, sigmaLott = sigmaLott, distortedEVDiff =distortedEVDiff)$debug_df
  tmp$iter = i
  sim_data = rbind(sim_data, tmp)
}
```

Plot the time course of some particles.

```{r}
sim_data %>%
  filter(iter<20)%>%
  ggplot(aes(time, lotteryRDV, group=iter))+
  geom_line(color="gray")+
  scale_x_continuous(breaks = seq(0, 400, 1))+
  # scale_y_continuous(breaks = round(states,3))+
  scale_y_continuous(breaks = seq(-1,1,.1))+
  geom_hline(aes(yintercept=0), linetype="dashed")+
  geom_hline(aes(yintercept=1))+
  geom_hline(aes(yintercept=-1))+
  theme(panel.grid.minor = element_blank(), 
        axis.text = element_blank(),
        axis.ticks = element_blank())
```

Plot RT distribution and how many particles have crossed at certain time points

```{r}
sim_data %>%
  group_by(iter) %>%
  filter(time == max(time)) %>%
  ggplot(aes(time))+
  geom_histogram(bins=30)

sim_data %>%
  group_by(iter) %>%
  filter(lotteryRDV == max(lotteryRDV)) %>%
  ungroup() %>%
  group_by(time) %>%
  summarise(num_passed = n()) %>%
  ungroup() %>%
  mutate(cum_passed = cumsum(num_passed)) %>%
  ggplot(aes(time,cum_passed))+
  geom_bar(stat="identity")
```

Compute which discrete state each particle would be in based on the (continuous) RDV

```{r}
sim_data = sim_data %>%
  mutate(state_bin = cut(lotteryRDV, breaks =seq(-1.1,1.1,.1),labels=FALSE),
         state_bin = ifelse(is.na(state_bin) & lotteryRDV>1, max(state_bin, na.rm=T), 
                            ifelse(is.na(state_bin) & lotteryRDV<(-1), min(state_bin, na.rm=T), state_bin)))
```

What is the probability of a particle being in any given state bin at each time bin for a given set of drift and diffusion parameters for the single integrator?

```{r}
est_data = data.frame()

for(i in 0:max(sim_data$time)){
  tmp = data.frame(state_lb = seq(-1,.9, .1), state_ub = seq(-0.9,1,.1)) %>%
    rbind(data.frame(state_lb = -Inf, state_ub = -1)) %>%
    rbind(data.frame(state_lb = 1, state_ub = Inf)) %>%
    arrange(state_lb) %>%
    mutate(state_bin=1:n(), 
           time=i,
           cdf_lb = pnorm(state_lb,mean=dLott*distortedEVDiff*i, sd=sqrt(sigmaLott^2*i)),
           cdf_ub = pnorm(state_ub,mean=dLott*distortedEVDiff*i, sd=sqrt(sigmaLott^2*i)),
           p_bin = cdf_ub-cdf_lb)
  
  est_data = rbind(est_data, tmp)
}

```

*Are the simulated particles distributed in the predicted manner across time and space?*

```{r}
demo_data = sim_data %>%
  group_by(iter) %>%
  mutate(bound_state = ifelse(max(state_bin) == 22, 22, ifelse(min(state_bin) == 1, 1, NA))) %>%
  ungroup() %>%
  # to compute the correct proportion of particles in each bin after particles begin crossing a boundary add additional rows for the remaining time points placing that particle on the state above the boundary it had crossed
  right_join(expand.grid(c(1:max(sim_data$iter)), c(0:max(sim_data$time))) %>%
               rename(iter=Var1, time=Var2), by=c("iter", "time")) %>%
  mutate(state_bin = ifelse(is.na(state_bin), unique(na.omit(bound_state)), state_bin)) %>%
  group_by(time, state_bin) %>%
  summarise(num_particles = n(),
            prop_particles = num_particles/n_particles,
            .groups='keep') %>%
  # add column identifying it is simulated data
  mutate(data_type="obs") %>%
  # remove columns that won't be used
  select(time, state_bin, prop_particles, data_type) %>%
  # append predicted data renaming a column to match the simulated data
  rbind(est_data %>%
          mutate(data_type="pred") %>%
          rename(prop_particles=p_bin) %>%
          select(time, state_bin, prop_particles, data_type))
```

```{r}
demo_data %>%
  # filter(time>0 & time < 11) %>%
  filter(time>20 & time < 31) %>%
  ggplot(aes(state_bin, prop_particles, fill=data_type))+
  geom_bar(stat="identity", position="identity", alpha=0.5)+
  facet_grid(.~time)+
  coord_flip()+
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank(),
        legend.position = "bottom")+
  ylab("")+
  xlab("prop particles / state")+
  labs(fill="")+
  scale_x_continuous(breaks=c(.5:22.5)) +
  geom_vline(aes(xintercept=11.5), linetype="dashed")+
  geom_vline(aes(xintercept=22))+
  geom_vline(aes(xintercept=1))
```

Plot predicted versus observed proportion/probability of being in each state (panels) at each time point (colors)

```{r}
demo_data %>%
  spread(data_type, prop_particles) %>%
  mutate(obs=ifelse(is.na(obs),0, obs)) %>%
  ggplot(aes(pred, obs, color=time))+
  geom_point()+
  geom_abline(aes(intercept=0, slope=1))+
  # facet_wrap(~state_bin, scales = "free")+
  facet_wrap(~state_bin)+
  theme(legend.position = "none",
        axis.ticks = element_blank(),
        panel.grid = element_blank(),
        axis.text = element_blank())
```

```{r}
rm(run_lott_integrator, sim_data, est_data, demo_data)
```

# Recovery checks

## One integrator

```{r}
source(paste0(helpers_path, 'ddModels/r_ddm_models/ddm_oneIntegrator_sepProbDistortion.R'))
```

### Same trial

```{r}
d = 0.03
distortedEVDiff = .3
distortedQVDiff = .1
sigma = .05
n_particles = 250
sim_data = data.frame()

for(i in 1:n_particles){
  tmp = sim_trial(d = d, sigma = sigma, distortedEVDiff =distortedEVDiff, distortedQVDiff =distortedQVDiff, EVLeft = NA, EVRight = NA, QVLeft = NA, QVRight = NA, probFractalDraw = NA)
  tmp$iter = i
  sim_data = rbind(sim_data, tmp)
}
```

```{r}
fit_trial_list = list(model1 = fit_trial)

d_range = seq(.01, .1, .01)
sigma_range = seq(.01, .1, .01)

recovery_df = data.frame(test_d = NA, test_sigma = NA, nll = NA)

for(i in 1:length(d_range)){
  for(j in 1:length(sigma_range)){
    test_d = d_range[i]
    test_sigma = sigma_range[j]
    test_nll = get_task_nll(data_ = sim_data, par = c(test_d, test_sigma), par_names_ = c("d", "sigma"), model_name = "model1", fix_pars_ = list())
    recovery_df = rbind(recovery_df,
                        data.frame(test_d = test_d, test_sigma = test_sigma, nll = test_nll))
    
  }
}
recovery_df = recovery_df %>% drop_na()
```


```{r}
recovery_df %>%
  mutate(test_d = as.factor(test_d),
         test_sigma = as.factor(test_sigma)) %>%
  ggplot(aes(test_d, test_sigma, fill=nll))+
  geom_tile()

#Zoom in
recovery_df %>%
  filter(test_sigma>.04 & test_d < .06)%>%
  mutate(test_d = as.factor(test_d),
         test_sigma = as.factor(test_sigma)) %>%
  ggplot(aes(test_d, test_sigma, fill=nll))+
  geom_tile()
```

### Different trials (~single subject)


```{r}
sim_trial_list = list()
sim_trial_list[['model1']] = sim_trial
```

```{r}
n_trials = 300
stimuli = clean_beh_data[1:n_trials,] %>%
  select(leftLotteryEV, rightLotteryEV, leftQValue, rightQValue, probFractalDraw, reactionTime, choiceLeft, subnum, distorted_ev_diff, distorted_qv_diff) %>%
  rename(EVLeft = leftLotteryEV, EVRight = rightLotteryEV, QVLeft = leftQValue, QVRight = rightQValue, distortedEVDiff = distorted_ev_diff, distortedQVDiff = distorted_qv_diff)
sim_subj = sim_task(stimuli, model_name = "model1", d=d, sigma=sigma)
# sim_subj
```

```{r}
d_range = seq(.01, .1, .02)
sigma_range = seq(.01, .1, .02)

recovery_df = data.frame(test_d = NA, test_sigma = NA, nll = NA)

for(i in 1:length(d_range)){
  for(j in 1:length(sigma_range)){
    test_d = d_range[i]
    test_sigma = sigma_range[j]
    test_nll = get_task_nll(data_ = sim_subj, par = c(test_d, test_sigma), par_names_ = c("d", "sigma"), model_name = "model1", fix_pars_ = list())
    recovery_df = rbind(recovery_df,
                        data.frame(test_d = test_d, test_sigma = test_sigma, nll = test_nll))
    
  }
}
recovery_df = recovery_df %>% drop_na()

```

```{r}
recovery_df %>%
  mutate(test_d = as.factor(test_d),
         test_sigma = as.factor(test_sigma)) %>%
  ggplot(aes(test_d, test_sigma, fill=nll))+
  geom_tile()

recovery_df %>%
  filter(test_sigma > .03) %>%
  mutate(test_d = as.factor(test_d),
         test_sigma = as.factor(test_sigma)) %>%
  ggplot(aes(test_d, test_sigma, fill=nll))+
  geom_tile()
```




## Three integrators

```{r}
source(paste0(helpers_path, 'ddModels/r_ddm_models/ddm_threeIntegrators_sepProbDistortion.R'))
```

### Same trial

```{r}
d = 0.03
distortedEVDiff = .3
distortedQVDiff = .1
sigma = .05
n_particles = 250
sim_data = data.frame()

for(i in 1:n_particles){
  tmp = sim_trial(dLott=0.03, dFrac=0.06, dArb=0.05, sigmaLott = 0.03, sigmaFrac = 0.03, distortedEVDiff =distortedEVDiff, distortedQVDiff =distortedQVDiff, EVLeft = NA, EVRight = NA, QVLeft = NA, QVRight = NA, probFractalDraw = .5, barrierDecay = 0)
  tmp$iter = i
  sim_data = rbind(sim_data, tmp)
}
```

```{r}
fit_trial_list = list(model3 = fit_trial)

dLott_range = c(.01, .03, .09)
dFrac_range = c(.01, .06, .09)
dArb_range = c(.01, .05, .09)
sigma_fix = .03

recovery_df = data.frame(test_dLott = NA, test_dFrac = NA, test_dArb = NA, nll = NA)

for(i in 1:length(dLott_range)){
  for(j in 1:length(dFrac_range)){
    for(k in 1:length(dArb_range)){
      test_dLott = dLott_range[i]
      test_dFrac = dFrac_range[j]
      test_dArb = dArb_range[k]
      test_nll = get_task_nll(data_ = sim_data, par = c(test_dLott, test_dFrac, test_dArb, sigma_fix, sigma_fix), par_names_ = c("dLott", "dFrac", "dArb","sigmaLott", "sigmaFrac"), model_name = "model3", fix_pars_ = list())
      recovery_df = rbind(recovery_df,
                          data.frame(test_dLott = test_dLott, test_dFrac = test_dFrac, test_dArb = test_dArb, nll = test_nll))
    }
  }
}
recovery_df = recovery_df %>% drop_na()
```


### Folded distribution schematic representation

Use the debug df from simulating a single trial with this model

```{r}
tmp = sim_trial(dLott=0.03, dFrac=0.04, dArb=0.04, sigmaLott = 0.03, sigmaFrac = 0.03, distortedEVDiff = sub_data$distortedEVDiff[100], distortedQVDiff = sub_data$distortedQVDiff[100], probFractalDraw = sub_data$probFractalDraw[100], barrierDecay = 0, EVLeft = sub_data$EVLeft[100], EVRight = sub_data$EVRight[100], QVLeft = sub_data$QVLeft[100], QVRight = sub_data$QVRight[100], debug=T)

tmp$debug_df %>%
  select(time, arbitratorRDV, lotteryRDV, fractalRDV) %>%
  gather(key, value, -time) %>%
  ggplot(aes(time, value, color=key))+
  geom_line()+
  geom_hline(aes(yintercept=1))+
  geom_hline(aes(yintercept=-1))+
  geom_hline(aes(yintercept=0), linetype="dashed")+
  theme(legend.position = "bottom",
        panel.grid = element_blank())+
  labs(color="")
```

```{r}
tmp
```

## TBD

- What do you with these for the neuroimaging analyses?
- Model all subjects together? Or can you get hierarchical Bayesian posteriors?
- Are the individual parameter needed for anything or is the more interesting question one of model comparison to explain the RT patterns?