---
title: 'Experience vs. description based decision-making project: Revisiting DD models'
output:
  html_document:
    toc: yes
    toc_float: yes
    code_folding: 'hide'
  pdf_document:
    toc: yes
---


```{r include=FALSE}
library(tidyverse)
theme_set(theme_bw())
library(here)
cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
helpers_path = here('analysis/helpers/')

source(paste0(helpers_path,'ddModels/sim_task.R'))
source(paste0(helpers_path,'ddModels/fit_task.R'))
source(paste0(helpers_path,'01_clean_behavioral_data.R'))
source(paste0(helpers_path, 'get_qvals.R'))
source(paste0(helpers_path,'optimPostProcess/sim_sanity_checks.R'))


set.seed(38573)
```

Add distorted value estimates using the hierarchical RL fit.

```{r message=FALSE, warning=FALSE}

source(paste0(helpers_path, 'rlModels/fit_rl_hierarchical_oneParamDoubleSymmLinearProbDistortion_rpeBoth.R'))

clean_beh_data = par_ests %>%
  group_by(subnum, par) %>%
  summarise(est = mean(value), .groups='keep') %>%
  spread(par, est) %>%
  left_join(clean_beh_data, by='subnum')

## Add Q values of fractals to each trial
clean_beh_data = clean_beh_data %>%
  group_by(subnum) %>%
  do(get_qvals(., model_name="original")) %>%
  ungroup()

clean_beh_data = clean_beh_data %>%
  mutate(rightLotteryEV = referenceProb * referenceValue,
         leftLotteryEV = lotteryValue * lotteryProb,
         lottery_ev_diff = leftLotteryEV - rightLotteryEV,
         fractal_qv_diff = leftQValue - rightQValue,
         distorted_ev_diff = (1-theta)*(1-probFractalDraw)*lottery_ev_diff,
         distorted_qv_diff = theta*probFractalDraw*fractal_qv_diff)
```

**Potential problem** for 3 integrator model: The model would predict slower decisions because the difference in the absolute value of each attribute integrator RDV would be small.

**BUT** by design there aren't trials that provide strong (distorted) evidence for a single side so for the current set of stimuli this should not be a concern.

```{r}
clean_beh_data %>%
  ggplot(aes(distorted_ev_diff, distorted_qv_diff)) +
  geom_point()+
  geom_abline(aes(slope=1, intercept=0))
```

# The 3 integrator model

Source trial simulating and fitting function three integrator model. The model only had the ddm parameters as variables; choice parameters (learning rate and probability distortion) are fitted elsewhere and used to compute the distorted value differences fed into this model.

```{r}
sim_trial_list = list()
source(paste0(helpers_path, 'ddModels/r_ddm_models/ddm_model4b_separatedProbDistortion.R'))
sim_trial_list[['model4b']] = sim_trial
```

Test if trial simulating function works.

```{r}
sub_data = clean_beh_data %>%
  filter(subnum  %in% c("01", "03", "05","07", "09", "11", "13", "15", "17", "19")) %>%
  select(leftLotteryEV, rightLotteryEV, leftQValue, rightQValue, probFractalDraw, reactionTime, choiceLeft, subnum, distorted_ev_diff, distorted_qv_diff) %>%
  rename(EVLeft = leftLotteryEV, EVRight = rightLotteryEV, QVLeft = leftQValue, QVRight = rightQValue, distortedEVDiff = distorted_ev_diff, distortedQVDiff = distorted_qv_diff)
```

```{r}
sim_trial(dLott=0.03, dFrac=0.04, dArb=0.04, sigmaLott = 0.03, sigmaFrac = 0.03, distortedEVDiff = sub_data$distortedEVDiff[100], distortedQVDiff = sub_data$distortedQVDiff[100], probFractalDraw = sub_data$probFractalDraw[100], barrierDecay = 0, EVLeft = sub_data$EVLeft[100], EVRight = sub_data$EVRight[100], QVLeft = sub_data$QVLeft[100], QVRight = sub_data$QVRight[100])
```

## Folded distribution schematic representation

Use the debug df from simulating a single trial with this model

```{r}
tmp = sim_trial(dLott=0.03, dFrac=0.04, dArb=0.04, sigmaLott = 0.03, sigmaFrac = 0.03, distortedEVDiff = sub_data$distortedEVDiff[100], distortedQVDiff = sub_data$distortedQVDiff[100], probFractalDraw = sub_data$probFractalDraw[100], barrierDecay = 0, EVLeft = sub_data$EVLeft[100], EVRight = sub_data$EVRight[100], QVLeft = sub_data$QVLeft[100], QVRight = sub_data$QVRight[100], debug=T)

tmp$debug_df %>%
  select(time, arbitratorRDV, lotteryRDV, fractalRDV) %>%
  gather(key, value, -time) %>%
  ggplot(aes(time, value, color=key))+
  geom_line()+
  geom_hline(aes(yintercept=1))+
  geom_hline(aes(yintercept=-1))+
  geom_hline(aes(yintercept=0), linetype="dashed")+
  theme(legend.position = "bottom",
        panel.grid = element_blank())+
  labs(color="")
```

```{r}
tmp
```


## Aggregate checks 

Simulate data with a 3 integrator model.

Simulate a dataset using half the stimuli filtered earlier and this three integrator model.

**Note: this selection of parameters provides a nice qualitative fit to data. It assumes integration is slower for lotteries than for fractals. This is one way of capturing the difference in processing lottery values, which can only be done after the stimulus presentation, as apposed to fractal values which are learned about trialwise. There are alternative ways of modeling this: e.g. integration for the fractal integrator might start before the stimulus presentation screen; ndt for the lottery integrator (or the arbitrator when the lottery is more relevant) might be longer etc. The way to determine which of these hypotheses is most supported by data would be a model comparison.**

```{r}
m4b_1 = sim_task(sub_data, model_name = "model4b", dLott=0.03, dFrac=0.06, dArb=0.05, sigmaLott = 0.03, sigmaFrac = 0.03)
# m4b_1 = sim_task(sub_data, model_name = "model4b", dLott=0.01, dFrac=0.02, dArb=0.015, sigmaLott = 0.01, sigmaFrac = 0.01)
```

```{r}
sim_sanity_checks(m4b_1, checks=c(1,3,4,5,6,7,8), compare_logits = TRUE)
```


## Question: Absolute check

Are the predicted response times close to the true response times? Not really. Despite the apparent correspondence when looking at aggregate data grouped by probFractalDraw level the individual trial estimates are not correlated at all.

Does a well-established model do better in such an absolute check of model fit?

```{r}
m4b_1 %>%
  left_join(sub_data, by=c("EVLeft", "EVRight", "QVLeft", "QVRight", "distortedEVDiff", "distortedQVDiff", "probFractalDraw")) %>%
  ggplot(aes(reactionTime.y, reactionTime.x))+
  geom_point(color="light gray")+
  geom_smooth(formula="y~x", method="lm")+
  geom_abline(aes(slope=1, intercept=0), linetype="dashed")+
  xlab("True RT")+
  ylab("Predicted RT")
```

# Sum of random variables approach to drift rate and noise estimation

Given the choice and RT for a trial the likelihood of ...

**Why might this not have been used previously? Some ideas:**  
**The boundary in traditional DDM is not fixed at 1 and -1. Instead it is a parameter. It's not immediately apparent to me **

## Demonstration

Assuming drift rate and noise parameters for a single integrator we simulate n particles for the same trial (i.e. same value difference) and make a histogram of a particles in each state at each time step.

```{r}
initialBarrier = 1
approxStateStep = .1
halfNumStateBins = round(initialBarrier / approxStateStep)
stateStep = initialBarrier / (halfNumStateBins + 0.5)

# The vertical axis is divided into states.
states = seq(-1*(initialBarrier) + (stateStep / 2), initialBarrier - (stateStep / 2), stateStep)
```

```{r}
run_lott_integrator = function(dLott, sigmaLott, distortedEVDiff, nonDecisionTime=0, bias=0, barrierDecay = 0, timeStep=10, barrier = 1, maxIter = 400, debug=TRUE){
  
  if(debug){
    debug_df = data.frame(time=0, lotteryRDV=0)
  }
  
  lotteryRDV = bias
  time = 1
  elapsedNDT = 0
  choice = 0
  RT = NA
  nonDecIters = nonDecisionTime / timeStep
  initialBarrier = barrier
  barrier = rep(initialBarrier, maxIter)
  
  # The values of the barriers can change over time
  for(t in seq(1, maxIter, 1)){
    barrier[t] = initialBarrier / (1 + barrierDecay * t)
  }
  
  lottery_mu = dLott * distortedEVDiff
  
  while (time<maxIter){
    
    # If the arbitrator RDV hits one of the barriers make decision
    if (lotteryRDV >= barrier[time] | lotteryRDV <= -barrier[time]){
      
      # Convert ms back to secs
      RT = (time * timeStep)/1000 
      
      if (lotteryRDV >= barrier[time]){
        choice = "left"
      } else if (lotteryRDV <= -barrier[time]){
        choice = "right"
      }
      break
    } 
    
    # Otherwise continue sampling evidence
    if (elapsedNDT < nonDecIters){
      elapsedNDT = elapsedNDT + 1
    } else{
      lotteryRDV = lotteryRDV + rnorm(1, lottery_mu, sigmaLott)
    }
    
    if (debug){
      debug_row = data.frame(time = time, lotteryRDV = round(lotteryRDV, 3))
      debug_df = rbind(debug_df, debug_row)
    }
    
    # Increment sampling iteration
    time = time + 1
  }
  
  out = data.frame(distortedEVDiff = distortedEVDiff, choice=choice, reactionTime = RT, dLott=dLott, sigmaLott=sigmaLott, barrier=barrier[time], nonDecisionTime=nonDecisionTime, bias=bias, timeStep=timeStep, maxIter=maxIter)
  
  if(debug){
    out = list(out=out, debug_df=debug_df)
  }
  
  return(out)
}

```


```{r}
dLott = 0.03
distortedEVDiff = .4
# distortedEVDiff = 0
sigmaLott = .05
# n_particles = 1000
n_particles = 250
sim_data = data.frame()

for(i in 1:n_particles){
  tmp = run_lott_integrator(dLott = dLott, sigmaLott = sigmaLott, distortedEVDiff =distortedEVDiff)$debug_df
  tmp$iter = i
  sim_data = rbind(sim_data, tmp)
}
```

Plot the time course of each particle.

```{r}
sim_data %>%
  filter(iter<20)%>%
  ggplot(aes(time, lotteryRDV, group=iter))+
  geom_line(color="gray")+
  scale_x_continuous(breaks = seq(0, 400, 1))+
  scale_y_continuous(breaks = round(states,3))+
  geom_hline(aes(yintercept=0), linetype="dashed")+
  geom_hline(aes(yintercept=1))+
  geom_hline(aes(yintercept=-1))+
  theme(panel.grid.minor = element_blank(), 
        axis.text = element_blank())
```

Plot RT distribution and how many particles have crossed at certain time points

```{r}
sim_data %>%
  group_by(iter) %>%
  filter(time == max(time)) %>%
  ggplot(aes(time))+
  geom_histogram(bins=30)

sim_data %>%
  group_by(iter) %>%
  filter(lotteryRDV == max(lotteryRDV)) %>%
  ungroup() %>%
  group_by(time) %>%
  summarise(num_passed = n()) %>%
  ungroup() %>%
  mutate(cum_passed = cumsum(num_passed)) %>%
  ggplot(aes(time,cum_passed))+
  geom_bar(stat="identity")
```

Compute which discrete state each particle would be in based on the (continuous) RDV

```{r}
# for(i in 1:nrow(sim_data)){
#   sim_data$state_bin[i] = which.min(abs(states - sim_data$lotteryRDV[i]))
# }

for(i in 1:nrow(sim_data)){
  # if(sim_data$lotteryRDV[i]==0){
  #   sim_data$lotteryRDV[i] = runif(1, min= (-0.01),max=0.01)
  # }
  
  sim_data$state_bin[i] = which.min(abs(seq(-1,1,stateStep) - sim_data$lotteryRDV[i]))
}
```

Proportion of particles crossed at each time bin versus the proportion I expected to pass.

```{r}
sim_data %>%
  group_by(iter) %>%
  filter(lotteryRDV == max(lotteryRDV)) %>%
  ungroup() %>%
  group_by(time) %>%
  summarise(num_passed = n()) %>%
  ungroup() %>%
  mutate(cum_passed = cumsum(num_passed),
         prop_passed = cum_passed/n_particles,
         exp_passed = pnorm(1, mean = dLott*distortedEVDiff*time , sd = sqrt((sigmaLott^2)*time), lower.tail = F)) %>%
  ggplot(aes(exp_passed, prop_passed))+
  geom_point()+
  geom_abline(aes(intercept=0, slope=1))
```

What is the probability of a particle being in any given state bin at each time bin for a given set of drift and diffusion parameters for the single integrator?

```{r}
dLott = 0.03
distortedEVDiff = .4
# distortedEVDiff = 0
sigmaLott = .05

est_data = data.frame()

for(i in 0:max(sim_data$time)){
  tmp = data.frame(state_lb = seq(-1,.9, .1), state_ub = seq(-0.9,1,.1)) %>%
    rbind(data.frame(state_lb = -Inf, state_ub = -1)) %>%
    rbind(data.frame(state_lb = 1, state_ub = Inf)) %>%
    arrange(state_lb) %>%
    mutate(state_bin=1:n(), 
           time=i,
           cdf_lb = pnorm(state_lb,mean=dLott*distortedEVDiff*i, sd=sqrt(sigmaLott^2*i)),
           cdf_ub = pnorm(state_ub,mean=dLott*distortedEVDiff*i, sd=sqrt(sigmaLott^2*i)),
           p_bin = cdf_ub-cdf_lb)
  
 est_data = rbind(est_data, tmp)
}
```

*Are the simulated particles distributed in the predicted manner across time and space?*

```{r}
demo_data = sim_data %>%
  group_by(iter) %>%
  mutate(bound_state = ifelse(max(state_bin) == 22, 22, ifelse(min(state_bin) == 1, 1, NA))) %>%
  ungroup() %>%
  # to compute the correct proportion of particles in each bin after particles begin crossing a boundary add additional rows for the remaining time points placing that particle on the state above the boundary it had crossed
  right_join(expand.grid(c(1:max(sim_data$iter)), c(0:max(sim_data$time))) %>%
               rename(iter=Var1, time=Var2), by=c("iter", "time")) %>%
  mutate(state_bin = ifelse(is.na(state_bin), unique(na.omit(bound_state)), state_bin)) %>%
  group_by(time, state_bin) %>%
  summarise(num_particles = n(),
            prop_particles = num_particles/n_particles,
            .groups='keep') %>%
  # add column identifying it is simulated data
  mutate(data_type="obs") %>%
  # remove columns that won't be used
  select(time, state_bin, prop_particles, data_type) %>%
  # append predicted data renaming a column to match the simulated data
  rbind(est_data %>%
          mutate(data_type="pred") %>%
          rename(prop_particles=p_bin) %>%
          select(time, state_bin, prop_particles, data_type))
```

```{r}
demo_data %>%
  # filter(time>0 & time < 11) %>%
  filter(time>20 & time < 31) %>%
  ggplot(aes(state_bin, prop_particles, fill=data_type))+
  geom_bar(stat="identity", position="identity", alpha=0.5)+
  facet_grid(.~time)+
  coord_flip()+
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank(),
        legend.position = "bottom")+
  ylab("")+
  xlab("prop particles / state")+
  labs(fill="")+
  scale_x_continuous(breaks=c(.5:22.5)) +
  geom_vline(aes(xintercept=11.5), linetype="dashed")+
  geom_vline(aes(xintercept=22))+
  geom_vline(aes(xintercept=1))
```

Plot predicted versus observed proportion/probability of being in each state (panels) at each time point (colors)

```{r}
demo_data %>%
  spread(data_type, prop_particles) %>%
  mutate(obs=ifelse(is.na(obs),0, obs)) %>%
  ggplot(aes(pred, obs, color=time))+
  geom_point()+
  geom_abline(aes(intercept=0, slope=1))+
  # facet_wrap(~state_bin, scales = "free")+
  facet_wrap(~state_bin)+
  theme(legend.position = "none",
        axis.ticks = element_blank(),
        panel.grid = element_blank(),
        axis.text = element_blank())
```

To use this logic for parameter estimation we would maximize the likelihood of crossing the boundary corresponding to the choice at the observed response time. So we need to confirm that the likelihood of each simulated particle hitting the boundary at the simulated timestep is higher for the true parameter combination is than the likelihood computed using an incorrect parameter computation.

## Likelihood checks

For this single integrator is the density (/likelihood) at time step == true rt always higher for the true parameter combination compared to a wrong parameter combo? Or is the density always larger for something stupid like the size of the mean?

The density of a particle that has crossed a barrier at time t is `dnorm(crossed barrier, mean=drift rate * t, sd = sqrt(sigma ^ 2 * t))`.

Here I merged the data on when each particle crossed the barrier with the estimated densities of being at the state closest to the boundary at that time point given the true drift rate and sigma. So these aren't quiet right. More accurately I could compute the density for the `lotteryRDV` for each particle at the time they've crossed the boundary. Still, in empirical data I will only have choice which I translate to a boundary so this is probably a close enough approximation.

```{r}
true_liks = sim_data %>%
  group_by(iter) %>%
  filter(time == max(time)) %>%
  left_join(est_data %>%
  filter(states>.95), by="time") # this isn't full proof. ideally you'd merge on time and statebin since it's possible that the other boundary was hit
```

Having larger scaling factor doesn't translate higher density. The plot shows the computed densities of crossing the boundary at the time each particle did using the true and incorrect parameters. For most particles the true density is higher.

```{r}
false_dLott = 0.08
distortedEVDiff = .4
false_sigmaLott = .08

false_est_data = data.frame()

for(i in 0:max(sim_data$time)){
 tmp=data.frame(time = i, 
            states = states, 
            pdf_states = dnorm(states, mean=false_dLott*distortedEVDiff*i, sd=sqrt(false_sigmaLott^2*i)))
 false_est_data = rbind(false_est_data, tmp)
}

false_liks = sim_data %>%
  group_by(iter) %>%
  filter(time == max(time)) %>%
  left_join(false_est_data %>%
  filter(states>.95), by="time")

true_liks %>%
  mutate(lik_type = "true") %>%
  rbind(false_liks %>% mutate(lik_type="false")) %>%
  select(iter, pdf_states, lik_type) %>%
  spread(lik_type, pdf_states) %>%
  ggplot(aes(true, false))+
  geom_point()+
  geom_abline(aes(intercept=0, slope=1))
```

Even with false parameters that are relatively close to the true ones the true pdf's are higher for most particles.

```{r}
false_dLott = 0.04
# false_dLott = 0.03
distortedEVDiff = .4
false_sigmaLott = .06
# false_sigmaLott = .01

false_est_data = data.frame()

for(i in 0:max(sim_data$time)){
 tmp=data.frame(time = i, 
            states = states, 
            pdf_states = dnorm(states, mean=false_dLott*distortedEVDiff*i, sd=sqrt(false_sigmaLott^2*i)))
 false_est_data = rbind(false_est_data, tmp)
}

false_liks = sim_data %>%
  group_by(iter) %>%
  filter(time == max(time)) %>%
  left_join(false_est_data %>%
  filter(states>.95), by="time")

true_liks %>%
  mutate(lik_type = "true") %>%
  rbind(false_liks %>% mutate(lik_type="false")) %>%
  select(iter, pdf_states, lik_type) %>%
  spread(lik_type, pdf_states) %>%
  ggplot(aes(true, false))+
  geom_point()+
  geom_abline(aes(intercept=0, slope=1))
```

## Different trials (~single subject)

Try it for a set of distorted EV differences (akin to a subject). Note this is integrating only on lotteries so not simulating/modeling the actual task yet.

```{r}
n_trials = 300
sim_subj = data.frame()
true_dLott = 0.03
true_sigmaLott = 0.05

for(i in 1:n_trials){
  tmp = run_lott_integrator(dLott = true_dLott, sigmaLott = true_sigmaLott, distortedEVDiff = clean_beh_data$distorted_ev_diff[i])$debug_df
  tmp$iter = i
  sim_subj = rbind(sim_subj, tmp)
}
```

Plot RDV time course for some trials.

```{r}
sim_subj %>%
  filter(iter<10)%>%
  ggplot(aes(time, lotteryRDV, group=iter))+
  geom_line(color="gray")+
  scale_x_continuous(breaks = seq(0, 400, 1))+
  scale_y_continuous(breaks = round(states,3))+
  geom_hline(aes(yintercept=0), linetype="dashed")+
  geom_hline(aes(yintercept=1))+
  geom_hline(aes(yintercept=-1))+
  theme(panel.grid.minor = element_blank(), 
        axis.text = element_blank())
```

Compute the probability density of being in the given state at the time the integrator terminated for each trial using the true drift rate and sigma. This time using the actual RDV to compute the density but note again that this latent variable won't exist in empirical data so we'll assume an RDV of 1 or -1 depending on the boundary.

```{r}
sim_subj_rts = sim_subj %>%
  group_by(iter) %>%
  filter(time == max(time))

subj_liks = data.frame()

for(i in 1:nrow(sim_subj_rts)){
  trial_time = sim_subj_rts$time[i]
  trial_rdv = sim_subj_rts$lotteryRDV[i]
  trial_evDiff = clean_beh_data$distorted_ev_diff[i]
 tmp=data.frame(iter = sim_subj_rts$iter[i],
                time = trial_time,
                rdv = trial_rdv,
                pdf_rdv = dnorm(trial_rdv, mean=true_dLott*trial_evDiff*trial_time, sd=sqrt(true_sigmaLott^2*trial_time)))
 subj_liks = rbind(subj_liks, tmp)
}
```

The sum of these would be the likelihood of the data (so are these like elpd's - expected log prob densities?)

```{r}
sum(subj_liks$pdf_rdv)
```

Confirm that this sum of likelihoods for each trial would be lower if you used the incorrect drift rate and sigma. **Doesn't do a good job estimating the noise**

```{r}
false_dLott = 0.05
false_sigmaLott = 0.05
false_subj_liks = data.frame()

for(i in 1:nrow(sim_subj_rts)){
  trial_time = sim_subj_rts$time[i]
  trial_rdv = sim_subj_rts$lotteryRDV[i]
  trial_evDiff = clean_beh_data$distorted_ev_diff[i]
 tmp=data.frame(iter = sim_subj_rts$iter[i],
                time = trial_time,
                rdv = trial_rdv,
                pdf_rdv = dnorm(trial_rdv, mean=false_dLott*trial_evDiff*trial_time, sd=sqrt(false_sigmaLott^2*trial_time)))
 false_subj_liks = rbind(false_subj_liks, tmp)
}

sum(false_subj_liks$pdf_rdv)
```

## Three integrators



```{r}
get_rdv_pdf = function(data, test_d, test_sigma){
  data = data %>%
    mutate(rdv = ifelse(choice == "left", 1, -1),
           num_timeSteps = round(reactionTime*1000/timeStep),
           trial_drift = distortedEVDiff + distortedQVDiff,
           pdf_rdv = dnorm(rdv, mean=test_d*trial_drift*num_timeSteps, sd=sqrt(test_sigma^2*num_timeSteps)))
  
  return(sum(data$pdf_rdv))
         
}

```


## TBD

- What do you with these for the neuroimaging analyses?
- Model all subjects together? Or can you get hierarchical Bayesian posteriors?
- Are the individual parameter needed for anything or is the more interesting question one of model comparison to explain the RT patterns?