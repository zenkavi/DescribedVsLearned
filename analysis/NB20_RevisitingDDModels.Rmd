---
title: 'Experience vs. description based decision-making project: Revisiting DD models'
output:
  html_document:
    toc: yes
    toc_float: yes
    code_folding: 'hide'
  pdf_document:
    toc: yes
---

```{r include=FALSE}
library(tidyverse)
theme_set(theme_bw())
library(here)
cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
helpers_path = here('analysis/helpers/')

source(paste0(helpers_path,'ddModels/sim_task.R'))
source(paste0(helpers_path,'ddModels/fit_task.R'))
source(paste0(helpers_path,'01_clean_behavioral_data.R'))
source(paste0(helpers_path, 'get_qvals.R'))
source(paste0(helpers_path,'optimPostProcess/sim_sanity_checks.R'))


set.seed(38573)
```

Add distorted value estimates using the hierarchical RL fit.

```{r message=FALSE, warning=FALSE}
# See NB19_RevisitingRLModels.Rmd before deciding on which model to import
# source(paste0(helpers_path, 'rlModels/fit_rl_hierarchical_oneParamDoubleSymmLinearProbDistortion_rpeBoth.R'))
source(paste0(helpers_path, 'rlModels/fit_rl_hierarchical_oneParamSymmLinearProbDistortion_rpeBoth.R'))

clean_beh_data = par_ests %>%
  group_by(subnum, par) %>%
  summarise(est = mean(value), .groups='keep') %>%
  spread(par, est) %>%
  left_join(clean_beh_data, by='subnum')

## Add Q values of fractals to each trial
clean_beh_data = clean_beh_data %>%
  group_by(subnum) %>%
  do(get_qvals(., model_name="rpeBoth")) %>%
  ungroup()

clean_beh_data = clean_beh_data %>%
  mutate(rightLotteryEV = referenceProb * referenceValue,
         leftLotteryEV = lotteryValue * lotteryProb,
         lottery_ev_diff = leftLotteryEV - rightLotteryEV,
         fractal_qv_diff = leftQValue - rightQValue,
         # distorted_ev_diff = (1-theta)*(1-probFractalDraw)*lottery_ev_diff, #Normalized Symm distortion
         # distorted_ev_diff = (1-probFractalDraw)*lottery_ev_diff, #Asymm distortion
         distorted_ev_diff = (1 - (theta*probFractalDraw))*lottery_ev_diff, #Not normalized Symm distortion
         distorted_qv_diff = theta*probFractalDraw*fractal_qv_diff)

rm(fit, g_par_ests, par_ests)
```

# RT effects to explain

```{r}
# Extract set of stimuli that will be used for simulations
sub_data = clean_beh_data %>%
  # filter(subnum  %in% c("01", "03", "05","07", "09", "11", "13", "15", "17", "19")) %>%
  select(leftLotteryEV, rightLotteryEV, leftQValue, rightQValue, probFractalDraw, reactionTime, choiceLeft, subnum, distorted_ev_diff, distorted_qv_diff) %>%
  rename(EVLeft = leftLotteryEV, EVRight = rightLotteryEV, QVLeft = leftQValue, QVRight = rightQValue, distortedEVDiff = distorted_ev_diff, distortedQVDiff = distorted_qv_diff)
```

```{r}
sim_sanity_checks(sub_data, checks=c(1,3,4,5,6,7,8), compare_logits = F, compare_rts = F)
```

## Slow errors check

```{r}
m_data = sub_data %>%
  filter(probFractalDraw != .5) %>%
  mutate(choice = ifelse(choiceLeft == 1, "left", "right"),
         leftLotterySubjBetter = distortedEVDiff > 0,
         leftFractSubjBetter = distortedQVDiff > 0,
         choseBetterSubjLott = ifelse(choice == "left" & leftLotterySubjBetter, "correct", ifelse(choice == "right" & !leftLotterySubjBetter, "correct", "incorrect")),
         choseBetterSubjFrac = ifelse(choice == "left" & leftFractSubjBetter, "correct", ifelse(choice == "right" & !leftFractSubjBetter, "correct", "incorrect")),
         logRt = log(reactionTime),
         fractalMoreRelevant = ifelse(probFractalDraw > .5, "fractal more relevant", "lottery more relevant"),
         fractalMoreRelevant = factor(fractalMoreRelevant, levels=c("lottery more relevant", "fractal more relevant"))) %>%
  select(fractalMoreRelevant, choseBetterSubjLott, choseBetterSubjFrac, logRt, subnum) %>%
        gather(key, value, -fractalMoreRelevant, -logRt, -subnum)
```

Two models for each panel

```{r}
m1 = lme4::lmer(logRt ~ fractalMoreRelevant * value + (1|subnum), data=m_data %>% filter(key == "choseBetterSubjLott"))
m2 = lme4::lmer(logRt ~ fractalMoreRelevant * value + (1|subnum), data=m_data %>% filter(key == "choseBetterSubjFrac"))
```

```{r}
summary(m1)
```

```{r}
summary(m2)
```

Single model looking only on choice based on relevant attribute

```{r}
sm_data = rbind(m_data %>%
        filter(fractalMoreRelevant == "fractal more relevant" & key == "choseBetterSubjFrac"),
      m_data %>%
        filter(fractalMoreRelevant == "lottery more relevant" & key == "choseBetterSubjLott"))

m3 = lme4::lmer(logRt ~ value + (1|subnum), data=sm_data)

summary(m3)
```

# DD Models

## One integrator

```{r}
source(paste0(helpers_path, 'ddModels/r_ddm_models/ddm_oneIntegrator_sepProbDistortion.R'))
```

```{r}
sim_trial_list = list()
sim_trial_list[['model1']] = sim_trial
```

### Demo

```{r}
m1 = sim_task(sub_data, model_name = "model1", d=0.04, sigma = 0.06)
```

```{r}
sim_sanity_checks(m1, checks=c(1,3,4), compare_logits = TRUE)
```

## Two integrators

### Demo

```{r}
source(paste0(helpers_path, 'ddModels/r_ddm_models/ddm_twoIntegrators_sepProbDistortion.R'))
sim_trial_list[['model2']] = sim_trial
```

```{r}
# m2 = sim_task(sub_data, model_name = "model2", dLott=0.03, dFrac=0.06, sigmaLott = 0.03, sigmaFrac = 0.06) #nice rt patterns
m2 = sim_task(sub_data, model_name = "model2", dLott=0.03, dFrac=0.04, sigmaLott = 0.03, sigmaFrac = 0.06)
```

```{r}
sim_sanity_checks(m2, checks=c(1,3,4,5), compare_logits = TRUE)
```

## Three integrators

Potentially unnecessarily complicated and did not survive any identifiability analyses even when fixing noise parameters

### Demo

Simulate data with a 3 integrator model using half the stimuli filtered earlier and this three integrator model.

**Note: this selection of parameters provides a nice qualitative fit to data. It assumes integration is slower for lotteries than for fractals. This is one way of capturing the difference in processing lottery values, which can only be done after the stimulus presentation, as apposed to fractal values which are learned about trialwise. There are alternative ways of modeling this: e.g. integration for the fractal integrator might start before the stimulus presentation screen; ndt for the lottery integrator (or the arbitrator when the lottery is more relevant) might be longer etc. The way to determine which of these hypotheses is most supported by data would be a model comparison.**

```{r}
source(paste0(helpers_path, 'ddModels/r_ddm_models/ddm_threeIntegrators_sepProbDistortion.R'))
sim_trial_list[["model3"]] = sim_trial
```

```{r}
# m3_1 = sim_task(sub_data, model_name = "model3", dLott=0.03, dFrac=0.06, dArb=0.05, sigmaLott = 0.03, sigmaFrac = 0.03)
m3_1 = sim_task(sub_data, model_name = "model3", dLott=0.03, dFrac=0.06, dArb=0.07, sigmaLott = 0.03, sigmaFrac = 0.03)
```

```{r}
sim_sanity_checks(m3_1, checks=c(1,3,4,5,6,7,8), compare_logits = TRUE)
```

# Fits to data

```{r}
cpueaters_path = '/Users/zeynepenkavi/CpuEaters/DescribedVsLearned_beh/analysis/helpers/'
source(paste0(helpers_path, 'optimPostProcess/get_optim_out.R'))
```

```{r}
sim_trial_list = list()
```

```{r}
# Extract set of stimuli that will be used for simulations
sub_stims = clean_beh_data %>%
  select(leftLotteryEV, rightLotteryEV, leftQValue, rightQValue, probFractalDraw, reactionTime, choiceLeft, subnum, distorted_ev_diff, distorted_qv_diff) %>%
  rename(EVLeft = leftLotteryEV, EVRight = rightLotteryEV, QVLeft = leftQValue, QVRight = rightQValue, distortedEVDiff = distorted_ev_diff, distortedQVDiff = distorted_qv_diff)
```


## One integrator

**distorted values in these fits were from the normalized model!**

```{r}
optim_out_path = paste0(cpueaters_path, 'ddModels/cluster_scripts/optim_out/fitOneInt/')
subnums = c("01", "02", "03", "04", "05", "06", "07", "08", "09", "10", "11", "12", "13", "14", "15", "16", "17", "18", "19", "20", "22", "23", "24", "25", "27")
data_prefix ="sub"
data_suffix = "_data"
model = "oneIntegrator_sepProbDistortion"  

ddm_fit_pars = data.frame()

for(i in 1:length(subnums)){
  cur_subnum = subnums[i]
  tmp = get_optim_out(model_=model, data_=paste0(data_prefix, cur_subnum, data_suffix), optim_out_path_=optim_out_path, iters_ = F)
  tmp$subnum = cur_subnum
  ddm_fit_pars = rbind.all.columns(tmp, ddm_fit_pars)
}
```

Summarise subject parameters as a mean of the converged values from different starting points

```{r}
oneIntEsts = ddm_fit_pars %>%
  group_by(subnum) %>%
  summarise(d = mean(Param1),
            sigma = mean(Param2))
```

Simulate predicted data

```{r}
source(paste0(helpers_path, 'ddModels/r_ddm_models/ddm_oneIntegrator_sepProbDistortion.R'))
sim_trial_list[['model1']] = sim_trial
```

```{r}

oneIntPpc = data.frame()

for(i in 1:length(unique(oneIntEsts$subnum))){
  cur_sub = unique(oneIntEsts$subnum)[i]
  cur_stims =  sub_stims %>% filter(subnum == cur_sub)
  cur_pars = oneIntEsts %>% filter(subnum == cur_sub)
  sim_subj = sim_task(cur_stims, model_name = "model1", d=cur_pars$d, sigma=cur_pars$sigma)
  sim_subj$subnum = cur_sub
  oneIntPpc = rbind(oneIntPpc, sim_subj)
}

```

```{r}
sim_sanity_checks(oneIntPpc, checks=c(1,3,4,5,6,7,8), compare_logits = T, compare_rts = T, true_data = sub_stims)
```

```{r}
sim_sanity_checks(oneIntPpc %>% filter(!subnum %in% c("02", "09", "14")), checks=c(1,3,4,5,6,7,8), compare_logits = T, compare_rts = T, true_data = sub_stims %>% filter(!subnum %in% c("02", "09", "14")))
```

## One integrator without extreme cases

```{r}
optim_out_path = paste0(cpueaters_path, 'ddModels/cluster_scripts/optim_out/fitOneIntnoExt/')
subnums = c("01", "02", "03", "04", "05", "06", "07", "08", "09", "10", "11", "12", "13", "14", "15", "16", "17", "18", "19", "20", "22", "23", "24", "25", "27")
data_prefix ="sub"
data_suffix = "_data"
model = "oneIntegrator_sepProbDistortion"  

ddm_fit_pars = data.frame()

for(i in 1:length(subnums)){
  cur_subnum = subnums[i]
  tmp = get_optim_out(model_=model, data_=paste0(data_prefix, cur_subnum, data_suffix), optim_out_path_=optim_out_path, iters_ = F)
  tmp$subnum = cur_subnum
  ddm_fit_pars = rbind.all.columns(tmp, ddm_fit_pars)
}
```

```{r}
oneIntEstsnoExt = ddm_fit_pars %>%
  group_by(subnum) %>%
  summarise(d = mean(Param1),
            sigma = mean(Param2))
```

Compare parameters from fits with and without the extreme cases

```{r}
oneIntEsts %>%
  mutate(fit = "wExt") %>%
  rbind(oneIntEstsnoExt %>% mutate(fit = "noExt")) %>%
  gather(par, value, -subnum, -fit) %>%
  group_by(par) %>%
  spread(fit, value) %>%
  ggplot(aes(noExt, wExt))+
  geom_point()+
  geom_abline(aes(slope=1, intercept=0), color="gray")+
  facet_wrap(~par, scales="free")+
  theme(panel.grid = element_blank())
```

Simulate predicted data

```{r}

oneIntPpcnoExt = data.frame()

for(i in 1:length(unique(oneIntEstsnoExt$subnum))){
  cur_sub = unique(oneIntEstsnoExt$subnum)[i]
  cur_stims =  sub_stims %>% filter(subnum == cur_sub)
  cur_pars = oneIntEstsnoExt %>% filter(subnum == cur_sub)
  sim_subj = sim_task(cur_stims, model_name = "model1", d=cur_pars$d, sigma=cur_pars$sigma)
  sim_subj$subnum = cur_sub
  oneIntPpcnoExt = rbind(oneIntPpcnoExt, sim_subj)
}

```

```{r}
sim_sanity_checks(oneIntPpcnoExt, checks=c(1,3,4,5,6,7,8), compare_logits = T, compare_rts = T, true_data = sub_stims)
```

Despite the similarity in parameter estimates are the oneIntegrator without extreme case fits that used the *distorted values from the asymmetric model* worse fits/have lower likelihoods?

```{r}

```

## Two integrator

```{r}

```

## Three integrators

Estimated drift rates are an order of magnitude larger than those needed to replicate patterns in the true data.



