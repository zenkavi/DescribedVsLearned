---
title: 'Experience vs. description based decision-making project: Revisiting DD models'
output:
  html_document:
    toc: yes
    toc_float: yes
    code_folding: 'hide'
  pdf_document:
    toc: yes
---


```{r include=FALSE}
library(tidyverse)
theme_set(theme_bw())
library(here)
cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
helpers_path = here('analysis/helpers/')

source(paste0(helpers_path,'ddModels/sim_task.R'))
source(paste0(helpers_path,'ddModels/fit_task.R'))
source(paste0(helpers_path,'01_clean_behavioral_data.R'))
source(paste0(helpers_path, 'get_qvals.R'))
source(paste0(helpers_path,'optimPostProcess/sim_sanity_checks.R'))


set.seed(38573)
```

Add distorted value estimates using the hierarchical RL fit.

```{r message=FALSE}

source(paste0(helpers_path, 'rlModels/fit_rl_hierarchical_oneParamDoubleSymmLinearProbDistortion_rpeBoth.R'))

clean_beh_data = par_ests %>%
  group_by(subnum, par) %>%
  summarise(est = mean(value), .groups='keep') %>%
  spread(par, est) %>%
  left_join(clean_beh_data, by='subnum')

## Add Q values of fractals to each trial
clean_beh_data = clean_beh_data %>%
  group_by(subnum) %>%
  do(get_qvals(., model_name="original")) %>%
  ungroup()

clean_beh_data = clean_beh_data %>%
  mutate(rightLotteryEV = referenceProb * referenceValue,
         leftLotteryEV = lotteryValue * lotteryProb,
         lottery_ev_diff = leftLotteryEV - rightLotteryEV,
         fractal_qv_diff = leftQValue - rightQValue,
         distorted_ev_diff = (1-theta)*(1-probFractalDraw)*lottery_ev_diff,
         distorted_qv_diff = theta*probFractalDraw*fractal_qv_diff)
```

**Potential problem** for 3 integrator model: The model would predict slower decisions because the difference in the absolute value of each attribute integrator RDV would be small.

**BUT** by design there aren't trials that provide strong (distorted) evidence for a single side so for the current set of stimuli this should not be a concern.

```{r}
clean_beh_data %>%
  ggplot(aes(distorted_ev_diff, distorted_qv_diff)) +
  geom_point()+
  geom_abline(aes(slope=1, intercept=0))
```

# Testing the 3 integrator model

## Aggregate checks - pass

Simulate data with a 3 integrator model.

Select half the data to use as input stimuli for simulated data.

```{r}
sub_data = clean_beh_data %>%
  filter(subnum  %in% c("01", "03", "05","07", "09", "11", "13", "15", "17", "19")) %>%
  select(leftLotteryEV, rightLotteryEV, leftQValue, rightQValue, probFractalDraw, reactionTime, choiceLeft, subnum, distorted_ev_diff, distorted_qv_diff) %>%
  rename(EVLeft = leftLotteryEV, EVRight = rightLotteryEV, QVLeft = leftQValue, QVRight = rightQValue, distortedEVDiff = distorted_ev_diff, distortedQVDiff = distorted_qv_diff)
```

Source trial simulating and fitting function three integrator model. The model only had the ddm parameters as variables; choice parameters (learning rate and probability distortion) are fitted elsewhere and used to compute the distorted value differences fed into this model.

```{r}
sim_trial_list = list()
source(paste0(helpers_path, 'ddModels/r_ddm_models/ddm_model4b_separatedProbDistortion.R'))
sim_trial_list[['model4b']] = sim_trial
```

Test if trial simulating function works.

```{r}
sim_trial(dLott=0.03, dFrac=0.04, dArb=0.04, sigmaLott = 0.03, sigmaFrac = 0.03, distortedEVDiff = sub_data$distortedEVDiff[100], distortedQVDiff = sub_data$distortedQVDiff[100], probFractalDraw = sub_data$probFractalDraw[100], barrierDecay = 0, EVLeft = sub_data$EVLeft[100], EVRight = sub_data$EVRight[100], QVLeft = sub_data$QVLeft[100], QVRight = sub_data$QVRight[100])
```

Simulate a dataset using half the stimuli filtered earlier and this three integrator model.

```{r}
m4b_1 = sim_task(sub_data, model_name = "model4b", dLott=0.03, dFrac=0.06, dArb=0.05, sigmaLott = 0.03, sigmaFrac = 0.03)
```

```{r}
sim_sanity_checks(m4b_1, checks=c(1,3,4,5,6,7,8), compare_logits = TRUE)
```

## Problem 1: Absolute check

Are the predicted response times close to the true response times? Not really. Despite the apparent correspondence when looking at aggregate data grouped by probFractalDraw level the individual trial estimates are not correlated at all.

Does a well-established model do better in such an absolute check of model fit?

```{r}
m4b_1 %>%
  left_join(sub_data, by=c("EVLeft", "EVRight", "QVLeft", "QVRight", "distortedEVDiff", "distortedQVDiff", "probFractalDraw")) %>%
  ggplot(aes(reactionTime.y, reactionTime.x))+
  geom_point(color="light gray")+
  geom_smooth(formula="y~x", method="lm")+
  geom_abline(aes(slope=1, intercept=0), linetype="dashed")+
  xlab("True RT")+
  ylab("Predicted RT")
```

## Problem 2: Parameter estimation

# Sum of random variables approach to drift rate and noise estimation

Given the choice and RT for a trial the likelihood of ...

## Demonstration

Assuming drift rate and noise parameters for a single integrator we simulate n particles for the same trial (i.e. same value difference) and make a histogram of a particles in each state at each time step.

```{r}
run_lott_integrator = function(dLott, sigmaLott, distortedEVDiff, nonDecisionTime=0, bias=0, barrierDecay = 0, timeStep=10, barrier = 1, maxIter = 400, debug=TRUE){
  
  if(debug){
    debug_df = data.frame(time=0, lotteryRDV=0)
  }
  
  lotteryRDV = bias
  time = 1
  elapsedNDT = 0
  choice = 0
  RT = NA
  nonDecIters = nonDecisionTime / timeStep
  initialBarrier = barrier
  barrier = rep(initialBarrier, maxIter)
  
  # The values of the barriers can change over time
  for(t in seq(1, maxIter, 1)){
    barrier[t] = initialBarrier / (1 + barrierDecay * t)
  }
  
  lottery_mu = dLott * distortedEVDiff
  
  while (time<maxIter){
    
    # If the arbitrator RDV hits one of the barriers make decision
    if (lotteryRDV >= barrier[time] | lotteryRDV <= -barrier[time]){
      
      # Convert ms back to secs
      RT = (time * timeStep)/1000 
      
      if (lotteryRDV >= barrier[time]){
        choice = "left"
      } else if (lotteryRDV <= -barrier[time]){
        choice = "right"
      }
      break
    } 
    
    # Otherwise continue sampling evidence
    if (elapsedNDT < nonDecIters){
      elapsedNDT = elapsedNDT + 1
    } else{
      lotteryRDV = lotteryRDV + rnorm(1, lottery_mu, sigmaLott)
    }
    
    if (debug){
      debug_row = data.frame(time = time, lotteryRDV = round(lotteryRDV, 3))
      debug_df = rbind(debug_df, debug_row)
    }
    
    # Increment sampling iteration
    time = time + 1
  }
  
  out = data.frame(distortedEVDiff = distortedEVDiff, choice=choice, reactionTime = RT, dLott=dLott, sigmaLott=sigmaLott, barrier=barrier[time], nonDecisionTime=nonDecisionTime, bias=bias, timeStep=timeStep, maxIter=maxIter)
  
  if(debug){
    out = list(out=out, debug_df=debug_df)
  }
  
  return(out)
}

```

```{r}
initialBarrier = 1
approxStateStep = .1
halfNumStateBins = round(initialBarrier / approxStateStep)
stateStep = initialBarrier / (halfNumStateBins + 0.5)

# The vertical axis is divided into states.
states = seq(-1*(initialBarrier) + (stateStep / 2), initialBarrier - (stateStep / 2), stateStep)
```

```{r}
n_particles = 250
sim_data = data.frame()

for(i in 1:n_particles){
  tmp = run_lott_integrator(dLott = 0.03, sigmaLott = 0.05, distortedEVDiff = 0.4)$debug_df
  tmp$iter = i
  sim_data = rbind(sim_data, tmp)
}
```

Plot the time course of each particle.

```{r}
sim_data %>%
  ggplot(aes(time, lotteryRDV, group=iter))+
  geom_line(color="gray")+
  scale_x_continuous(breaks = seq(0, 400, 1))+
  scale_y_continuous(breaks = round(states,3))+
  geom_hline(aes(yintercept=0), linetype="dashed")+
  geom_hline(aes(yintercept=1))+
  geom_hline(aes(yintercept=-1))+
  theme(panel.grid.minor = element_blank(), 
        axis.text = element_blank())
```

Compute which discrete state each particle would be in based on the (continuous) RDV

```{r}
for(i in 1:nrow(sim_data)){
  sim_data$state_bin[i] = which.min(abs(states - sim_data$lotteryRDV[i]))
}
```

Plot number of particles in each state (on the y axis) for 10 time steps in facets.

```{r}
sim_data %>%
  # filter(time< 11) %>%
  filter(time>20 &time<31) %>%
  ggplot(aes(state_bin))+
  geom_histogram(bins=21)+
  coord_flip()+
  facet_grid(.~time)+
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank())+
  ylab("")+
  xlab("# particles / state")+
  scale_x_continuous(breaks=c(1:21)) +
  geom_vline(aes(xintercept=11), linetype="dashed")+
  geom_vline(aes(xintercept=21))+
  geom_vline(aes(xintercept=1))
```

Plot estimated pdf of each state for the summed RDV for the same time steps. **The simulated particles are distributed (above) in the predicted (below) manner**

```{r}
dLott = 0.03
distortedEVDiff = .4
sigmaLott = .05

est_data = data.frame()

for(i in 0:max(sim_data$time)){
 tmp=data.frame(time = i, 
            states = states, 
            pdf_states = dnorm(states, mean=dLott*distortedEVDiff*i, sd=sqrt(sigmaLott^2*i)))
 est_data = rbind(est_data, tmp)
}

est_data %>%
  # filter(time<11) %>%
    filter(time>20 &time<31) %>%
  ggplot(aes(states, pdf_states))+
  geom_bar(stat="identity")+
  coord_flip()+
  facet_grid(.~time)+
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank())+
  ylab("")+
  xlab("pdf / state")+
  scale_x_continuous(breaks=states) +
  geom_vline(aes(xintercept=0), linetype="dashed")+
  geom_vline(aes(xintercept=1))+
  geom_vline(aes(xintercept=-1))
```
To use this logic for parameter estimation we would maximize the likelihood of crossing the boundary corresponding to the choice at the observed response time. So we need to confirm that the likelihood of each simulated particle hitting the boundary at the simulated timestep is higher for the true parameter combination is than the likelihood computed using an incorrect parameter computation.

## Likelihood checks

For this single integrator is the density (/likelihood) at time step == true rt always higher for the true parameter combination compared to a wrong parameter combo? Or is the density always larger for something stupid like the size of the mean?

The density of a particle that has crossed a barrier at time t is `dnorm(crossed barrier, mean=drift rate * t, sd = sqrt(sigma ^ 2 * t))`.

Here I merged the data on when each particle crossed the barrier with the estimated densities of being at the state closest to the boundary at that time point given the true drift rate and sigma. So these aren't quiet right. More accurately I could compute the density for the `lotteryRDV` for each particle at the time they've crossed the boundary. Still, in empirical data I will only have choice which I translate to a boundary so this is probably a close enough approximation.

```{r}
true_liks = sim_data %>%
  group_by(iter) %>%
  filter(time == max(time)) %>%
  left_join(est_data %>%
  filter(states>.95), by="time") # this isn't full proof. ideally you'd merge on time and statebin since it's possible that the other boundary was hit
```

Having larger scaling factor doesn't translate higher density. The plot shows the computed densities of crossing the boundary at the time each particle did using the true and incorrect parameters. For most particles the true density is higher.

```{r}
false_dLott = 0.08
distortedEVDiff = .4
false_sigmaLott = .08

false_est_data = data.frame()

for(i in 0:max(sim_data$time)){
 tmp=data.frame(time = i, 
            states = states, 
            pdf_states = dnorm(states, mean=false_dLott*distortedEVDiff*i, sd=sqrt(false_sigmaLott^2*i)))
 false_est_data = rbind(false_est_data, tmp)
}

false_liks = sim_data %>%
  group_by(iter) %>%
  filter(time == max(time)) %>%
  left_join(false_est_data %>%
  filter(states>.95), by="time")

true_liks %>%
  mutate(lik_type = "true") %>%
  rbind(false_liks %>% mutate(lik_type="false")) %>%
  select(iter, pdf_states, lik_type) %>%
  spread(lik_type, pdf_states) %>%
  ggplot(aes(true, false))+
  geom_point()+
  geom_abline(aes(intercept=0, slope=1))
```

Even with false parameters that are relatively close to the true ones the true pdf's are higher for most particles.

```{r}
false_dLott = 0.04
distortedEVDiff = .4
# false_sigmaLott = .06
false_sigmaLott = .01

false_est_data = data.frame()

for(i in 0:max(sim_data$time)){
 tmp=data.frame(time = i, 
            states = states, 
            pdf_states = dnorm(states, mean=false_dLott*distortedEVDiff*i, sd=sqrt(false_sigmaLott^2*i)))
 false_est_data = rbind(false_est_data, tmp)
}

false_liks = sim_data %>%
  group_by(iter) %>%
  filter(time == max(time)) %>%
  left_join(false_est_data %>%
  filter(states>.95), by="time")

true_liks %>%
  mutate(lik_type = "true") %>%
  rbind(false_liks %>% mutate(lik_type="false")) %>%
  select(iter, pdf_states, lik_type) %>%
  spread(lik_type, pdf_states) %>%
  ggplot(aes(true, false))+
  geom_point()+
  geom_abline(aes(intercept=0, slope=1))
```
## Different trials (~single subject)

Try it for a set of distorted EV differences (akin to a subject). Note this is integrating only on lotteries so not simulating/modeling the actual task yet.

```{r}
n_trials = 300
sim_subj = data.frame()
true_dLott = 0.03
true_sigmaLott = 0.05

for(i in 1:n_trials){
  tmp = run_lott_integrator(dLott = true_dLott, sigmaLott = true_sigmaLott, distortedEVDiff = clean_beh_data$distorted_ev_diff[i])$debug_df
  tmp$iter = i
  sim_subj = rbind(sim_subj, tmp)
}
```

Plot RDV time course for some trials.

```{r}
sim_subj %>%
  filter(iter<)%>%
  ggplot(aes(time, lotteryRDV, group=iter))+
  geom_line(color="gray")+
  scale_x_continuous(breaks = seq(0, 400, 1))+
  scale_y_continuous(breaks = round(states,3))+
  geom_hline(aes(yintercept=0), linetype="dashed")+
  geom_hline(aes(yintercept=1))+
  geom_hline(aes(yintercept=-1))+
  theme(panel.grid.minor = element_blank(), 
        axis.text = element_blank())
```

Compute the probability density of being in the given state at the time the integrator terminated for each trial using the true drift rate and sigma. This time using the actual RDV to compute the density but note again that this latent variable won't exist in empirical data so we'll assume an RDV of 1 or -1 depending on the boundary.

```{r}
sim_subj_rts = sim_subj %>%
  group_by(iter) %>%
  filter(time == max(time))

subj_liks = data.frame()

for(i in 1:nrow(sim_subj_rts)){
  trial_time = sim_subj_rts$time[i]
  trial_rdv = sim_subj_rts$lotteryRDV[i]
  trial_evDiff = clean_beh_data$distorted_ev_diff[i]
 tmp=data.frame(iter = sim_subj_rts$iter[i],
                time = trial_time,
                rdv = trial_rdv,
                pdf_rdv = dnorm(trial_rdv, mean=true_dLott*trial_evDiff*trial_time, sd=sqrt(true_sigmaLott^2*trial_time)))
 subj_liks = rbind(subj_liks, tmp)
}
```

The sum of these would be the likelihood of the data (so are these like elpd's - expected log prob densities?)

```{r}
sum(subj_liks$pdf_rdv)
```

Confirm that this sum of likelihoods for each trial would be lower if you used the incorrect drift rate and sigma. **Doesn't do a good job estimating the noise**

```{r}
false_dLott = 0.05
false_sigmaLott = 0.05
false_subj_liks = data.frame()

for(i in 1:nrow(sim_subj_rts)){
  trial_time = sim_subj_rts$time[i]
  trial_rdv = sim_subj_rts$lotteryRDV[i]
  trial_evDiff = clean_beh_data$distorted_ev_diff[i]
 tmp=data.frame(iter = sim_subj_rts$iter[i],
                time = trial_time,
                rdv = trial_rdv,
                pdf_rdv = dnorm(trial_rdv, mean=false_dLott*trial_evDiff*trial_time, sd=sqrt(false_sigmaLott^2*trial_time)))
 false_subj_liks = rbind(false_subj_liks, tmp)
}

sum(false_subj_liks$pdf_rdv)
```

## Both attributes single integrator

Try it with a model integrating on both attributes instead of just on lotteries.

```{r}
run_single_integrator = function(d, sigma, distortedEVDiff, distortedQVDiff, nonDecisionTime=0, bias=0, barrierDecay = 0, timeStep=10, barrier = 1, maxIter = 400, debug=TRUE){
  
  if(debug){
    debug_df = data.frame(time=0, RDV=0)
  }
  
  RDV = bias
  time = 1
  elapsedNDT = 0
  choice = 0
  RT = NA
  nonDecIters = nonDecisionTime / timeStep
  initialBarrier = barrier
  barrier = rep(initialBarrier, maxIter)
  
  # The values of the barriers can change over time
  for(t in seq(1, maxIter, 1)){
    barrier[t] = initialBarrier / (1 + barrierDecay * t)
  }
  
  mu = d * (distortedEVDiff + distortedQVDiff)
  
  while (time<maxIter){
    
    # If the arbitrator RDV hits one of the barriers make decision
    if (RDV >= barrier[time] | RDV <= -barrier[time]){
      
      # Convert ms back to secs
      RT = (time * timeStep)/1000 
      
      if (RDV >= barrier[time]){
        choice = "left"
      } else if (RDV <= -barrier[time]){
        choice = "right"
      }
      break
    } 
    
    # Otherwise continue sampling evidence
    if (elapsedNDT < nonDecIters){
      elapsedNDT = elapsedNDT + 1
    } else{
      RDV = RDV + rnorm(1, mu, sigma)
    }
    
    if (debug){
      debug_row = data.frame(time = time, RDV = round(RDV, 3))
      debug_df = rbind(debug_df, debug_row)
    }
    
    # Increment sampling iteration
    time = time + 1
  }
  
  #If a choice hasn't been made by the time limit
  if(is.na(RT)){
    # Choose whatever you have most evidence for
    if (RDV >= 0){
      choice = "left"
    } else if (RDV <= 0){
      choice = "right"
    }
    if(debug){
      print("Max iterations reached.")
    }
    timeOut = 1
    RT=rlnorm(1, mean = 1.25, sd = 0.1)
  }
  
  out = data.frame(distortedEVDiff = distortedEVDiff, distortedQVDiff = distortedQVDiff, choice=choice, reactionTime = RT, d=d, sigma=sigma, barrier=barrier[time], nonDecisionTime=nonDecisionTime, bias=bias, timeStep=timeStep, maxIter=maxIter)
  
  if(debug){
    out = list(out=out, debug_df=debug_df)
  }
  
  return(out)
}
```

```{r}
get_rdv_pdf = function(data, test_d, test_sigma){
  data = data %>%
    mutate(rdv = ifelse(choice == "left", 1, -1),
           num_timeSteps = round(reactionTime*1000/timeStep),
           trial_drift = distortedEVDiff + distortedQVDiff,
           pdf_rdv = dnorm(rdv, mean=test_d*trial_drift*num_timeSteps, sd=sqrt(test_sigma^2*num_timeSteps)))
  
  return(sum(data$pdf_rdv))
         
}

```

```{r}
n_trials = 1000
sim_subj = data.frame()
true_d = 0.05
true_sigma = 0.03

for(i in 1:n_trials){
  tmp = run_single_integrator(d = true_d, sigma = true_sigma, 
                              distortedEVDiff = clean_beh_data$distorted_ev_diff[i],
                              distortedQVDiff = clean_beh_data$distorted_qv_diff[i], debug=FALSE)
  tmp$iter = i
  sim_subj = rbind(sim_subj, tmp)
}
```

Compute rdv pdf's for a grid of parameters. Do the true parameters have the highest density? It can identify the correct drift rate if you fix the noise.

```{r}
tmp = expand.grid(seq(.01, .09, .01), c(.03)) %>%
  rename(d = Var1, sigma = Var2)

for (i in 1:nrow(tmp)){
  tmp$rdv_pdf[i] = get_rdv_pdf(sim_subj, tmp$d[i], tmp$sigma[i])
}

tmp %>%
  arrange(-rdv_pdf)
```

## Three integrators

```{r}

```

## Full posteriors + hierarchical fit