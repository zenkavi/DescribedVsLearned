---
title: 'Experience vs. description based decision-making project: Revisiting DD models'
output:
  html_document:
    toc: yes
    toc_float: yes
    code_folding: 'hide'
  pdf_document:
    toc: yes
---


```{r include=FALSE}
library(tidyverse)
theme_set(theme_bw())
library(here)
cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
helpers_path = here('analysis/helpers/')
source(paste0(helpers_path,'01_clean_behavioral_data.R'))
```

```{r message=FALSE}
source(paste0(helpers_path, 'get_qvals.R'))

source(paste0(helpers_path, 'rlModels/fit_rl_hierarchical_oneParamDoubleSymmLinearProbDistortion_rpeBoth.R'))

clean_beh_data = par_ests %>%
  group_by(subnum, par) %>%
  summarise(est = mean(value), .groups='keep') %>%
  spread(par, est) %>%
  left_join(clean_beh_data, by='subnum')

## Add Q values of fractals to each trial
clean_beh_data = clean_beh_data %>%
  group_by(subnum) %>%
  do(get_qvals(., model_name="original")) %>%
  ungroup()

clean_beh_data = clean_beh_data %>%
  mutate(rightLotteryEV = referenceProb * referenceValue,
         leftLotteryEV = lotteryValue * lotteryProb,
         lottery_ev_diff = leftLotteryEV - rightLotteryEV,
         fractal_qv_diff = leftQValue - rightQValue,
         distorted_ev_diff = (1-theta)*(1-probFractalDraw)*lottery_ev_diff,
         distorted_qv_diff = theta*probFractalDraw*fractal_qv_diff)
```

**Potential problem** for 3 integrator model: The model would predict slower decisions because the difference in the absolute value of each attribute integrator RDV would be small.

**BUT** by design there aren't trials that provide strong (distorted) evidence for a single side so for the current set of stimuli this should not be a concern.

```{r}
clean_beh_data %>%
  ggplot(aes(distorted_ev_diff, distorted_qv_diff)) +
  geom_point()+
  geom_abline(aes(slope=1, intercept=0))
```

Simulate data with a 3 integrator model.

```{r message=FALSE}
source(paste0(helpers_path,'ddModels/sim_task.R')) # do this first not to mess with cluster setup?
source(paste0(helpers_path,'optimPostProcess/sim_sanity_checks.R'))

set.seed(38573)
```

Select half the data to use as input stimuli for simulated data.

```{r}
sub_data = clean_beh_data %>%
  filter(subnum  %in% c("01", "03", "05","07", "09", "11", "13", "15", "17", "19")) %>%
  select(leftLotteryEV, rightLotteryEV, leftQValue, rightQValue, probFractalDraw, reactionTime, choiceLeft, subnum, distorted_ev_diff, distorted_qv_diff) %>%
  rename(EVLeft = leftLotteryEV, EVRight = rightLotteryEV, QVLeft = leftQValue, QVRight = rightQValue, distortedEVDiff = distorted_ev_diff, distortedQVDiff = distorted_qv_diff)
```

Source trial simulating and fitting function three integrator model. The model only had the ddm parameters as variables; choice parameters (learning rate and probability distortion) are fitted elsewhere and used to compute the distorted value differences fed into this model.

```{r}
sim_trial_list = list()
source(paste0(helpers_path, 'ddModels/r_ddm_models/ddm_model4b_separatedProbDistortion.R'))
sim_trial_list[['model4b']] = sim_trial
```

Test if trial simulating function works.

```{r}
sim_trial(dLott=0.03, dFrac=0.04, dArb=0.04, sigmaLott = 0.03, sigmaFrac = 0.03, sigmaArb = 0.01, distortedEVDiff = sub_data$distortedEVDiff[100], distortedQVDiff = sub_data$distortedQVDiff[100], probFractalDraw = sub_data$probFractalDraw[100], barrierDecay = 0, EVLeft = sub_data$EVLeft[100], EVRight = sub_data$EVRight[100], QVLeft = sub_data$QVLeft[100], QVRight = sub_data$QVRight[100])
```

Simulate a dataset using half the stimuli filtered earlier and this three integrator model.

```{r}
m4b_1 = sim_task(sub_data, model_name = "model4b", dLott=0.04, dFrac=0.03, dArb=0.06, sigmaLott = 0.03, sigmaFrac = 0.03, sigmaArb = 0.01)
```

Check how well the RT patterns in the simulated data match the true data.

With this parameter setting predicted choice is systematically slower for trials where the fractals matter more. The true data histograms versus predicted density curves also do not look in general.

```{r}
sim_sanity_checks(m4b_1, checks=c(1,3,4))
```

Increasing the fractal drift rate can take care of the slower choice for when fractals are more relevant to a large degree except for the edge case trials where only the fractals matter.

Value difference effects on RT are exaggerated.

```{r}
m4b_2 = sim_task(sub_data, model_name = "model4b", dLott=0.03, dFrac=0.06, dArb=0.06, sigmaLott = 0.03, sigmaFrac = 0.03, sigmaArb = 0.01)
```

```{r}
sim_sanity_checks(m4b_2, checks=c(1,3,4,6, 7))
```

Are the effects by what the choice depends on there?

"Incorrect" choice based on the relevant attribute is slower for both attributes.

```{r}
tmp_true = sub_data %>%
  mutate(leftLotterySubjBetter = distortedEVDiff > 0,
         leftFractSubjBetter = distortedQVDiff > 0,
         choseBetterSubjLott = ifelse(choiceLeft == 1 & leftLotterySubjBetter, TRUE, ifelse(choiceLeft == 0 & !leftLotterySubjBetter, TRUE, FALSE)),
         choseBetterSubjFrac = ifelse(choiceLeft == 1 & leftFractSubjBetter, TRUE, ifelse(choiceLeft == 0 & !leftFractSubjBetter, TRUE, FALSE)),
         logRt = log(reactionTime),
         fractalMoreRelevant = ifelse(probFractalDraw > .5, "fractal more relevant", "lottery more relevant"),
         fractalMoreRelevant = factor(fractalMoreRelevant, levels=c("lottery more relevant", "fractal more relevant"))) %>%
  select(fractalMoreRelevant, choseBetterSubjLott, choseBetterSubjFrac, logRt) %>%
  gather(key, value, -fractalMoreRelevant, -logRt) %>%
  group_by(fractalMoreRelevant, key, value) %>%
  summarise(.groups="keep",
            meanLogRt = mean(logRt),
            semLogRt = sd(logRt)/sqrt(n())) %>%
  mutate(data_type =  "true")


tmp = m4b_2 %>%
  mutate(leftLotterySubjBetter = distortedEVDiff > 0,
         leftFractSubjBetter = distortedQVDiff > 0,
         choseBetterSubjLott = ifelse(choice == "left" & leftLotterySubjBetter, TRUE, ifelse(choice == "right" & !leftLotterySubjBetter, TRUE, FALSE)),
         choseBetterSubjFrac = ifelse(choice == "left" & leftFractSubjBetter, TRUE, ifelse(choice == "right" & !leftFractSubjBetter, TRUE, FALSE)),
         logRt = log(reactionTime),
         fractalMoreRelevant = ifelse(probFractalDraw > .5, "fractal more relevant", "lottery more relevant"),
         fractalMoreRelevant = factor(fractalMoreRelevant, levels=c("lottery more relevant", "fractal more relevant"))) %>%
  select(fractalMoreRelevant, choseBetterSubjLott, choseBetterSubjFrac, logRt) %>%
  gather(key, value, -fractalMoreRelevant, -logRt) %>%
  group_by(fractalMoreRelevant, key, value) %>%
  summarise(.groups="keep",
            meanLogRt = mean(logRt),
            semLogRt = sd(logRt)/sqrt(n())) %>%
  mutate(data_type="stim") 

tmp %>%
  rbind(tmp_true) %>%
  ggplot(aes(fractalMoreRelevant, meanLogRt, color=value, shape=key, alpha=data_type))+
  geom_point(position=position_dodge(width=.5))+
  geom_errorbar(aes(ymin=meanLogRt-semLogRt, ymax=meanLogRt+semLogRt),width=.2,position=position_dodge(width=.5))+
  xlab("")+
  theme(legend.position = "bottom")+
  scale_alpha_manual(values = c(1, .5))



  
```

Are the predicted response times close to the true response times? No! Despite the apparent correspondence when looking at aggregate data grouped by probFractalDraw level the individual trial estimates are not correlated at all.

```{r}
m4b_2 %>%
  left_join(sub_data, by=c("EVLeft", "EVRight", "QVLeft", "QVRight", "distortedEVDiff", "distortedQVDiff", "probFractalDraw")) %>%
  ggplot(aes(reactionTime.y, reactionTime.x))+
  geom_point(color="light gray")+
  geom_smooth(formula="y~x", method="lm")+
  geom_abline(aes(slope=1, intercept=0), linetype="dashed")+
  xlab("True RT")+
  ylab("Predicted RT")
```

Does a well-established model do better in such an absolute check of model fit????


Check the fitting function

```{r}

```
