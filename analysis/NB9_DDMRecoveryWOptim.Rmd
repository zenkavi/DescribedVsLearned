---
title: "Experience vs. description based decision-making project: DDM parameter recovery by perturbing the `optim` function"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: 'hide'
---

# Setup

Set up environment and load in data

```{r include=FALSE, message=FALSE}
library(tidyverse)
library(here)
cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
theme_set(theme_classic())
sem <- function(x) {sd(x, na.rm=T) / sqrt(length(x))}
helpers_path = here('analysis/helpers/')

set.seed(38573)
```

```{r message=FALSE}
source(paste0(helpers_path,'ddmSims/fit_task.R'))
source(paste0(helpers_path,'ddmSims/sim_task.R'))
test_trial_conditions = read.csv(paste0(helpers_path, 'ddmSims/test_data/test_trial_conditions.csv'))
```

```{r include=FALSE, message=FALSE}
library(visualMLE)
```

Empty lists to store the trial simulators for the forthcoming models.

```{r}
sim_trial_list = list()
fit_trial_list = list()
```

```{r}
true_d = .06
true_sigma = .08
true_delta = 3
true_gamma = 3
```

# Prob distortion detection fails

With model1b, closest model to model2b without early integration

```{r}
source(paste0(helpers_path, 'ddmSims/r_ddm_models/ddm_model1b.R'))

sim_trial_list[['model1b']] = sim_trial
fit_trial_list[['model1b']] = fit_trial
```

```{r}
trialsPerCondition=15 

# Replicate same conditions n times
test_data = dplyr::bind_rows(replicate(trialsPerCondition, test_trial_conditions, simplify = FALSE))

# Simulate choice and RT for the replicated trial conditions
test_data = sim_task(test_data, model_name = "model1b", d = true_d, sigma = true_sigma, delta = true_delta, gamma = true_gamma) %>%drop_na()
```

Example 1: c(.01, .01, 1, 1)

```{r}
optim_out = optim_save(c(.01, .01, 1, 1), get_task_nll, data=test_data, par_names = c("d", "sigma", "delta", "gamma"), model_name="model1b", control = list(maxit=75))
```

```{r}
optim_out$par
```

```{r}
tmp = data.frame(key = c("d", "sigma", "delta", "gamma"), true_val = c(true_d, true_sigma, true_delta, true_gamma))

optim_out$iterations_df %>%
  gather(key, value, -Result, -Iteration) %>%
  mutate(key =ifelse(key == "Param1", "d", ifelse(key == "Param2", "sigma", ifelse(key == "Param3", "delta", "gamma")))) %>%
  ggplot(aes(Iteration, value))+
  geom_point(aes(color=Result))+
  geom_line(alpha=.5, color="gray")+
  facet_wrap(~key, scales="free")+
  geom_hline(data=tmp, aes(yintercept = true_val), linetype="dashed")+
  theme(legend.position="bottom")
```

## Path plots

Path plots to see how to think through the perturbations

Look into the Nelder-Mead algorithm to understand how the particle moves.
[Here's a quick tutorial](http://www.brnt.eu/phd/node10.html#SECTION00622200000000000000) on how it works
[Here's some R code that walks through the moves of the simplex](https://m-clark.github.io/models-by-example/nelder-mead.html)

```{r}
optim_out$iterations_df %>%
  mutate(iter_block = round(Iteration/10),
         start_point = c(1,diff(iter_block)),
         end_point = -1*lead(c(0, diff(iter_block))),
         end_point = ifelse(is.na(end_point), 0, end_point),
         point_shape = as.factor(start_point+end_point),
         iter_block = paste0("Iteration block = ", iter_block)) %>%
  rename(d= Param1, sigma = Param2)%>%
  ggplot(aes(d, sigma, color=Iteration))+
  geom_point(aes(shape=point_shape, size=point_shape))+
  geom_path(arrow = arrow(type = "closed", length = unit(0.05, "npc")))+
  geom_point(aes(x = true_d, y = true_sigma), size=3, color="red", pch=8)+
  facet_wrap(~iter_block, scales="free")+
  theme(legend.position = "none")+
  scale_shape_manual(values = c(22,20,21))+
  scale_size_manual(values=c(2.5,2,2.5))
```

```{r}
optim_out$iterations_df %>%
  mutate(iter_block = round(Iteration/10),
         start_point = c(1,diff(iter_block)),
         end_point = -1*lead(c(0, diff(iter_block))),
         end_point = ifelse(is.na(end_point), 0, end_point),
         point_shape = as.factor(start_point+end_point),
         iter_block = paste0("Iteration block = ", iter_block)) %>%
  rename(delta= Param3, gamma = Param4)%>%
  ggplot(aes(delta, gamma, color=Iteration))+
  geom_point(aes(shape=point_shape, size=point_shape))+
  geom_path(arrow = arrow(type = "closed", length = unit(0.05, "npc")))+
  geom_point(aes(x = true_delta, y = true_gamma), size=3, color="red", pch=8)+
  facet_wrap(~iter_block, scales="free")+
  theme(legend.position = "none")+
  scale_shape_manual(values = c(22,20,21))+
  scale_size_manual(values=c(2.5,2,2.5))
```

## Change in NLL

Why don't delta and gamma move? Why were we able to estimate the correct d and sigma even when delta and gamma were incorrectly fixed at 1?
Because changing them even drastically leads to very small improvement in the objective function (nll).

```{r}
true_nll = get_task_nll(test_data, par = c(true_d, true_sigma, true_delta, true_gamma), par_names = c("d", "sigma", "delta", "gamma"), model_name = "model1b")
true_nll
```

Proportional difference in NLL when using different delta and gamma values compared to the true NLL.

In this specific example, the absence of a big difference between setting delta and gamma to 1 means that the recovery can't tell if there is prob distortion or not.

```{r}
(get_task_nll(test_data, par = c(true_d, true_sigma, 1, 1), par_names = c("d", "sigma", "delta", "gamma"), model_name = "model1b")-true_nll)/true_nll
```

```{r}
(get_task_nll(test_data, par = c(true_d, true_sigma, 2, 2), par_names = c("d", "sigma", "delta", "gamma"), model_name = "model1b")-true_nll)/true_nll
```

On the other hand, a similar change in d and sigma leads to a massive difference

```{r}
(get_task_nll(test_data, par = c(true_d/3, true_sigma/3, 1, 1), par_names = c("d", "sigma", "delta", "gamma"), model_name = "model1b")-true_nll)/true_nll
```

## Alternative parameterization

How do things look like with a differently parameterized prob distortion function?
How does the prob distortion and the ranges of the parameters change?

```{r}
delt = 3
gamm = 3

data.frame(prob = seq(0,1,.1)) %>%
  mutate(distorted_prob_1a = (delt * (prob)^gamm) / ( (delt * (prob)^gamm) + (1-prob)^gamm ),
         distorted_prob_1b = exp(-delt*(-log(prob))^gamm)  ) %>%
  gather(key, value, -prob) %>%
  ggplot(aes(prob, value, color=key))+
  geom_point()+
  geom_line()+
  geom_abline(aes(intercept=0, slope = 1), linetype="dashed")+
  theme(legend.position = "bottom")+
  labs(color="")
```

```{r}
source(paste0(helpers_path, 'ddmSims/r_ddm_models/ddm_model1a.R'))

sim_trial_list[['model1a']] = sim_trial
fit_trial_list[['model1a']] = fit_trial
```

Is recovery any better for these prob distortion parameters?

```{r}
trialsPerCondition=15 

# Replicate same conditions n times
test_data = dplyr::bind_rows(replicate(trialsPerCondition, test_trial_conditions, simplify = FALSE))

# Simulate choice and RT for the replicated trial conditions
test_data = sim_task(test_data, model_name = "model1a", d = true_d, sigma = true_sigma, delta = true_delta, gamma = true_gamma) %>%drop_na()
```

```{r}
optim_out = optim_save(c(.01, .01, 1, 1), get_task_nll, data=test_data, par_names = c("d", "sigma", "delta", "gamma"), model_name="model1a", control = list(maxit=75))
```

```{r}
optim_out$par
```

```{r}
tmp = data.frame(key = c("d", "sigma", "delta", "gamma"), true_val = c(true_d, true_sigma, true_delta, true_gamma))

optim_out$iterations_df %>%
  gather(key, value, -Result, -Iteration) %>%
  mutate(key =ifelse(key == "Param1", "d", ifelse(key == "Param2", "sigma", ifelse(key == "Param3", "delta", "gamma")))) %>%
  ggplot(aes(Iteration, value))+
  geom_point(aes(color=Result))+
  geom_line(alpha=.5, color="gray")+
  facet_wrap(~key, scales="free")+
  geom_hline(data=tmp, aes(yintercept = true_val), linetype="dashed")+
  theme(legend.position="bottom")
```

Path plots

```{r}
optim_out$iterations_df %>%
  mutate(iter_block = round(Iteration/10),
         start_point = c(1,diff(iter_block)),
         end_point = -1*lead(c(0, diff(iter_block))),
         end_point = ifelse(is.na(end_point), 0, end_point),
         point_shape = as.factor(start_point+end_point),
         iter_block = paste0("Iteration block = ", iter_block)) %>%
  rename(d= Param1, sigma = Param2)%>%
  ggplot(aes(d, sigma, color=Iteration))+
  geom_point(aes(shape=point_shape, size=point_shape))+
  geom_path(arrow = arrow(type = "closed", length = unit(0.05, "npc")))+
  geom_point(aes(x = true_d, y = true_sigma), size=3, color="red", pch=8)+
  facet_wrap(~iter_block, scales="free")+
  theme(legend.position = "none")+
  scale_shape_manual(values = c(22,20,21))+
  scale_size_manual(values=c(2.5,2,2.5))
```

```{r}
optim_out$iterations_df %>%
  mutate(iter_block = round(Iteration/10),
         start_point = c(1,diff(iter_block)),
         end_point = -1*lead(c(0, diff(iter_block))),
         end_point = ifelse(is.na(end_point), 0, end_point),
         point_shape = as.factor(start_point+end_point),
         iter_block = paste0("Iteration block = ", iter_block)) %>%
  rename(delta= Param3, gamma = Param4)%>%
  ggplot(aes(delta, gamma, color=Iteration))+
  geom_point(aes(shape=point_shape, size=point_shape))+
  geom_path(arrow = arrow(type = "closed", length = unit(0.05, "npc")))+
  geom_point(aes(x = true_delta, y = true_gamma), size=3, color="red", pch=8)+
  facet_wrap(~iter_block, scales="free")+
  theme(legend.position = "none")+
  scale_shape_manual(values = c(22,20,21))+
  scale_size_manual(values=c(2.5,2,2.5))
```

```{r}
true_nll = get_task_nll(test_data, par = c(true_d, true_sigma, true_delta, true_gamma), par_names = c("d", "sigma", "delta", "gamma"), model_name = "model1a")
true_nll
```

Proportional difference in NLL when using different delta and gamma values compared to the true NLL.

In this specific example, the absence of a big difference between setting delta and gamma to 1 means that the recovery can't tell if there is prob distortion or not.

```{r}
(get_task_nll(test_data, par = c(true_d, true_sigma, 1, 1), par_names = c("d", "sigma", "delta", "gamma"), model_name = "model1a")-true_nll)/true_nll
```

```{r}
(get_task_nll(test_data, par = c(true_d, true_sigma, 2, 2), par_names = c("d", "sigma", "delta", "gamma"), model_name = "model1a")-true_nll)/true_nll
```

On the other hand, a similar change in d and sigma leads to a massive difference

```{r}
(get_task_nll(test_data, par = c(true_d/3, true_sigma/3, 1, 1), par_names = c("d", "sigma", "delta", "gamma"), model_name = "model1a")-true_nll)/true_nll
```

Changing the objective function might, but what to change it to?
Before that try to understand the effect of the distortion parameters (delta and gamma) on the likelihood computation.
These two come into play for the likelihood computation in the weighted mu

E.g. what does a 10% change in each parameter do to the weightedMuMean on average across trials

```{r}
delt = 3
gamm = 3
dR = .06
changeFactor = 1.1

test_trial_conditions %>%
  mutate(QVDiff = QVLeft-QVRight,
         EVDiff = EVLeft-EVRight,
         distortedProb = (delt * (probFractalDraw)^gamm) / ( (delt * (probFractalDraw)^gamm) + (1-probFractalDraw)^gamm ),
         distortedProbDelt = (delt * changeFactor * (probFractalDraw)^gamm) / ( (delt* changeFactor * (probFractalDraw)^gamm) + (1-probFractalDraw)^gamm ),
         distortedProbGamm = (delt * (probFractalDraw)^(gamm* changeFactor)) / ( (delt* changeFactor * (probFractalDraw)^(gamm* changeFactor)) + (1-probFractalDraw)^(gamm* changeFactor) ),
         weightedMuMean = dR * ( (distortedProb*QVDiff) + ((1-probFractalDraw)*EVDiff) ),
         wmmDr =  dR *changeFactor* ( (distortedProb*QVDiff) + ((1-probFractalDraw)*EVDiff) ),
         wmmDelt = dR * ( (distortedProbDelt*QVDiff) + ((1-probFractalDraw)*EVDiff) ),
         wmmGamm = dR * ( (distortedProbGamm*QVDiff) + ((1-probFractalDraw)*EVDiff) ) ) %>%
  select(weightedMuMean, wmmDr, wmmDelt, wmmGamm) %>%
  gather(key, value, -weightedMuMean) %>%
  mutate(wmmAbsDiff = abs(abs(value) - abs(weightedMuMean)) ) %>%
  ggplot(aes(wmmAbsDiff, fill=key))+
  geom_density(position="identity", alpha=.5, bins=30)+
  theme(legend.position="bottom")+
  labs(fill="")
```

How much to perturb in what dimension?
Stopping criteria for perturbations?

```{r}

```

How would that be different from many random starts?
Random starts can be parallelized more by generating a list of starter values and then submitting each as a separate job.
Can also generate many random datasets.
How would you then organize the output of each?
Would you compute posterior among these different optimized values?

Quick test with a loop to make sure naming and saving works

```{r}
num_starts = 2

all_optim_out = list()

for(i in 1:num_starts){
  start_d = runif(1, 0, 1)
  start_sigma = runif(1, 0, 1)
  start_delta = runif(1, 1, 5)
  start_gamma = runif(1, 1, 5)
  
  start_str = paste(round(start_d, 2), round(start_sigma, 2), round(start_delta, 2), round(start_gamma, 2), sep = ", ")
  
  all_optim_out[[start_str]] = optim_save(c(start_d, start_sigma, start_delta, start_sigma), get_task_nll, data=test_data, par_names = c("d", "sigma", "delta", "gamma"), model_name="model1b", control = list(maxit=10))
}
```

```{r}
names(all_optim_out)
```

```{r}
all_optim_out[[1]]$iterations_df
```

# Stop clusters

```{r}
# parallel::stopCluster(cl = my.fit.cluster)
# parallel::stopCluster(cl = my.sim.cluster)
```

------------------------------------------------------------------------------------------------------------------------------------------------------------------------
How viable is this with Stan or with the Lombardi toolbox using hierarchical models? Can you use the trial likelihood function to code the bespoke model in Stan?

```{r}

```

What do we know so far?
  Slight lottery bias in pooled data
  Categorically faster RTs when probFractalDraw == 1 in pooled data
  Slower decisions the more both attributes need to be considered in pooled data
  Individual differences both in all of the above stylized facts and in the best fitting RL model
What will we do with these parameters? 
  Individual difference analyses?
  Trial level covariate for imaging?
Move things over to remote cluster for subject estimation?
Combined DDM + RL modeling:
  Wouldn't be able to take advantage of `foreach` package in `sim_task` and `fit_task`.
  Without paralellization how would you implement it in `sim_task` and `fit_task` with serial updating of QValues?
  Would it be possible to implement in Stan?