---
title: 'Experience vs. description based decision-making project: RL model fits onto
  behavioral data'
output:
  html_document:
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---

# Set up environment and load in data

```{r include=FALSE}
library(tidyverse)
library(gridExtra)
library(rstan)
library(brms)
library(GGally)
library(here)
cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
```

```{r}
helpers_path = here('helpers/')
```

# Read in *clean* behavioral data

```{r}
source(paste0(helpers_path,'01_clean_behavioral_data.R'))
source(paste0(helpers_path, 'extract_var_for_stan.R'))
source(paste0(helpers_path, 'get_qvals.R'))
source(paste0(helpers_path, 'facet_wrap_equal.R'))
source(paste0(helpers_path, 'organize_stan_output.R'))
```

Set theme for plots

```{r}
theme_set(theme_bw())
```

# Evidence for "learning" fractal values

Is there evidence if "learning" to justify the fitting of an RL model?

As time goes by are subjects more likely to choose the fractal with the higher reward probability? There is a subtle increase in the probability of choosing the fractal with the higher reward probability. This change in probability of choosing the fractal with the higher reward probability is larger when the reward depends only on fractals (except for the last session, which bizarrely has the opposite pattern).

```{r}
tmp = clean_beh_data %>%
  mutate(leftFractalBetter = fractalLeftProb>fractalRightProb,
         choseBetterFractal = ifelse(leftFractalBetter & choiceLeft == 1, 1, 
                                     ifelse(!leftFractalBetter & choiceLeft == 0, 1, 0)))
p1 = tmp %>%
  ggplot(aes(trialNum, choseBetterFractal, col=as.factor(session)))+
  # ggplot(aes(trialNum, choseBetterFractal))+
  geom_smooth(formula = 'y~x', method = "glm", method.args = list(family=binomial), fullrange=TRUE, alpha=.1)+
  labs(title="All trials", x="Trial Number", y="p(Chose better fractal)", color="Session")+
  coord_cartesian(ylim=c(0.4,.8))+
  scale_color_manual(values=cbbPalette)+
  theme(legend.position="bottom")

p2 = tmp %>%
  filter(probFractalDraw>.5) %>%
  ggplot(aes(trialNum, choseBetterFractal, col=as.factor(session)))+
  # ggplot(aes(trialNum, choseBetterFractal))+
  geom_smooth(formula = 'y~x', method = "glm", method.args = list(family=binomial), fullrange=TRUE, alpha=.1)+
  labs(title="p(Fractal)>.5", x="Trial Number", y="p(Chose better fractal)", color="Session")+
  coord_cartesian(ylim=c(0.4,.8))+
  scale_color_manual(values=cbbPalette)+
  theme(legend.position="bottom")

grid.arrange(p1, p2, nrow=1, ncol=2)
```

```{r}
rm(p1, p2)
```

Bayesian multilevel model checking if the slopes in the above figure are different than 0. 

When looking at all trials (left figure) the overall slope of all sessions is not different than 0 (not shown below). When including an interaction term with session number then there is a very small but non-zero increase in the probability of choosing the better fractal (i.e. learning) in sessions 1 and 3 but not in the other sessions.

```{r eval=FALSE}
# The default prior for population parameters is an improper flat prior over the reals
m = brm(choseBetterFractal ~ trialNum*as.factor(session) + (1|subnum),
        data=tmp, family=bernoulli(link="logit"))
```

```{r eval=FALSE}
summary(m)
```

Filtering only trials where probability of a fractal draw is >.5, i.e. trials where learning about the fractal values would be more consequential we find a slightly stronger increase in the probability of choosing the better fractal later in the session in sessions 1 through 3 though still no effect for the other sessions.

```{r eval=FALSE}
m2= brm(choseBetterFractal ~ trialNum*as.factor(session) + (1|subnum),
        data=tmp %>% filter(probFractalDraw>.5), family=bernoulli(link="logit"))
```

```{r eval=FALSE}
summary(m2)
```

```{r eval=FALSE}
rm(m, m2, tmp)
```

# Define two systems model

On each trial is subjects process not only the two fractals but also their relevance to the trial's reward, as well as, the lottery information. So to describe the choice process in this task, including accounting for learning about fractals across trials, the computational model must include all of these factors.

In this model the probability of choosing the option on the left is described as a softmax temperature that depends on the value difference between the left and right pairs. The free parameter $\beta$ controls the extent to which the choice depends on this value difference instead of chance.

$$p(choice = left) = \frac{1}{1+e^{-\beta(V_{left} - V_{right})}}$$
The value of each pair is defined as a weighted average of both of its components. The two components of each pair, the lottery and the fractal is weighted proportional to their relevance for that trial.

$$V_i = (1-w(pFrac))EV_i + w(pFrac)QV_i, \:i \in \{left, right\}$$
Given the large amount of evidence from behavioral economics suggesting that people do not perceive probabilities linearly but distort them the probability weighting function $\w$ is conceived of as a non-linear function. As described in Gonzalez and Wu (1999) we parameterized it as

$$w(pFrac) = \frac{\delta*pFrac^{\gamma}}{\delta*pFrac^{\gamma}+(1- pFrac)^{\gamma}}$$
where $\delta$ (primarily) controls the elevation/intercept to capture attractiveness of the outcome weighted by the distorted probability and $\gamma$ (primarily) controls curvature to capture the discriminability of distorted probabilities from each other. They call this shape "linear in log odds." Note the parameters control the two aspects of the curve primarily and not completely because the ends of the curves are fixed at 0 and 1 ("pinching").

This is not the only probability distortion function that could have been used. An alternative proposed by Prelec (1998) is

$$w(pFrac) = exp(-\delta(-log(pFrac))^\gamma)$$
which is very similar in shape to the previous one. This one has the key property of "compound invariance" and only studies specifically designed to pit the two functions each other can discriminate them.

While these two models with two parameters each are often indistinguishable models with only one parameter for the probability distortion function is often a worse fit.

The expected value of each lottery is computed by multiplying the probability of winning with the reward amount.

$$EV_{i, t} = p_{i, t} V{i_t}$$
The Q-value of each fractal is adjusted in each trial by a reward prediction error weighted by a learning rate ($\alpha$).

$$QV_{i,t} = QV_{i, t-1} + \alpha(R_{i, t-1}- QV_{i, t-1})$$
# Identifiability analysis

What does the likelihood surface look like for various combinations of parameters? Are they distinct enough to be identified successfully?

For each parameter combination the simulations below are for 25 subjects completing 5 runs of 60 trials with the same drift parameters for the fractal reward probabilities (i.e. the same task structure as the empirical data).

Lighter colors are higher log likelihood.

```{r}
tmp = sim_out %>%
  group_by(alpha, beta, delta, gamma) %>%
  summarise(logLik_m = mean(logLik),
            logLik_sd = sd(logLik),
            .groups='keep')

tmp %>%
  mutate(alpha = as.factor(alpha),
         beta=as.factor(beta),
         gamma = paste0("\u03b3= ", gamma),
         delta = paste0("\u03b4= ", delta)) %>%
  ggplot(aes(alpha, beta, fill=logLik_m))+
  geom_tile()+
  facet_grid(gamma~delta)+
  theme(legend.position = "bottom")+
  labs(title = "Mean log likelihood from 25 simulations per cell", 
       x = "\u03b1",
       y = "\u03b2",
       fill="")
```
Since each cell in the plot below is the average of 25 subjects there is variability in the likelihoods. The plot below depicts this variability for each parameter combination.

```{r}
tmp %>%
  mutate(alpha = as.factor(alpha),
         beta=as.factor(beta),
         gamma = paste0("\u03b3= ", gamma),
         delta = paste0("\u03b4= ", delta)) %>%
  ggplot(aes(alpha, beta, fill=logLik_sd))+
  geom_tile()+
  facet_grid(gamma~delta)+
  theme(legend.position = "bottom")+
  labs(title = "SD of log likelihood from 25 simulations per cell", 
       x = "\u03b1",
       y = "\u03b2",
       fill="")
```

Does anything else become apparent when looking at delta and gamma on the axes?

```{r}
tmp %>%
  mutate(gamma = as.factor(gamma),
         delta = as.factor(delta),
         alpha = paste0("\u03b1= ", alpha),
         beta = paste0("\u03b2= ", beta)) %>%
  ggplot(aes(gamma, delta, fill=logLik_m))+
  geom_tile()+
  facet_grid(alpha~beta)+
  theme(legend.position = "bottom")+
  labs(title = "Mean log likelihood from 25 simulations per cell", 
       x = "\u03b3",
       y = "\u03b4",
       fill="")
```

```{r}
tmp %>%
  mutate(gamma = as.factor(gamma),
         delta = as.factor(delta),
         alpha = paste0("\u03b1= ", alpha),
         beta = paste0("\u03b2= ", beta)) %>%
  ggplot(aes(gamma, delta, fill=logLik_sd))+
  geom_tile()+
  facet_grid(alpha~beta)+
  theme(legend.position = "bottom")+
  labs(title = "SD of log likelihood from 25 simulations per cell", 
       x = "\u03b3",
       y = "\u03b4",
       fill="")
```

Why are the likelihoods both higher on average but also more variable for large values of beta combined with large values of alpha?

What happens when beta is large? No left choice.

```{r}
pars = data.frame(alpha=.5, beta=4.5, delta=0.7, gamma=0.3)
trials = sim_trials()
tmp = sim_choice_data(trials, pars)
tmp = tmp$data
tmp %>%
  mutate(leftBetter = 4.5*(optValLeft-optValRight),
         choiceProb = exp(leftBetter)/(1+exp(leftBetter)),
         flip=rbinom(300,1,choiceProb)) 
```

Are things better with more data?

```{r}

```

# Non-hierarchical fit

Organize data

```{r}
num_subjs = length(unique(clean_beh_data$subnum))

num_trials = clean_beh_data %>%
  count(subnum) %>%
  select(n)
num_trials = num_trials$n

#subjects in rows, trials in columns
choices = extract_var_for_stan(clean_beh_data, choiceLeft)

clean_beh_data = clean_beh_data %>%
  mutate(leftLotteryEV = lotteryValue*lotteryProb,
         rightLotteryEV = referenceValue*referenceProb)

ev_left = extract_var_for_stan(clean_beh_data, leftLotteryEV)

ev_right = extract_var_for_stan(clean_beh_data, rightLotteryEV)

fractal_outcomes_left = extract_var_for_stan(clean_beh_data, leftFractalReward)

fractal_outcomes_right = extract_var_for_stan(clean_beh_data, rightFractalReward)

trial_pFrac = extract_var_for_stan(clean_beh_data, probFractalDraw)

m_data=list(num_subjs = num_subjs,
            num_trials = num_trials,
            choices = choices,
            ev_left = ev_left,
            ev_right = ev_right,
            fractal_outcomes_left = fractal_outcomes_left,
            fractal_outcomes_right = fractal_outcomes_right,
            trial_pFrac = trial_pFrac)

rm(num_subjs, num_trials, choices, ev_left, ev_right, fractal_outcomes_left, fractal_outcomes_right, trial_pFrac)
```

Fit model for all subjects separately

```{r}
if(file.exists(paste0(helpers_path, 'stanModels/fit_twoValSystemsWithRL.RDS'))){
  fit = readRDS(paste0(helpers_path, 'stanModels/fit_twoValSystemsWithRL.RDS'))
  rm(m_data)
} else {
  m = stan_model(paste0(helpers_path, 'stanModels/fit_twoValSystemsWithRL.stan'))
  fit = sampling(m, data=m_data)
  saveRDS(fit, paste0(helpers_path, 'stanModels/fit_twoValSystemsWithRL.RDS'))
  rm(m, m_data)}
```

```{r}
out = organize_stan_output(fit, subj_par_names = c("alpha","gamma", "delta", "beta"))
par_ests = out$par_ests
rm(out)
```

## Log likelihood distributions

Distribution of likelihood across iterations for each subject. Not very peaky for most. Suggests that MLE might not be very well differentiated from other combinations of parameters.

```{r}
par_ests %>%
  filter(par == "alpha") %>%
  ggplot(aes(logLik)) +
  geom_histogram(alpha=.5, bins=30)+
  facet_wrap(~subnum, scales="free_x")+
  xlab("Log Likelihood")+
  ylab("")+
  ggtitle("Distribution of log likelihoods across the samples for each subject")+
  theme(axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        panel.grid = element_blank())
```

## MLE vs median estimates

How do the ML and posterior medians differ? 

**Note: posterior median estimates are from different iterations!**

```{r}
tmp1 = par_ests %>%
  group_by(subnum, par) %>%
  filter(logLik == max(logLik))

tmp2 = par_ests %>%
  group_by(subnum, par) %>%
  summarise(est = median(value), .groups='keep')

tmp = tmp2 %>%
  left_join(tmp1, by=c("subnum", "par")) 

tmp %>%
  ggplot(aes(est, value))+
  geom_point()+
  geom_abline(slope=1, intercept = 0, linetype="dashed")+
  facet_wrap_equal(~par, scales='free')+
  xlab("Posterior median")+
  ylab("Maximum likelihood")
```
```{r}
tmp %>%
  select(-logLik) %>%
  gather(key, value, -subnum, -par) %>%
  mutate(key = ifelse(key=="est", "Posterior median", "Maximum likelihood")) %>%
  ggplot(aes(value, fill=key))+
  geom_histogram(alpha=.5, bins=30, position="identity")+
  facet_wrap(~par, scale='free')+
  theme(legend.title = element_blank(),
        legend.position = "bottom")+
  xlab("")+
  ylab("")
```

Where does the MLE fall on the posteriors of each parameter? **Often not in a very representative part of the full posterior.**

```{r fig.height=8, fig.width=4}
par_ests %>%
  ggplot(aes(value))+
  geom_histogram(alpha=.5, bins = 30)+
  facet_grid(subnum~par, scale="free")+
  geom_vline(aes(xintercept = value), tmp1, color=cbbPalette[5])+
  theme(axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        panel.grid = element_blank(),
        strip.text.x = element_text(size=14),
        axis.text.x = element_text(size=14))+
  xlab("")+
  ylab("")
```

How far are the likelihoods of the median estimates from the maximum likelihood?

```{r}
tmp3 = par_ests %>%
  group_by(subnum, par) %>%
  mutate(est = median(value), 
         diff_from_med = abs(value - est)) %>%
  filter(diff_from_med == min(diff_from_med)) %>%
  filter(logLik == max(logLik)) %>%
  select(subnum, par, logLik) %>%
  spread(par, logLik)

par_ests %>%
  filter(par == "alpha") %>%
  ggplot(aes(logLik)) +
  geom_histogram(alpha=.5, bins=30)+
  facet_wrap(~subnum, scales="free_x")+
  xlab("Log Likelihood")+
  ylab("")+
  ggtitle("Distribution of log likelihoods across the samples for each subject")+
  theme(axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        panel.grid = element_blank(),
        legend.position="bottom")+
  geom_vline(aes(xintercept = alpha, color="alpha"), tmp3)+
  geom_vline(aes(xintercept = beta, color="beta"), tmp3)+
  geom_vline(aes(xintercept = delta, color="delta"), tmp3)+
  geom_vline(aes(xintercept = gamma, color="gamma"), tmp3)+
    scale_color_manual(name="",values = c(alpha = cbbPalette[1], beta=cbbPalette[2], delta=cbbPalette[3], gamma=cbbPalette[4]))

```

```{r}
rm(tmp, tmp1, tmp2, tmp3)
```


## Covariance between parameters

Are there any dependencies between the parameters?

```{r}
par_ests %>%
  group_by(subnum, par) %>%
  summarise(est = median(value), .groups='keep') %>%
  # filter(logLik == max(logLik)) %>%
  ungroup() %>%
  # select(-logLik)%>%
  spread(par, est) %>%
  # spread(par, value) %>%
  select(-subnum) %>%
  ggpairs(progress = FALSE)
```

# Hierarchical model

```{r}
num_subjs = length(unique(clean_beh_data$subnum))

num_trials = clean_beh_data %>%
  count(subnum) %>%
  select(n)
num_trials = num_trials$n

#subjects in rows, trials in columns
choices = extract_var_for_stan(clean_beh_data, choiceLeft)

clean_beh_data = clean_beh_data %>%
  mutate(leftLotteryEV = lotteryValue*lotteryProb,
         rightLotteryEV = referenceValue*referenceProb)

ev_left = extract_var_for_stan(clean_beh_data, leftLotteryEV)

ev_right = extract_var_for_stan(clean_beh_data, rightLotteryEV)

fractal_outcomes_left = extract_var_for_stan(clean_beh_data, leftFractalReward)

fractal_outcomes_right = extract_var_for_stan(clean_beh_data, rightFractalReward)

trial_pFrac = extract_var_for_stan(clean_beh_data, probFractalDraw)

m_data=list(num_subjs = num_subjs,
            num_trials = num_trials,
            choices = choices,
            ev_left = ev_left,
            ev_right = ev_right,
            fractal_outcomes_left = fractal_outcomes_left,
            fractal_outcomes_right = fractal_outcomes_right,
            trial_pFrac = trial_pFrac)

rm(num_subjs, num_trials, choices, ev_left, ev_right, fractal_outcomes_left, fractal_outcomes_right, trial_pFrac)
```

```{r}
if(file.exists(paste0(helpers_path, 'stanModels/fit_twoValSystemsWithRL_hierarchical.RDS'))){
  fit = readRDS(paste0(helpers_path, 'stanModels/fit_twoValSystemsWithRL_hierarchical.RDS'))
  rm(m_data)
} else {
  m = stan_model(paste0(helpers_path, 'stanModels/fit_twoValSystemsWithRL_hierarchical.stan'))
  fit = sampling(m, data=m_data)
  saveRDS(fit, paste0(helpers_path, 'stanModels/fit_twoValSystemsWithRL_hierarchical.RDS'))
  rm(m, m_data)}
```

```{r}
out = organize_stan_output(fit, 
                           subj_par_names=c("alpha","gamma", "delta", "beta"),
                           group_par_names=c("g_alpha","g_gamma", "g_delta", "g_beta"))
par_ests = out$par_ests
g_par_ests = out$g_par_ests
rm(out)
```

```{r}
par_ests
```

## Log likelihood distribution

```{r}
par_ests %>%
  filter(par == "alpha") %>%
  ggplot(aes(logLik)) +
  geom_histogram(alpha=.5, bins=30)+
  facet_wrap(~subnum, scales="free_x")+
  xlab("Log Likelihood")+
  ylab("")+
  ggtitle("Distribution of log likelihoods across the samples for each subject")+
  theme(axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        panel.grid = element_blank())
```

## MLE vs median posterior estimates

```{r}
tmp1 = par_ests %>%
  group_by(subnum, par) %>%
  filter(logLik == max(logLik))

tmp2 = par_ests %>%
  group_by(subnum, par) %>%
  summarise(est = median(value), .groups='keep')

tmp = tmp2 %>%
  left_join(tmp1, by=c("subnum", "par")) 

tmp %>%
  ggplot(aes(est, value))+
  geom_point()+
  geom_abline(slope=1, intercept = 0, linetype="dashed")+
  facet_wrap_equal(~par, scales='free')+
  xlab("Posterior median")+
  ylab("Maximum likelihood")
```

```{r fig.height=8, fig.width=4}
par_ests %>%
  ggplot(aes(value))+
  geom_histogram(alpha=.5, bins = 30)+
  facet_grid(subnum~par, scale="free")+
  geom_vline(aes(xintercept = value), tmp1, color=cbbPalette[5])+
  theme(axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        panel.grid = element_blank(),
        strip.text.x = element_text(size=14),
        axis.text.x = element_text(size=14))+
  xlab("")+
  ylab("")
```

## Covariance between parameters

# TO DO: Posterior predictive checks

Sample parameters from posterior
Simulate choice data with sampled parameters


```{r}

```
