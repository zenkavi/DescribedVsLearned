---
title: "Experience vs. description based decision-making project: RL model fits onto behavioral data"
output: 
  html_document:
    toc: true
    toc_float: true
---

# Set up environment and load in data

```{r}
library(tidyverse)
library(gridExtra)
library(rstan)
library(brms)
library(here)
```

```{r}
helpers_path = here('helpers/')
```

# Read in *clean* behavioral data

```{r}
source(paste0(helpers_path,'01_clean_behavioral_data.R'))
source(paste0(helpers_path, 'extract_var_for_stan.R'))
```

Set theme for plots

```{r}
theme_set(theme_bw())
```

# Evidence for "learning" fractal values

Is there evidence if "learning" to justify the fitting of an RL model?

As time goes by are subjects more likely to choose the fractal with the higher reward probability? There is a subtle increase in the probability of choosing the fractal with the higher reward probability. This change in probability of choosing the fractal with the higher reward probability is larger when the reward depends only on fractals (except for the last session, which bizarrely has the opposite pattern).

```{r}
tmp = clean_beh_data %>%
  mutate(leftFractalBetter = fractalLeftProb>fractalRightProb,
         choseBetterFractal = ifelse(leftFractalBetter & choiceLeft == 1, 1, 
                                     ifelse(!leftFractalBetter & choiceLeft == 0, 1, 0)))
p1 = tmp %>%
  ggplot(aes(trialNum, choseBetterFractal, col=as.factor(session)))+
  # ggplot(aes(trialNum, choseBetterFractal))+
  geom_smooth(formula = 'y~x', method = "glm", method.args = list(family=binomial), fullrange=TRUE, alpha=.1)+
  labs(title="All trials", x="Trial Number", y="p(Chose better fractal)", color="Session")+
  coord_cartesian(ylim=c(0.4,.8))+
  scale_color_manual(values=cbbPalette)+
  theme(legend.position="bottom")

p2 = tmp %>%
  filter(probFractalDraw>.5) %>%
  ggplot(aes(trialNum, choseBetterFractal, col=as.factor(session)))+
  # ggplot(aes(trialNum, choseBetterFractal))+
  geom_smooth(formula = 'y~x', method = "glm", method.args = list(family=binomial), fullrange=TRUE, alpha=.1)+
  labs(title="p(Fractal)>.5", x="Trial Number", y="p(Chose better fractal)", color="Session")+
  coord_cartesian(ylim=c(0.4,.8))+
  scale_color_manual(values=cbbPalette)+
  theme(legend.position="bottom")

grid.arrange(p1, p2, nrow=1, ncol=2)
```

```{r}
rm(p1, p2)
```

Bayesian multilevel model checking if the slopes in the above figure are different than 0. 

When looking at all trials (left figure) the overall slope of all sessions is not different than 0 (not shown below). When including an interaction term with session number then there is a very small but non-zero increase in the probability of choosing the better fractal (i.e. learning) in sessions 1 and 3 but not in the other sessions.

```{r}
# The default prior for population parameters is an improper flat prior over the reals
m = brm(choseBetterFractal ~ trialNum*as.factor(session) + (1|subnum),
        data=tmp, family=bernoulli(link="logit"))
```

```{r}
summary(m)
```

Filtering only trials where probability of a fractal draw is >.5, i.e. trials where learning about the fractal values would be more consequential we find a slightly stronger increase in the probability of choosing the better fractal later in the session in sessions 1 through 3 though still no effect for the other sessions.

```{r}
m2= brm(choseBetterFractal ~ trialNum*as.factor(session) + (1|subnum),
        data=tmp %>% filter(probFractalDraw>.5), family=bernoulli(link="logit"))
```

```{r}
summary(m2)
```

```{r}
rm(m, m2, tmp)
```

# Reshape data for RL (only) model

```{r}
num_subjs = length(unique(clean_beh_data$subnum))

num_trials = clean_beh_data %>%
  count(subnum) %>%
  select(n)
num_trials = num_trials$n

#subjects in rows, trials in columns
choices = extract_var_for_stan(clean_beh_data, choiceLeft)

outcomes_left = extract_var_for_stan(clean_beh_data, leftFractalReward)
  
outcomes_right = extract_var_for_stan(clean_beh_data, rightFractalReward)

m_data=list(num_subjs = num_subjs,
            num_trials = num_trials,
            choices = choices,
            outcomes_left = outcomes_left,
            outcomes_right=outcomes_right)

rm(num_subjs, num_trials, choices, outcomes_left, outcomes_right)
```

# Fit model for all subjects

```{r}
if(file.exists(paste0(helpers_path, 'stanModels/fit_qlearning.RDS'))){
  fit = readRDS(paste0(helpers_path, 'stanModels/fit_qlearning.RDS'))
} else {
  m = stan_model('../helpers/stanModels/fit_qlearning.stan')
  fit = sampling(m, data=m_data)}
```

# Organize output

```{r}
# Extract parameters from fit object
par_ests = data.frame(extract(fit, c("alphas", "betas")))  %>%
  gather(key, value) %>%
  separate(key, c('par', 'subj'), sep='\\.')

# Add correct subject identifiers
par_ests = data.frame(subnum = unique(clean_beh_data$subnum)) %>%
  mutate(subj = as.character(1:n())) %>%
  right_join(par_ests, by='subj') %>%
  select(-subj)
```

Distributions of the recovered (median) parameters

```{r}
par_ests %>%
  group_by(subnum, par) %>%
  summarise(est = median(value), .groups='keep') %>%
  ggplot(aes(est))+
  geom_histogram(alpha=.5, bins=30)+
  facet_wrap(~par, scales='free_x')+
  xlab("Median estimates for all subjects")
```

```{r}
par_ests %>%
  filter(par == "alphas") %>%
  ggplot(aes(value))+
  geom_histogram(alpha=.5, bins=50)+
  facet_wrap(~subnum, scales='free_y')+
  labs(title = "Posterior distribution of learning rates for each subject",
       xlab="", ylab="")
```

```{r}
par_ests %>%
  filter(par == "betas") %>%
  ggplot(aes(value))+
  geom_histogram(alpha=.5, bins=50)+
  facet_wrap(~subnum, scales='free_y')+
  labs(title = "Posterior distribution of inverse temperatures for each subject",
       xlab="", ylab="")
```

Save median estimates to `clean_beh_data`

```{r}
clean_beh_data = par_ests %>%
  group_by(subnum, par) %>%
  summarise(est = median(value), .groups='keep') %>%
  spread(par, est) %>%
  left_join(clean_beh_data, by='subnum')
rm(fit, par_ests)
```

Add Q values to each trial using median parameter estimates from the Q-learning model fit.

```{r}

get_qvals = function(subj_data){
  subj_data$leftQValue = 0
  subj_data$rightQValue = 0
  for (i in 2:nrow(subj_data)){
  subj_data$leftQValue[i] = subj_data$alphas[i] * (subj_data$leftFractalReward[i-1] - subj_data$leftQValue[i-1])
  subj_data$rightQValue[i] = subj_data$alphas[i] * (subj_data$rightFractalReward[i-1] - subj_data$rightQValue[i-1])
  }
  return(subj_data)
}

clean_beh_data = clean_beh_data %>%
  group_by(subnum) %>%
  do(get_qvals(.)) %>%
  ungroup()

```
