---
title: 'Experience vs. description based decision-making project: Two system valuation systems model fit'
output:
  html_document:
    toc: yes
    toc_float: yes
    code_folding: 'hide'
  pdf_document:
    toc: yes
---

# Set up

```{r include=FALSE}
library(tidyverse)
library(here)
cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
```

```{r}
helpers_path = here('helpers/')
```

# Read in data

```{r include=FALSE}
source(paste0(helpers_path,'01_clean_behavioral_data.R'))
source(paste0(helpers_path, 'facet_wrap_equal.R'))
source(paste0(helpers_path, 'identifiability_analysis.R'))
source(paste0(helpers_path, 'make_posterior_predictive_data.R'))
```

Set theme for plots

```{r}
theme_set(theme_bw())
```

# Define model

On each trial subjects process not only the two fractals but also their relevance to the trial's reward, as well as, the lottery information. So to describe the choice process in this task, including accounting for learning about fractals across trials, the computational model must include all of these factors.

In this model the probability of choosing the option on the left is described as a softmax function that depends on the value difference between the left and right pairs. The free parameter $\beta$ controls the extent to which the choice depends on this value difference instead of chance.

$$p(choice = left) = \frac{1}{1+e^{-\beta(V_{left} - V_{right})}}$$
The value of each pair is defined as a weighted average of both of its components. The two components of each pair, the lottery and the fractal is weighted proportional to their relevance for that trial.

$$V_i = (1-w(pFrac))EV_i + w(pFrac)QV_i, \:i \in \{left, right\}$$
Given the large amount of evidence from behavioral economics suggesting that people do not perceive probabilities linearly but distort them the probability weighting function $w$ is conceived of as a non-linear function. As described in Gonzalez and Wu (1999) we parameterized it as

$$w(pFrac) = \frac{\delta*pFrac^{\gamma}}{\delta*pFrac^{\gamma}+(1- pFrac)^{\gamma}}$$
where $\delta$ (primarily) controls the elevation/intercept to capture attractiveness of the outcome weighted by the distorted probability and $\gamma$ (primarily) controls curvature to capture the discriminability of distorted probabilities from each other. They call this shape "linear in log odds." Note the parameters control the two aspects of the curve primarily and not completely because the ends of the curves are fixed at 0 and 1 ("pinching").

This is not the only probability distortion function that could have been used. An alternative proposed by Prelec (1998) is $w(pFrac) = exp(-\delta(-log(pFrac))^\gamma)$ which is very similar in shape to the previous one. This one has the key property of "compound invariance" and only studies specifically designed to pit the two functions each other can discriminate them.

While these two models with two parameters each are often indistinguishable models with only one parameter for the probability distortion function is often a worse fit.

The expected value of each lottery is computed by multiplying the probability of winning with the reward amount.

$$EV_{i, t} = p_{i, t} V{i_t}$$
The Q-value of each fractal is adjusted in each trial by a reward prediction error weighted by a learning rate ($\alpha$).

$$QV_{i,t} = QV_{i, t-1} + \alpha(R_{i, t-1}- QV_{i, t-1})$$

# Model fit

```{r}
source(paste0(helpers_path, 'fit_twoValSystemsWithRL.R'))
# Rename objects 
par_ests_nh = par_ests
fit_nh = fit
rm(par_ests, fit)
source(paste0(helpers_path, 'fit_twoValSystemsWithRL_hierarchical.R'))
```

## Log likelihood distributions

Distribution of likelihood across iterations for each subject. Hierarchical fits have higher likelihoods and are more peakier, which would help with distinguishing an MLE estimate if desired.

**Focusing on hierarchical fits only below.**

```{r}
par_ests %>%
  filter(par == "alpha") %>%
  mutate(fit_type = "hierarchical") %>% 
  rbind(par_ests_nh %>%
          filter(par == "alpha") %>%
          mutate(fit_type = "non-hierarchical"))%>%
  ggplot(aes(logLik, fill=fit_type)) +
  geom_histogram(alpha=.5, bins=30, position="identity")+
  facet_wrap(~subnum, scales="free_x")+
  xlab("Log Likelihood")+
  ylab("")+
  ggtitle("Distribution of log likelihoods across the samples for each subject")+
  theme(axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        panel.grid = element_blank(),
        legend.position = "bottom",
        legend.title = element_blank())+
  scale_fill_manual(values=c(cbbPalette[1], cbbPalette[2]))
```

```{r echo=FALSE}
rm(par_ests_nh, fit_nh)
```

## Parameter point estimates

### Group parameters

```{r}
g_par_ests %>%
  mutate(type='posterior') %>%
  rbind(data.frame(g_alpha = rbeta(16000,1,1),
                   g_gamma = rgamma(16000,1,5),
                   g_delta = rgamma(16000,1,5),
                   g_beta = rgamma(16000,1,2)) %>%
          gather(key, value) %>%
          mutate(type="prior")) %>%
  mutate(type=factor(type,levels=c("prior","posterior"))) %>%
  ggplot(aes(value, fill=type))+
  geom_histogram(bins=30, alpha=.5, position="identity")+
  facet_wrap(~key, scales='free')+
  theme(legend.position = "bottom",
        legend.title = element_blank(),
        panel.grid = element_blank())+
  scale_fill_manual(values = c(cbbPalette[1], cbbPalette[2]))+
  xlab("")+
  ylab("")
```

### Individual parameters

How do the ML and posterior medians differ? 

Most pronounced differences are on $\beta$.

**Note: posterior median estimates are from different iterations/samples whereas the estimates that have the highest likelihood are from the same sample.**

```{r}
tmp1 = par_ests %>%
  group_by(subnum, par) %>%
  filter(logLik == max(logLik))

tmp2 = par_ests %>%
  group_by(subnum, par) %>%
  summarise(est = mean(value), .groups='keep')

tmp = tmp2 %>%
  left_join(tmp1, by=c("subnum", "par")) 

tmp %>%
  ggplot(aes(est, value))+
  geom_point()+
  geom_abline(slope=1, intercept = 0, linetype="dashed")+
  facet_wrap_equal(~par, scales='free')+
  xlab("Posterior mean")+
  ylab("Maximum likelihood")
```

Where does the MLE fall on the posteriors of each parameter? 

```{r fig.height=8, fig.width=8}
par_ests %>%
  ggplot(aes(value))+
  geom_histogram(alpha=.5, bins = 30)+
  facet_grid(subnum~par, scale="free")+
  geom_vline(aes(xintercept = value), tmp1, color=cbbPalette[5])+
  theme(axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        panel.grid = element_blank(),
        strip.text.x = element_text(size=14),
        axis.text.x = element_text(size=14))+
  xlab("")+
  ylab("")
```
How far are the likelihoods of the median estimates from the maximum likelihood?

```{r fig.height=5, fig.width=8}
tmp3 = par_ests %>%
  group_by(subnum, par) %>%
  mutate(est = median(value), 
         diff_from_med = abs(value - est)) %>%
  filter(diff_from_med == min(diff_from_med)) %>%
  filter(logLik == max(logLik)) %>%
  select(subnum, par, logLik) %>%
  spread(par, logLik)

par_ests %>%
  filter(par == "alpha") %>%
  ggplot(aes(logLik)) +
  geom_histogram(alpha=.5, bins=30)+
  facet_wrap(~subnum, scales="free_x")+
  xlab("Log Likelihood")+
  ylab("")+
  ggtitle("Distribution of log likelihoods across the samples for each subject \noverlaid with likelihoods of posterior mean parameter estimates")+
  theme(axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        panel.grid = element_blank(),
        legend.position="bottom")+
  geom_vline(aes(xintercept = alpha, color="alpha"), tmp3)+
  geom_vline(aes(xintercept = beta, color="beta"), tmp3)+
  geom_vline(aes(xintercept = delta, color="delta"), tmp3)+
  geom_vline(aes(xintercept = gamma, color="gamma"), tmp3)+
    scale_color_manual(name="",values = c(alpha = cbbPalette[1], beta=cbbPalette[2], delta=cbbPalette[3], gamma=cbbPalette[4]))

```

```{r echo=FALSE}
rm(tmp, tmp1, tmp2, tmp3)
```

**MLE**  
- Point estimates from the same sampling iteration  
- MLE estimates differ most from point estimates for beta and gamma so choosing these as point estimates doesn't yield different data for all parameters.  

**Mean of posterior**  
- Point estimates from the different sampling iteration  
- Mean estimates have likelihoods that fall in the middle (instead of the max) of the likelihood distributions.

When would we need point estimates?  
- Individual difference analyses  
- Prediction error estimates for imaging  

## Covariance between parameters

Are there any dependencies between the parameters? Looking at correlations between samples from the **same** iteration.

```{r}
tmp = g_par_ests %>%
  group_by(key) %>%
  mutate(iter = 1:n()) %>%
  group_by(iter) %>%
  spread(key, value) %>%
  ungroup() %>%
  select(-iter)

round(cor(tmp), 3)  
```

```{r echo=FALSE}
rm(tmp)
```

# Identifiability analysis

Simulate 25 subjects using the median posterior group parameters as true parameters

```{r}
trueGroupPars = g_par_ests %>%
  group_by(key) %>%
  summarise(median_value = round(median(value), 2)) %>%
  spread(key, median_value)

numSims = 25

set.seed(2349813)

truePars = data.frame(alpha=truncnorm::rtruncnorm(numSims, mean=trueGroupPars$g_alpha, a=0, b=1), 
                      beta=truncnorm::rtruncnorm(numSims, mean=trueGroupPars$g_beta, a=0, b=5), 
                      gamma=truncnorm::rtruncnorm(numSims, mean=trueGroupPars$g_gamma, a=0),  
                      delta=truncnorm::rtruncnorm(numSims, mean=trueGroupPars$g_delta, a=0))

modelName = 'fit_twoValSystemsWithRL_hierarchical'

group_par_names=c("g_alpha","g_gamma", "g_delta", "g_beta")
id_par_ests = identifiability_analysis(truePars, modelName, group_par_names = group_par_names)

rm(modelName)
```

```{r}
id_par_ests$par_ests %>%
filter(par == "alpha") %>%
  ggplot(aes(logLik)) +
  geom_histogram(alpha=.5, bins=30)+
  facet_wrap(~subnum, scales="free_x")+
  xlab("Log Likelihood")+
  ylab("")+
  ggtitle("Distribution of log likelihoods across the samples for each subject")+
  theme(axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        panel.grid = element_blank())
```

Recovered group parameter posteriors. True values in blue.

```{r}
id_par_ests$g_par_ests %>%
  ggplot(aes(value))+
  geom_histogram(alpha=.5, bins = 30)+
  geom_vline(aes(xintercept = value), trueGroupPars %>% gather(key, value), color=cbbPalette[5])+
  facet_wrap(~key, scale="free")+
  theme(axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        panel.grid = element_blank(),
        strip.text.x = element_text(size=14),
        axis.text.x = element_text(size=14))+
  xlab("")+
  ylab("")
```

Recovered individual parameter posteriors. True values in blue.

```{r fig.height=8, fig.width=8}
id_par_ests$par_ests %>%
  ggplot(aes(value))+
  geom_histogram(alpha=.5, bins = 30)+
  facet_grid(subnum~par, scale="free")+
  geom_vline(aes(xintercept = value), truePars %>% mutate(subnum =1:n()) %>% gather(par, value, -subnum), color=cbbPalette[5])+
  theme(axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        panel.grid = element_blank(),
        strip.text.x = element_text(size=14),
        axis.text.x = element_text(size=14))+
  xlab("")+
  ylab("")
```

```{r echo=FALSE}
rm(id_par_ests, trueGroupPars, truePars)
```

# Posterior predictive checks

Do the subject parameters capture the behavioral patterns in the data?

Sample from each subject's posterior distributions 100 times and simulate data using these sampled parameters. Plot predicted versus actual choice as a function of the probability of a fractal draw.

```{r}
modelName = 'fit_twoValSystemsWithRL_hierarchical'

if(file.exists(paste0(helpers_path, 'pp_data_', modelName, '.RDS'))){
  pp_data = readRDS(paste0(helpers_path, 'pp_data_', modelName, '.RDS'))
} else{
  pp_data = make_posterior_predictive_data(numDraws = 100, modelName = modelName) 
}
```

```{r  fig.height=8, fig.width=8}
tmp = clean_beh_data %>%
  group_by(subnum, probFractalDraw) %>%
  summarise(propLeft = mean(choiceLeft), .groups='keep') %>%
  mutate(data_type = "actual")

pp_data %>%
  group_by(subnum, sampleNum, probFractalDraw) %>%
  summarise(propLeft = mean(choiceLeft), .groups='keep') %>%
  mutate(data_type = "predicted") %>%
  rbind(tmp) %>%
  ggplot(aes(factor(probFractalDraw), propLeft, color=data_type, alpha=data_type, shape=data_type, size=data_type))+
  # geom_boxplot(position=position_dodge(width = 0))+
  geom_point()+
  facet_wrap(~subnum) + 
  theme(panel.grid = element_blank(), legend.position = "bottom", legend.title = element_blank())+
  xlab("Probability of a Fractal Draw")+
  ylab("Proportion of left choices")+
  ggtitle("Actuals versus predicted choice generated using parameters sampled from the posterior")+
  scale_color_manual(values=c(cbbPalette[2], "gray"))+
  scale_alpha_manual(values=c(1, .5))+
  scale_shape_manual(values=c(19, 1))+
  scale_size_manual(values=c(2, 1))
  
```

```{r echo=FALSE}
rm(modelName, tmp)
```

# Linear w(p(Frac))

```{r}
tmp = par_ests_linearW %>%
  group_by(subnum, par) %>%
  summarise(est = mean(value), .groups='keep') %>%
  filter(par != "beta" & par != "alpha") %>%
  spread(par, est)

tmp = do.call("rbind", replicate(11, tmp, simplify = FALSE)) %>%
  arrange(subnum) %>%
  ungroup() %>%
  mutate(pFrac = rep(seq(0, 1, .1), 25), 
         wpFrac = w_int+w_slope*(pFrac)) 

tmp %>%
  mutate(w_int_pos = ifelse())
  ggplot(aes(pFrac, wpFrac, group=subnum))+
  geom_line(color=cbbPalette[5])+
  geom_abline(slope=1, intercept=0, linetype="dashed")
```

Comparison 