---
title: "Experience vs. description based decision-making project: Behavioral manipulation checks"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: 'hide'
---

# Set up environment and load in data

```{r include=FALSE}
library(tidyverse)
library(gridExtra)
library(brms)
library(here)
cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
theme_set(theme_bw())
sem <- function(x) {sd(x, na.rm=T) / sqrt(length(x))}
```

```{r include=FALSE}
helpers_path = here('helpers/')
source(paste0(helpers_path,'fit_twoValSystemsWithRL_hierarchical_noProbDistortion.R'))
source(paste0(helpers_path,'fit_twoValSystemsWithRL_hierarchical.R'))
source(paste0(helpers_path,'add_inferred_pars.R'))
```

Add estimates for model with linear w(p(Frac))

```{r}
clean_beh_data_noProbDistortion = add_inferred_pars(clean_beh_data, par_ests_noProbDistortion)
```

```{r}
clean_beh_data = add_inferred_pars(clean_beh_data, par_ests)
```

# Psychometric choice curves

Psychometric curve for all trials with weighted value difference. Q-values for the fractals and the weights for the two attributes are determined by the posterior means of the hierarchically fit two systems model.

```{r}
clean_beh_data %>%
  ggplot(aes(leftbundleValAdv, choiceLeft))+
  geom_line(aes(group=subnum),stat="smooth",formula = 'y~x', method = "glm", method.args = list(family=binomial), fullrange=TRUE, se=FALSE, alpha=.1, size=1, color=cbbPalette[5])+
  geom_smooth(formula = 'y~x', method = "glm", method.args = list(family=binomial), fullrange=TRUE, color=cbbPalette[3], alpha=.75)+
  labs(y = "p(Left)", x = "V(left)-V(right)", title="Psychometric choice curve based on weighted value difference between bundles")
```

Note that this is true for all levels of p(Frac) and conflict trial type

```{r warning=FALSE}
clean_beh_data %>%
  ggplot(aes(leftbundleValAdv, choiceLeft))+
  geom_line(aes(group=subnum),stat="smooth",formula = 'y~x', method = "glm", method.args = list(family=binomial), fullrange=TRUE, se=FALSE, alpha=.1, size=1, color=cbbPalette[5])+
  geom_smooth(formula = 'y~x', method = "glm", method.args = list(family=binomial), fullrange=TRUE, color=cbbPalette[3], alpha=.75)+
  labs(y = "p(Left)", x = "V(left)-V(right)")+
  facet_grid(conflictTrial ~ probFractalDraw)+
  theme(axis.ticks.x=element_blank(),
        axis.text.x=element_blank())
```

## Single relevant attribute trials

What percentage of all trials' rewards depend entirely either on the lottery or the fractals?

```{r}
tmp = clean_beh_data %>%
  filter(probFractalDraw == 1 | probFractalDraw == 0) %>%
  mutate(leftQVAdv = leftQValue - rightQValue,
         leftEVAdv = leftLotteryEV - rightLotteryEV)

round(nrow(tmp)/nrow(clean_beh_data), 3)
```

If subjects have understood the task then in these trials only the value difference of the relevant attribute should have an effect on choice.

- When p of fractal is 0 do subjects choose based on the EV difference between lotteries? Does the QV difference matter at all?

- When p of fractal is 1 do subjects choose based on the QV difference between fractals? Does the EV difference matter at all?

```{r warning=FALSE}
tmp %>%
  select(subnum, leftQVAdv, leftEVAdv, choiceLeft, probFractalDraw) %>%
  gather(key, value, -choiceLeft, -probFractalDraw, -subnum) %>%  
  mutate(key = ifelse(key == "leftEVAdv", "EV_left - EV_right", "QV_left - QV_right"),
         probFractalDraw = ifelse(probFractalDraw == 0, "p(Fractal)=0", "p(Fractal)=1")) %>%
  ggplot(aes(value, choiceLeft))+
  # geom_jitter(width=0.03, height=0.08, alpha = 0.05)+
  geom_line(aes(group=subnum),stat='smooth',formula = 'y~x', method = "glm", method.args = list(family=binomial), fullrange=TRUE, se=FALSE, alpha=.1, size=1, color=cbbPalette[5])+
  geom_smooth(formula = 'y~x', method = "glm", method.args = list(family=binomial), fullrange=TRUE, color=cbbPalette[3])+
  facet_grid(probFractalDraw~key)+
  xlab("")+
  ylab("p(Left)")
```

Multilevel model checking if the slopes of the top row above are different than 0.

```{r eval=FALSE}
m1 = brm(choiceLeft ~ leftQVAdv + leftEVAdv + (1|subnum),
        data=tmp %>% filter(probFractalDraw == 0), family=bernoulli(link="logit"))
```

When probability of a fractal is 0 the expected value (EV) difference between the lotteries (i.e. the only relevant attribute of each bundle) has a strong effect. The higher the EV of the left lottery the more likely the left bundle is chosen. 

There is also a much smaller negative effect of the Q-value difference even though the fractal values should be irrelevant. 

```{r eval=FALSE}
summary(m1)
```

Multilevel model checking if the slopes of the bottom row above are different than 0.

```{r eval=FALSE}
m2 = brm(choiceLeft ~ leftQVAdv + leftEVAdv + (1|subnum),
        data=tmp %>% filter(probFractalDraw == 1), family=bernoulli(link="logit"))
```

When the probability of the fractal draw is 1 so the reward only depends on the fractals only the learned Q-values for the fractals have an effect on choice (the lottery EV difference has no effect).

```{r eval=FALSE}
summary(m2)
```


```{r eval=FALSE}
rm(tmp, m1, m2)
```

# Attribute weights

Since there are two attributes in each bundle the observed and learned values of these attributes can agree or disagree on which bundle is better to choose. Trials where the fractal and lottery value difference favors the same bundle are called *no conflict* trials and trials where they favor different bundles are called *conflict* trials. 

What proportion of each type of trial is there? About half of the trials for each subject are conflict trials.

```{r}
round(prop.table(table(clean_beh_data$conflictTrial, clean_beh_data$subnum), margin=2), 2)
```

The true weight of each attribute is controlled and determined by the probability that a trial's reward would depend on a fractal or lottery draw. Subjects' choice can, however, indicate that they distort these weights. If a subjective distortion of this sort exists it should be reflected in choice as a bias towards one attribute over the other.

In conflict trials:  
- when reward depends more on lotteries people correctly choose the bundle with the better lottery above chance.  
- when reward depends equally on both attributes people are  slightly *biased* towards the bundle with the better lottery.  
- when reward depends more on fractals people choose the bundle with the better fractal more frequently *but not as much as they choose the better lottery for the equivalent levels of relevance*  

In no conflict trials:  
- when reward depends more or equally on lotteries people correctly choose the bundle that has both the better fractal and the better lottery.  
- when reward depends more on fractals people *incorrectly* choose the bundle with the better fractal and better lottery less frequently. This suggests that their decision **correctly** depends less on the lottery and more on the fractals but their inference about the fractal values are more noisy and therefore error prone because it involves learning and cannot be read off the screen.  

```{r}
clean_beh_data %>% 
  group_by(probFractalDraw, subnum, conflictTrial) %>%
  summarise(choseBetterLottery = mean(choseBetterLottery),
            .groups='keep') %>%
  ggplot(aes(as.factor(probFractalDraw), choseBetterLottery))+
  geom_boxplot(color=cbbPalette[2])+
  geom_hline(yintercept=0.5, linetype="dashed", size=1.5)+
  facet_grid(conflictTrial~.)+
  labs(x="p(Fractal)",y="Choice of bundle with better lottery")

```

## Psychometric analysis by conflict type

Modeling choice of the left bundle as a function of the lottery EV **and** fractal QV difference on each trial. Fixed effects analysis allowing for subject-specific random intercepts.

```{r}
source(paste0(helpers_path,'logit_choiceLeft.R'))
```

- Regardless of whether there is a conflict between the attributes for each bundle the effect of the fractal QV difference  **correctly increases** as fractals become more consequential for the trial's reward while the effect of the lottery EV difference **correctly decreases**.   
- While the effect of the QV difference is **correctly 0** when the lottery matters more lottery EV difference has an **incorrect positive** effect on choice even when it is less relevant for the trial's reward than the fractals. This reduces to 0 only when the lottery values are entirely irrelevant.  
- Consequently, for the same level of relevance lottery EV difference has a bigger effect on choice than the fractal QV difference.

```{r}
out_choiceLeft %>%
  mutate(est = as.numeric(est),
         l95 = as.numeric(l95),
         u95 = as.numeric(u95)) %>%
  mutate(iv = ifelse(iv == "leftEVAdv", "EV(left)-EV(right)", "QV(left)-QV(right)")) %>%
  ggplot(aes(probFractalDraw, est, col=iv))+
  geom_point(size=2)+
  geom_line(aes(group=iv))+
  geom_errorbar(aes(ymin=l95, ymax=u95), width=0.1)+
  geom_hline(yintercept=0,linetype="dashed")+
  facet_grid(conflictTrial ~ .)+
  theme(legend.position = "bottom")+
  scale_color_manual(values=c(cbbPalette[2], cbbPalette[1]))+
  labs(color="", y="Logit slope estimate (DV = choiceLeft)", x="p(Frac)")
```

## Probability distortion

$$V_i = (1-w(pFrac))EV_i + w(pFrac)QV_i, \:i \in \{left, right\}$$

$$w(pFrac) = \frac{\delta*pFrac^{\gamma}}{\delta*pFrac^{\gamma}+(1- pFrac)^{\gamma}}$$

How do the mean posterior estimates suggest the probability of a fractal draw is distorted for each subject?

Using Gonzalez and Wu's (1999) terminology the fact that for most subjects the probability weighting functions are below the 45-degree line (i.e. objective probability, i.e. $\delta < 1$) regardless of shape is not about diminishing sensitivity (slope) but about **attractiveness** (intercept, elevation). The fractal is almost always less attractive to subjects than it should be given its probability in determining their reward.

Note that for many subject the probability weighting function does not have the inverse S-shape observed in studies of outcome probabilities (i.e. $\gamma \geq 1$). Unlike most studies that investigate this curve where the probability refers directly to the reward, in this case the probability of a reward is the joint probability of p(Fractal)*p(chosen Fractal's reward). The probability depicted here is captures attribute weight when computing the value of a bundle.

```{r}
tmp = par_ests %>%
  group_by(subnum, par) %>%
  summarise(est = mean(value), .groups='keep') %>%
  filter(par != "beta" & par != "alpha") %>%
  spread(par, est)

tmp = do.call("rbind", replicate(11, tmp, simplify = FALSE)) %>%
  arrange(subnum) %>%
  ungroup() %>%
  mutate(pFrac = rep(seq(0, 1, .1), 25), 
         wpFrac = (delta*pFrac^gamma)/(delta*pFrac^gamma + (1-pFrac)^gamma)) 

tmp %>%
  mutate(delta_gt_1 = ifelse(delta>=1, "delta >= 1", "delta < 1 "),
         gamma_gt_1 = ifelse(gamma>=1,"gamma >= 1", "gamma < 1")) %>%
  ggplot(aes(pFrac, wpFrac, group=subnum))+
  geom_line(aes(col = as.factor(gamma_gt_1), linetype=as.factor(delta_gt_1)))+
  geom_abline(slope=1, intercept=0, linetype="dashed")+
  labs(x="p(Fractal)", y="w(pFractal)", title="Distortion of p(Fractal)", color = element_blank(), linetype=element_blank())+
  scale_color_manual(values=c(cbbPalette[3], cbbPalette[7]))+
  theme(legend.position = "bottom")
```

```{r echo=FALSE}
rm(tmp)
```

# Determinants of the choice bias

Hypothesis: is the bias due to noisy/bad estimates of the learned Q values?

Rule out it's not due to probability distortion by showing .... with parameters estimates from linear prob distortion function

```{r}
source(paste0(helpers_path,'logit_choiceLeft_randomEffects_conflictCollapsed.R'))
```

```{r eval=FALSE}
#Plot of random effects similar to the logit slopes graph above

out_choiceLeft_re %>%
  filter(var %in% c("qv_slopes", "ev_slopes")) %>%
  mutate(probFractalDraw = as.factor(probFractalDraw),
         var = ifelse(var == "ev_slopes", "EV(left)-EV(right)", "QV(left)-QV(right)")) %>%
  ggplot(aes(probFractalDraw, Estimate, color=var, shape=var))+
  geom_point()+
  # geom_errorbar(aes(ymin=Q2.5, ymax=Q97.5), width=.1)+
  geom_hline(aes(yintercept=0), linetype="dashed")+
  # facet_grid(conflictTrial~.)+
  theme(legend.position = "bottom")+
  labs(color="", shape="", x="p(Fractal)", y="Logit slope estimate (DV = choiceLeft)")+
  scale_color_manual(values=c(cbbPalette[2], cbbPalette[1]))
```

```{r eval=FALSE}
# A messier graph of the fast and slow learner difference in relying on the relevant attribute
clean_beh_data %>%
  group_by(subnum, probFractalDraw) %>%
  summarise(alpha=mean(alpha), .groups="keep") %>%
  left_join(out_choiceLeft_re %>%
              filter(var %in% c("qv_slopes", "ev_slopes")) %>%
              select(Estimate, subnum, var, probFractalDraw), 
            by=c("subnum", "probFractalDraw")) %>%
  ggplot(aes(alpha, Estimate, color=var))+
  geom_smooth(formula = 'y~x', method = "lm",fullrange=TRUE)+
  geom_point(alpha=.5, size=.5)+
  facet_grid(.~probFractalDraw)+
  labs(color="", x="Learning rate", y="Logit slope estimate (DV = choiceLeft)")+
  scale_color_manual(values=c(cbbPalette[2], cbbPalette[1]))+
  theme(legend.position="bottom")
  
```

Create learning groups by median split of learning rates.

```{r}
clean_beh_data %>%
  group_by(subnum) %>%
  summarise(alpha=unique(alpha)) %>%
  ungroup() %>%
  mutate(alpha_group = ifelse(alpha>median(alpha), "fast learner", "slow learner")) %>%
  ggplot(aes(alpha, fill=alpha_group))+
  geom_histogram(alpha=.5, position = "identity", bins = 5)+
  scale_fill_manual(values=c(cbbPalette[3], cbbPalette[7]))+
  theme(legend.position = "bottom")+
  labs(fill="", x="Learning rate")
```

When p(Frac) = 0.5 and there is no clear relevant attribute slow learners rely more on the EV difference and less on the QV difference compared to learners.  

When p(Frac) = 1 and the choice should only depend on the fractal values slow learners correctly ignore the EV difference like the learners but rely less on the QV difference compared to learners when making their decision.  

```{r}
clean_beh_data_noProbDistortion %>%
  group_by(subnum, probFractalDraw) %>%
  summarise(alpha=mean(alpha), .groups="keep") %>%
  left_join(out_choiceLeft_re %>%
              filter(var %in% c("qv_slopes", "ev_slopes")) %>%
              select(Estimate, subnum, var, probFractalDraw), 
            by=c("subnum", "probFractalDraw")) %>%
  ungroup() %>%
  mutate(alpha_group = ifelse(alpha > median(alpha), "fast learner", "slow learner")) %>%
  group_by(var, alpha_group, probFractalDraw) %>%
  summarise(mean_est = mean(Estimate),
            sem_est = sem(Estimate), .groups='keep') %>%
  ggplot(aes(as.factor(probFractalDraw), mean_est, col=alpha_group))+
    geom_point(position=position_dodge(width=0.75), size=2)+
    geom_errorbar(aes(ymin = mean_est - sem_est, ymax = mean_est+sem_est), width=0.25, position=position_dodge(width=0.75))+
  geom_hline(aes(yintercept=0), linetype="dashed")+
 facet_grid(.~var)+
  labs(color="", x="p(Frac)", y="Logit slope estimate (DV = choiceLeft)")+
  scale_color_manual(values=c(cbbPalette[3], cbbPalette[7]))+
  theme(legend.position="bottom")
```

Rule out it's not due to distortion of p(Frac): same graph using parameter estimates from a model without probability distortion

```{r}
source(paste0(helpers_path,'logit_choiceLeft_randomEffects_conflictCollapsed_noProbDistortion.R'))
```

```{r}
clean_beh_data_noProbDistortion %>%
  group_by(subnum, probFractalDraw) %>%
  summarise(alpha=mean(alpha), .groups="keep") %>%
  left_join(out_choiceLeft_re_noProbDistortion %>%
              filter(var %in% c("qv_slopes", "ev_slopes")) %>%
              select(Estimate, subnum, var, probFractalDraw), 
            by=c("subnum", "probFractalDraw")) %>%
  ungroup() %>%
  mutate(alpha_group = ifelse(alpha > median(alpha), "fast learner", "slow learner")) %>%
  group_by(var, alpha_group, probFractalDraw) %>%
  summarise(mean_est = mean(Estimate),
            sem_est = sem(Estimate), .groups='keep') %>%
  ggplot(aes(as.factor(probFractalDraw), mean_est, col=alpha_group))+
    geom_point(position=position_dodge(width=0.75), size=2)+
    geom_errorbar(aes(ymin = mean_est - sem_est, ymax = mean_est+sem_est), width=0.25, position=position_dodge(width=0.75))+
  geom_hline(aes(yintercept=0), linetype="dashed")+
 facet_grid(.~var)+
  labs(color="", x="p(Frac)", y="Logit slope estimate (DV = choiceLeft)")+
  scale_color_manual(values=c(cbbPalette[3], cbbPalette[7]))+
  theme(legend.position="bottom")
```

# Response times

```{r eval=FALSE}
# Reaction time histograms

clean_beh_data %>%
  ggplot(aes(log(reactionTime),fill=conflictTrial))+
  geom_histogram(bins=30, alpha=0.5, position="identity")+
  theme(legend.position="bottom")+
  scale_fill_manual(values=c(cbbPalette[1], cbbPalette[2]))+
  labs(x = "Log Reaction Time", fill="", y = "")+
  facet_grid(probFractalDraw~.)
  # facet_grid(.~ probFractalDraw)+
  # coord_flip()
```

```{r}
clean_beh_data %>%
  ggplot(aes(probFractalDraw, log(reactionTime), group=subnum))+
  # geom_boxplot()+
  geom_smooth(formula = y~I(x^2), method = "lm")+
  theme(legend.position="bottom")+
  labs(x = "p(Fractal Draw)", y = "Log Reaction Time")
```

Does how fast you make a decision relate to the choice?

```{r}
clean_beh_data %>%
  ggplot(aes(log(reactionTime), choiceLeft))+
  geom_smooth(formula = 'y~x', method = "glm", method.args = list(family=binomial), fullrange=TRUE)+
  facet_grid(.~probFractalDraw)
```

```{r}
m = brm(choiceLeft ~ log(reactionTime)*probFractalDraw+(1|subnum),
              data=clean_beh_data, family=bernoulli(link="logit"), silent=2, refresh=0)
```

```{r}
summary(m)
```

