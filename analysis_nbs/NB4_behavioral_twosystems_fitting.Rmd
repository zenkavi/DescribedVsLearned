---
title: "Experience vs. description based decision-making project: Two valuation systems modeling"
output: 
  html_document:
    toc: true
    toc_float: true
---

# Set up environment and load in data

```{r}
library(tidyverse)
library(rstan)
library(here)
```

```{r}
helpers_path = here('helpers/')
```

# Read in *clean* behavioral data

```{r}
# source(paste0(helpers_path,'fit_qlearning.R'))
source(paste0(helpers_path,'01_clean_behavioral_data.R'))
source(paste0(helpers_path, 'extract_var_for_stan.R'))
```

# Define two systems model

$$p(choice = left) = \frac{1}{1+e^{-\beta(V_{left} - V_{right})}}$$

$$V_i = (1-w(pFrac))EV_i + w(pFrac)QV_i, \:i \in \{left, right\}$$
$$EV_{i, t} = p_{i, t} V{i_t}$$

$$QV_{i,t} = QV_{i, t-1} + \alpha(R_{i, t-1}- QV_{i, t-1})$$

$$w(pFrac) = \frac{\delta*pFrac^{probDistortion}}{\delta*pFrac^{probDistortion}+(1- pFrac)^{probDistortion}}$$

How does the distortion change for different values of delta and probDistortion exponent? This analysis isn't based on data. It is to build intuition to understand how each prob fractal might be perceived as by subjects for different values of the model parameters.

Facets - values of pFrac (true prob of Fractal)
Blue w(pFrac)>pFrac - overweighting prob fractal
Red pFrac>w(pFrac) - underweighting prob fractal

```{r}
expand.grid(rep(list(seq(0,1.5,.1)), 3)) %>%
  rename(delta=Var1, probDistortion=Var2, pFrac=Var3)%>%
  mutate(wpFrac = (delta*(pFrac^probDistortion)) / ((delta*(pFrac^probDistortion) + ((1-pFrac)^probDistortion))), 
         distortion = pFrac-wpFrac) %>%
  filter(pFrac > 0 & pFrac < 1) %>%
  mutate(pFrac = paste0("p(Fractal) = ", pFrac)) %>%
  ggplot2::ggplot(aes(as.factor(delta), as.factor(probDistortion)))+
  geom_raster(aes(fill=distortion))+
  facet_wrap(~as.factor(pFrac))+
  labs(x="\U03B4", y="probDistortion", fill="pFrac - w(pFrac)", title="Distortion of p(Fractal)")+
  scale_fill_gradient2(low = scales::muted("blue"), high=scales::muted("red"))+
  theme(axis.ticks = element_blank(), axis.text.x = element_text(angle=90), legend.position="bottom")+
  scale_y_discrete(breaks=seq(0, 1.5, .3))
```

## Fit Q learning + two value system simultaneously

Organize data

```{r}
num_subjs = length(unique(clean_beh_data$subnum))

num_trials = clean_beh_data %>%
  count(subnum) %>%
  select(n)
num_trials = num_trials$n

#subjects in rows, trials in columns
choices = extract_var_for_stan(clean_beh_data, choiceLeft)

clean_beh_data = clean_beh_data %>%
  mutate(leftLotteryEV = lotteryValue*lotteryProb,
         rightLotteryEV = referenceValue*referenceProb)

ev_left = extract_var_for_stan(clean_beh_data, leftLotteryEV)

ev_right = extract_var_for_stan(clean_beh_data, rightLotteryEV)

fractal_outcomes_left = extract_var_for_stan(clean_beh_data, leftFractalReward)

fractal_outcomes_right = extract_var_for_stan(clean_beh_data, rightFractalReward)

trial_pFrac = extract_var_for_stan(clean_beh_data, probFractalDraw)

m_data=list(num_subjs = num_subjs,
            num_trials = num_trials,
            choices = choices,
            ev_left = ev_left,
            ev_right = ev_right,
            fractal_outcomes_left = fractal_outcomes_left,
            fractal_outcomes_right = fractal_outcomes_right,
            trial_pFrac = trial_pFrac)

rm(num_subjs, num_trials, choices, ev_left, ev_right, fractal_outcomes_left, fractal_outcomes_right, trial_pFrac)
```

## Fit model for all subjects

```{r}
if(file.exists(paste0(helpers_path, 'stanModels/fit_twoValSystemsWithRL.RDS'))){
  fit = readRDS(paste0(helpers_path, 'stanModels/fit_twoValSystemsWithRL.RDS'))
} else {
  m = stan_model('../helpers/stanModels/fit_twoValSystemsWithRL.stan')
  fit = sampling(m, data=m_data)}
```

## Organize model output

```{r}
# Extract parameters from fit object
par_ests = data.frame(extract(fit, c("alpha","probDistortion", "delta", "beta")))  %>%
  gather(key, value) %>%
  separate(key, c('par', 'subj'), sep='\\.')

# Add correct subject identifiers
par_ests = data.frame(subnum = unique(clean_beh_data$subnum)) %>%
  mutate(subj = as.character(1:n())) %>%
  right_join(par_ests, by='subj') %>%
  select(-subj)
```

Where do subjects fall in the theoretical graph above?

```{r}
tmp = par_ests %>%
  group_by(subnum, par) %>%
  summarise(est = median(value), .groups='keep') %>%
  spread(par, est)

expand.grid(rep(list(seq(0,1.5,.1)), 3)) %>%
  rename(delta=Var1, probDistortion=Var2, pFrac=Var3)%>%
  mutate(wpFrac = (delta*(pFrac^probDistortion)) / ((delta*(pFrac^probDistortion) + ((1-pFrac)^probDistortion))), 
         distortion = pFrac-wpFrac) %>%
  filter(pFrac > 0 & pFrac < 1) %>%
  mutate(pFrac = paste0("p(Fractal) = ", pFrac)) %>%
  ggplot2::ggplot(aes(as.factor(delta), as.factor(probDistortion)))+
  geom_raster(aes(fill=distortion))+
  facet_wrap(~as.factor(pFrac))+
  labs(x="\U03B4", y="probDistortion", fill="pFrac - w(pFrac)", title="Distortion of p(Fractal)")+
  scale_fill_gradient2(low = scales::muted("blue"), high=scales::muted("red"))+
  theme(axis.ticks = element_blank(), axis.text.x = element_text(angle=90), legend.position="bottom")+
  scale_y_discrete(breaks=seq(0, 1.5, .3))+
  annotate("point", x = 10*tmp$delta, y=10*tmp$probDistortion)
```
Distributions of the recovered (median) parameters

```{r}
par_ests %>%
  group_by(subnum, par) %>%
  summarise(est = median(value), .groups='keep') %>%
  ggplot(aes(est))+
  geom_histogram(alpha=.5, bins=30)+
  facet_wrap(~par, scales='free_x')+
  xlab("Median estimates for all subjects")
```

Are there any dependencies between the parameters? probDistortion is correlated with both beta and delta.

```{r}
par_ests %>%
  group_by(subnum, par) %>%
  summarise(est = median(value), .groups='keep') %>%
  ungroup() %>%
  spread(par, est) %>%
  select(-subnum) %>%
  ggpairs(progress = FALSE)
```

How do the median estimates suggest the probability of a fractal draw is distorted for each subject?

```{r}
tmp = par_ests %>%
  group_by(subnum, par) %>%
  summarise(est = median(value), .groups='keep') %>%
  filter(par != "beta" & par != "alpha") %>%
  spread(par, est)

tmp = do.call("rbind", replicate(11, tmp, simplify = FALSE)) %>%
  arrange(subnum) %>%
  ungroup() %>%
  mutate(pFrac = rep(seq(0, 1, .1), 25), 
         wpFrac = (delta*pFrac^probDistortion)/(delta*pFrac^probDistortion + (1-pFrac)^probDistortion)) 

tmp %>%
  mutate(delta_gt_1 = ifelse(delta>=1, 1, 0),
         probDistortion_gt_1 = ifelse(probDistortion>=1,"prodDistortion >= 1", "prodDistortion < 0")) %>%
  ggplot(aes(pFrac, wpFrac, group=subnum))+
  geom_line(aes(col = as.factor(probDistortion_gt_1)))+
  geom_abline(slope=1, intercept=0, linetype="dashed")+
  labs(x="p(Fractal)", y="w(pFractal)", title="Distortion of p(Fractal)", color = element_blank())
```

# Separate vs simulatanous estimation

How do the parameter estimates from the RL only fit differ from the alpha and betas in the simulatanous fit?

```{r}
fit_rl = readRDS(paste0(helpers_path, 'stanModels/fit_qlearning.RDS'))

par_ests_rl = data.frame(extract(fit_rl, c("alphas", "betas")))  %>%
  gather(key, value) %>%
  separate(key, c('par', 'subj'), sep='\\.')

# Add correct subject identifiers
par_ests_rl = data.frame(subnum = unique(clean_beh_data$subnum)) %>%
  mutate(subj = as.character(1:n())) %>%
  right_join(par_ests_rl, by='subj') %>%
  select(-subj)
```

The learning rates don't have a clear pattern of difference, although they are mostly not the same but the inverse temperatures are systematically higher when estimates

Higher inverse temperatures should suggest that the choice depends more on the value difference than chance.

```{r}
par_ests %>%
  group_by(subnum, par) %>%
  summarise(est = median(value), .groups='keep') %>%
  filter(par %in% c("beta", "alpha")) %>%
  mutate(fit = "simulatanous") %>%
  rbind(par_ests_rl %>%
          group_by(subnum, par) %>%
          summarise(est = median(value), .groups='keep') %>%
          filter(par %in% c("betas", "alphas")) %>%
          mutate(fit = "separate",
                 par = ifelse(par == "alphas", "alpha", "beta"))) %>%
  spread(fit, est) %>%
  ggplot(aes(separate, simulatanous))+
  geom_point()+
  facet_wrap(~par, scales="free")+
  geom_abline(intercept=0, slope=1, linetype="dashed")

```

With these RL estimates do you get the same pattern for the Q values? Yes.

```{r}
clean_beh_data = par_ests %>%
  group_by(subnum, par) %>%
  summarise(est = median(value), .groups='keep') %>%
  spread(par, est) %>%
  left_join(clean_beh_data, by='subnum')

## Add Q values to each trial

get_qvals = function(subj_data){
  subj_data$leftQValue = 0
  subj_data$rightQValue = 0
  for (i in 2:nrow(subj_data)){
    subj_data$leftQValue[i] = subj_data$alpha[i] * (subj_data$leftFractalReward[i-1] - subj_data$leftQValue[i-1])
    subj_data$rightQValue[i] = subj_data$alpha[i] * (subj_data$rightFractalReward[i-1] - subj_data$rightQValue[i-1])
  }
  return(subj_data)
}

clean_beh_data = clean_beh_data %>%
  group_by(subnum) %>%
  do(get_qvals(.)) %>%
  ungroup()

rm(get_qvals)

```

```{r}
clean_beh_data %>%
  filter(probFractalDraw == 1 | probFractalDraw == 0) %>%
  mutate(leftQVAdv = leftQValue - rightQValue,
         leftLotteryEV = lotteryValue*lotteryProb,
         rightLotteryEV = referenceValue*referenceProb,
         leftEVAdv = leftLotteryEV - rightLotteryEV) %>%
  select(leftQVAdv, leftEVAdv, choiceLeft, probFractalDraw) %>%
  gather(key, value, -choiceLeft, -probFractalDraw) %>%  
  mutate(key = ifelse(key == "leftEVAdv", "EV_left - EV_right", "QV_left - QV_right"),
         probFractalDraw = ifelse(probFractalDraw == 0, "p(Fractal)=0", "p(Fractal)=1"))%>%
  ggplot(aes(value, choiceLeft))+
  geom_jitter(width=0.03, height=0.08, alpha = 0.05)+
  geom_smooth(formula = 'y~x', method = "glm", method.args = list(family=binomial), fullrange=TRUE)+
  facet_grid(probFractalDraw~key)+
  xlab("")+
  ylab("p(Left)")
```

How about model "fit". How do you evaluate model fit? 

```{r}

```
