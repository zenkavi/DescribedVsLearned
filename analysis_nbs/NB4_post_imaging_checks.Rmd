---
title: "Experience vs. description based decision-making project: Post imaging analyses checks"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: 'hide'
---

Set up environment and load in data

```{r include=FALSE}
library(tidyverse)
library(gridExtra)
library(brms)
library(here)
cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
# theme_set(theme_bw())
theme_set(theme_classic())
sem <- function(x) {sd(x, na.rm=T) / sqrt(length(x))}
```

```{r include=FALSE}
helpers_path = here('helpers/')
source(paste0(helpers_path,'fit_twoValSystemsWithRL_hierarchical.R'))
source(paste0(helpers_path,'add_inferred_pars.R'))
```

Add estimated model parameters and inferred bundle values using those parameters.

```{r}
clean_beh_data = add_inferred_pars(clean_beh_data, par_ests)

clean_beh_data = clean_beh_data %>%
  mutate(wpFracBins = as.factor(round(wpFrac,1)),
         leftQVAdvWeighted = leftQVAdv*wpFrac,
         leftEVAdvWeighted = leftEVAdv*(1-wpFrac)) 
```

## Correlation between reggressors

### Value

Is there a correlation between valChosen and valUnchosen? Much smaller than before with the wrong Q-values.

```{r}
with(clean_beh_data, cor(valChosen, valUnchosen))
```
    
```{r}
clean_beh_data %>%
  select(leftFractalRpe, rightFractalRpe) %>%
  gather(key, value) %>%
  group_by(key) %>%
  mutate(value = scale(value, scale=F)) %>%
  ggplot(aes(value, fill=key))+
  geom_histogram(position = "identity", bins = 30, alpha=.5)
```
   
```{r}
clean_beh_data %>%
  group_by(subnum, session) %>%
  mutate(rpeLeftRightSum = scale(rpeLeftRightSum, scale=F)) %>%
  ggplot(aes(rpeLeftRightSum))+
  geom_histogram(bins=30, alpha=.5, position="identity")+
  facet_grid(subnum~session)
```   

Is RPE sum a good regressor to capture learning? 

For the off diagonal trials in the plot below rpe sum would be small but is there less learning in those trials? Probably not.
    
```{r}
clean_beh_data %>%
  ggplot(aes(leftFractalRpe, rightFractalRpe))+
  geom_point()+
  geom_abline(aes(slope=1, intercept=0), linetype="dashed")
```

Note that choices above the diagonal where the value of the unchosen bundle is higher than the value of the chosen bundle must be trials where subjects chose differently than what the value difference suggested.

```{r}
clean_beh_data %>%
  ggplot(aes(valChosen, valUnchosen, col=wpFrac))+
  geom_point()+
  geom_abline(aes(intercept=0, slope = 1), linetype="dashed") + 
  theme(legend.position="bottom")
```
valChosen and valUnchosen is the left and right bundle values rearranged by choice. What does the relationship between the bundle values look like regardless of choice? The correlation is even lower between the bundle values.

```{r}
with(clean_beh_data, cor(leftBundleVal, rightBundleVal))
```

```{r}
clean_beh_data %>%
  ggplot(aes(leftBundleVal, rightBundleVal, col=wpFrac))+
  geom_point()+
  geom_abline(aes(intercept=0, slope = 1), linetype="dashed") + 
  # facet_grid(.~as.factor(leftEVAdv))+
  theme(legend.position = "bottom")
```
Is this true same for all subjects? No.

```{r}
clean_beh_data %>%
  group_by(subnum) %>%
  mutate(bundleValCor = cor(rightBundleVal, leftBundleVal),
         lab_col = paste0(subnum, ", r = ",round(bundleValCor, 3))) %>%
  ggplot(aes(leftBundleVal, rightBundleVal, color = alpha))+
  geom_point()+
  geom_abline(aes(intercept=0, slope = 1), linetype="dashed")+
  facet_wrap(~lab_col)+
  theme(legend.position="bottom")
```

How much the value of the bundles correlate with each other is associated with learning. The higher the learning rate the less the values of the bundles correlate with each other.

```{r}
clean_beh_data %>%
  group_by(subnum) %>%
  summarise(val_cor = cor(leftBundleVal, rightBundleVal),
            alpha = unique(alpha)) %>%
  ggplot(aes(alpha, val_cor))+
  geom_point()+
  geom_smooth(method="lm", formula='y~x')+
  labs(y="Bundle value correlation", x="Learning rate")
```
Why does the bundle value correlations higher for low learning rates?

Because Q values have little variance for small learning rates.

```{r}
clean_beh_data %>%
  mutate(lab_col = paste0(subnum, ", alpha = ",round(alpha, 3))) %>%
  ggplot(aes(leftQValue*wpFrac, rightQValue*wpFrac, color=alpha))+
  geom_point()+
  facet_wrap(~lab_col)+
  geom_abline(aes(intercept=0, slope = 1), linetype="dashed")+
  theme(legend.position = "none")
```
So the relationship between bundle values is determined more by the weighted lottery EVs, which are correlated by design. 

The right bundle value is affected more strongly than the left bundle value because the right lottery is fixed.

```{r}
clean_beh_data %>%
  mutate(lab_col = paste0(subnum, ", alpha = ",round(alpha, 3))) %>%
  ggplot(aes(leftLotteryEV*(1-wpFrac), rightLotteryEV*(1-wpFrac), col=alpha))+
  geom_point()+
  geom_abline(aes(intercept=0, slope = 1), linetype="dashed")+
  facet_wrap(~lab_col)+
  theme(legend.position="none")
```
Are lottery EVs and fractal QVs on similar scales? Yes.

```{r}
clean_beh_data %>%
  select(leftLotteryEV, leftQValue, rightQValue) %>%
  gather(key, value) %>%
  ggplot(aes(value, fill=key))+
  geom_histogram(position = "identity", alpha = 0.5, bins=30)+
  scale_fill_manual(values=cbbPalette[1:3])+
  labs(fill="")
```
```{r eval=FALSE}
#Design

clean_beh_data %>%
  select(leftLotteryEV, fractalLeftProb, fractalRightProb) %>%
  gather(key, value) %>%
  ggplot(aes(value, fill=key))+
  geom_histogram(position = "identity", alpha = 0.5, bins=30)+
  scale_fill_manual(values=cbbPalette[3:1])+
  labs(fill="")
```

The Q value distribution is not the same for all subjects. As expected from the above exploration they are more variable for better learners.

What about the separation between the right and left Q Values for some subjects?

```{r}
clean_beh_data %>%
  mutate(lab_col = paste0(subnum, ", alpha = ",round(alpha, 3))) %>%
  select(lab_col, leftQValue, rightQValue) %>%
  gather(key, value, -lab_col) %>%
  ggplot(aes(value, fill=key))+
  geom_histogram(position = "identity", alpha = 0.5, bins=30)+
  scale_fill_manual(values=cbbPalette[1:2])+
  labs(fill="", x="", y = "")+
  facet_wrap(~lab_col)+
  theme(legend.position = "bottom")
```

Trial by trial Q value for a good learner

```{r}
clean_beh_data %>%
  filter(subnum == "19") %>%
  mutate(trialNumCont = 1:n()) %>%
  select(leftQValue, fractalLeftProb, rightQValue, fractalRightProb, trialNumCont) %>%
  gather(key, value, -trialNumCont) %>%
  mutate(side = ifelse(grepl("eft", key), "left", "right"),
         learned = ifelse(grepl("fractal", key), "true", "learned")) %>%
  ggplot(aes(trialNumCont, value, color=learned))+
  geom_point()+
  geom_line()+
  facet_grid(side~.)+
  labs(color="", title="High alpha (subnum 19)")+
  scale_color_manual(values=cbbPalette[5:6])+
  theme(legend.position="bottom")
```

```{r}
clean_beh_data %>%
  filter(subnum == "03") %>%
  mutate(trialNumCont = 1:n()) %>%
  select(leftQValue, fractalLeftProb, rightQValue, fractalRightProb, trialNumCont) %>%
  gather(key, value, -trialNumCont) %>%
  mutate(side = ifelse(grepl("eft", key), "left", "right"),
         learned = ifelse(grepl("fractal", key), "true", "learned")) %>%
  ggplot(aes(trialNumCont, value, color=learned))+
  geom_point()+
  geom_line()+
  facet_grid(side~.)+
  labs(color="", title="subnum 03")+
  scale_color_manual(values=cbbPalette[5:6])+
  theme(legend.position="bottom")

```

### Reward and rpe

How is rpe computed? reward - valChosen. So if valChosen is small rpe ~ reward.And valChosen is always pretty small!

```{r}
clean_beh_data %>%
  mutate(lab_col = paste0("valChosen Bin = ", round(valChosen, 1))) %>%
  ggplot(aes(reward, rpe))+
  geom_point()+
  geom_abline(aes(slope=1, intercept=0), linetype="dashed")+
  facet_wrap(~lab_col)
```

```{r}
clean_beh_data %>%
  filter(reward < 1.1) %>%
  ggplot(aes(reward, rpe, col=valChosen))+
  geom_point()+
  geom_abline(aes(slope=1, intercept=0), linetype="dashed")
```

Why is valChosen so small even for trials where the lottery reward is large?

valChosen has a comprable distribution to the lottery EVs. 

Lotteries that end with large reward have very low probabilities so low EVs.

Each fractal's RPE is on a comparable scale with EVs/QVs.

So how should you operationalize a "trial RPE", the surprise after a large reward from a lottery?
What behavioral effect might such a surprise have? Make you more likely to choose based on the lottery regardless of how likely it is?

```{r}
names(clean_beh_data)

clean_beh_data %>%
  filter(fractalDraw == 0) %>%
  group_by(reward, probFractalDraw) %>%
  tally() %>%
  filter(reward > 1) %>%
  mutate(probLotteryDraw = 1-probFractalDraw) %>%
  select(-probFractalDraw) %>%
  arrange(probLotteryDraw)
```

When is a large reward "surprising"?

```{r}
clean_beh_data %>%
  filter(fractalDraw ==0 & reward > 1 & probFractalDraw != 0) %>%
  mutate(probLotteryDraw = 1-probFractalDraw) %>%
  select(probLotteryDraw, lotteryProb, reward, conflictTrial, leftLotteryEV, leftQValue, rightLotteryEV, rightQValue) %>%
  arrange(-probLotteryDraw) 
```

```{r}
clean_beh_data %>%
  filter(fractalDraw == 0) %>%
  group_by(reward) %>%
  tally()
```

## Where is QV in the brain?

Are we not finding a neural correlate for fractal value regressors because they are more variable across subjects?

```{r}
source(paste0(helpers_path,'logit_choiceLeft_randomEffects_conflictCollapsed.R'))
source(paste0(helpers_path,'logit_choiceLeft_randomEffects_conflictCollapsed_pFracWeightedValDiff.R'))
```

```{r}
out_choiceLeft_re_pfw %>%
  filter(var != "intercept") %>%
  pivot_wider(values_from = c("Estimate", "Q2.5", "Q97.5"),
              id_cols = c(subnum, probFractalDraw),
              names_from = var) %>%
  ggplot(aes(Estimate_qv_slopes, Estimate_ev_slopes))+
  geom_point()+
  geom_errorbar(aes(ymin=Q2.5_ev_slopes, ymax=Q97.5_ev_slopes), alpha=.5, color=cbbPalette[2])+
  geom_errorbarh(aes(xmin=Q2.5_qv_slopes, xmax=Q97.5_qv_slopes), alpha=.5, color=cbbPalette[1])+
  facet_wrap(~probFractalDraw)+
  labs(x="Logit slope for QV(left)-QV(right)", y="Logit slope for EV(left)-EV(right)")+
  theme_bw()
```

```{r}
out_choiceLeft_re %>%
  filter(var != "intercept") %>%
  pivot_wider(values_from = c("Estimate", "Q2.5", "Q97.5"),
              id_cols = c(subnum, probFractalDraw),
              names_from = var) %>%
  ggplot(aes(Estimate_qv_slopes, Estimate_ev_slopes))+
  geom_point()+
  geom_errorbar(aes(ymin=Q2.5_ev_slopes, ymax=Q97.5_ev_slopes), alpha=.5, color=cbbPalette[2])+
  geom_errorbarh(aes(xmin=Q2.5_qv_slopes, xmax=Q97.5_qv_slopes), alpha=.5, color=cbbPalette[1])+
  facet_wrap(~probFractalDraw)+
  labs(x="Logit slope for QV(left)-QV(right)", y="Logit slope for EV(left)-EV(right)")+
  theme_bw()
```

Compare width of errorbars more directly?
You're trying to capture where there is more individual variability.
Not where an individual's estimates might be more variable.
So what you should look at is the variance of estimates for each slope at each pfractal level.

```{r}
out_choiceLeft_re_pfw %>%
  filter(var != "intercept") %>%
  ggplot(aes(as.factor(probFractalDraw), Estimate, color=var))+
  geom_boxplot()
```

```{r}
out_choiceLeft_re %>%
  filter(var != "intercept") %>%
  ggplot(aes(as.factor(probFractalDraw), Estimate, color=var))+
  geom_boxplot()
```