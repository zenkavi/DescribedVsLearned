---
title: "Experience vs. description based decision-making project: DDM fitting to subject data"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: 'hide'
---

# Setup

Set up environment and load in data

```{r include=FALSE}
library(tidyverse)
library(here)
cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
theme_set(theme_classic())
sem <- function(x) {sd(x, na.rm=T) / sqrt(length(x))}
helpers_path = here('helpers/')

set.seed(38573)
```

Adding in parameters from the two systems model for all subjects (i.e. not choosing the best fitting one per subject)

```{r}
 # do this first not to mess with cluster setup
source(paste0(helpers_path,'ddmSims/fit_task.R'))
source(paste0(helpers_path,'ddmSims/sim_task.R'))

source(paste0(helpers_path,'ddmSims/sim_sanity_checks.R'))
source(paste0(helpers_path,'twoSystemsFitting/fit_twoValSystemsWithRL_hierarchical.R'))
source(paste0(helpers_path,'add_inferred_pars.R'))
clean_beh_data = add_inferred_pars(clean_beh_data, par_ests, model_name="original")

rm(fit, g_par_ests, par_ests, get_qvals, organize_stan_output, add_inferred_pars, extract_var_for_stan)
```

Create empty list that will store the trial simulators for the forthcoming models.

```{r}
sim_trial_list = list()
fit_trial_list = list()
```


# Testing parameter recovery

## Model 2b: Asymmetric prob distortion

- Distort probFractalDraw when integrating info about fractals but no distortion for lotteries
- Intended to capture the stepwise nature of the logit slopes for the QV difference but the linear nature for the EV difference

```{r}
source(paste0(helpers_path, 'ddmSims/ddm_model2b.R'))
sim_trial_list[['model2b']] = sim_trial
fit_trial_list[['model2b']] = fit_trial
```

## Test single trial

Make sure trial simulator works. Use the mean RT from trials simulated using this model (2b) as the input for fitting in the next step. 

```{r}
100 %>% 
  rerun() %>%
  map_df(~data.frame(sim_trial(d=.04, sigma=0.01, barrierDecay=0, delta=3, gamma=3, EVLeft = .7, EVRight = .2, QVLeft = .7, QVRight = .2, probFractalDraw = .4))) %>%
  summarise(meanRT = mean(reactionTime),
            numLeft = sum(choice == "left"))

```

Are likelihoods higher on average for a parameter combination that is far from the true parameters? No. Not sure why.

```{r}
tmp1 = 100 %>% 
  rerun() %>%
  map_df(~data.frame(fit_trial(d=.04, sigma=0.01, barrierDecay=0, delta=3, gamma=3, EVLeft = .7, EVRight = .2, QVLeft = .7, QVRight = .2, probFractalDraw = .4, choice = "left", reactionTime = .7))) %>%
  mutate(par_type = "true")
```

```{r}
tmp2 = 100 %>% 
  rerun() %>%
  map_df(~data.frame(fit_trial(d=.002, sigma=0.1, barrierDecay=0, delta=3, gamma=3, EVLeft = .7, EVRight = .2, QVLeft = .7, QVRight = .2, probFractalDraw = .4, choice = "left", reactionTime = .7))) %>%
  mutate(par_type = "false")
```

**Why is the likelihood for the false parameters consistently (for all 100 iterations) higher than the likelihood for the true parameters?**

```{r}
rbind(tmp1, tmp2) %>%
  ggplot(aes(par_type, log(likelihood))) +
  geom_boxplot()
```

```{r}
rm(tmp1, tmp2)
```

## Test all trials in a task

Can the likelihood function correctly identify the parameter combination that generated the data? Is the sum of likelihoods for all trials highest for the correct combination than any other combination?

Filter one subject's data.

```{r}
sub_data = clean_beh_data %>%
  filter(subnum  %in% c("19")) %>%
  select(leftQValue, rightQValue, leftLotteryEV, rightLotteryEV, probFractalDraw, reactionTime, choiceLeft, subnum) %>%
  rename(EVLeft = leftLotteryEV, EVRight = rightLotteryEV, QVLeft = leftQValue, QVRight = rightQValue)
```

Use the trials from that subject's data to simulate data (not fitting anything to real data yet to see if recovery with known true parameters is possible)

```{r}
true_dat = sim_task(sub_data, model_name = "model2b", d=0.005, sigma = 0.007, delta = 3, gamma = 3, barrierDecay = 0.004, epsilon = 0.03)
```

Do these paramters generate reasonable data?

```{r}
sim_sanity_checks(true_dat, compare_logits = TRUE)
```

Fit using three parameter combinations.

```{r}
pars = list(d=0.02, sigma = 0.007, delta = 3, gamma = 3, barrierDecay = 0.004, epsilon = 0.03)
fit1_data = fit_task(true_dat, model_name = "model2b", pars_ = pars)
```

```{r}
pars = list(d=0.05, sigma = 0.01)
fit2_data = fit_task(true_dat, model_name = "model2b", pars_ = pars)
```

```{r}
pars = list(d=0.03, sigma = 0.01)
fit3_data = fit_task(true_dat, model_name = "model2b", pars_ = pars)
```

Are the sum of likelihoods higher using the true parameter combination compared to false combinations? Yes.

```{r}
sum(fit1_data$likelihood) > sum(fit2_data$likelihood)
```

```{r}
sum(fit1_data$likelihood) > sum(fit3_data$likelihood)
```

```{r}
fit1_data %>%
  mutate(par_type = "true") %>%
  rbind(fit2_data %>% mutate(par_type = "false_1")) %>%
  rbind(fit3_data %>% mutate(par_type = "false_2")) %>%
  group_by(par_type) %>%
  summarise(neg_log_lik = -sum(log(likelihood+1e-200))) %>%
  ggplot(aes(par_type, neg_log_lik))+
  geom_bar(stat="identity", width=.5)+
  geom_hline(aes(yintercept = min(neg_log_lik)), linetype="dashed")

```

# Parameter recovery

```{r}
true_dat = sim_task(sub_data, model_name = "model2b", d=0.02, sigma = 0.007)
```

Is the task neg log likelihood **smaller** for the true parameters compared to wrong ones? **No! So identifiability is problematic?**

```{r}
get_task_nll(data=true_dat, par=c(.02, .007), par_names=c("d", "sigma"), model_name="model2b")
```

```{r}
get_task_nll(data=true_dat, par=c(.0115, .0115), par_names=c("d", "sigma"), model_name="model2b")
```

What do the trial likelihoods look like with true data generating and false parameters? Is there any variability? Are they all 0? Do these parameters generate sensible data, e.g. for a trial with large EV/QV differences? 

```{r}
pars = list(d=0.02, sigma = 0.007)
fit1_data = fit_task(true_dat, model_name = "model2b", pars_ = pars)
unique(fit1_data$likelihood)
```

```{r}
pars = list(d=0.0115, sigma = 0.0115)
fit2_data = fit_task(true_dat, model_name = "model2b", pars_ = pars)
unique(fit2_data$likelihood)
```

```{r}
true_dat
```

```{r}
sim_sanity_checks(true_dat)
```

## Built-in optimizer

This does work (checked with fewer iterations) but very slowly. **But I currently don't know if it gets to estimates that are any good.**

Still, I could get subject estimates running this with an appropriate number of iterations on a cluster. Note that locally I did this with 7 cores per subject so would need to make sure there are sufficient cores in the AWS instance type. **How long would this take per subject on a single AWS instance with sufficient resources (make sure to try it with data you know the true parameters for)?**

But especially with this number of subjects and given that the research question is not one of individual differences the more important question here will be to do a model comparison. **What do I need for the model comparison?**

Note also that in simulations timeouts are possible. The decisions are not entirely random for these trials but **a large number of timeout trials could influence the optimizers ability to recover parameters.**

```{r}
true_dat = sim_task(sub_data, model_name = "model2b", d=0.02, sigma = 0.007)
sum(true_dat$timeOut)/nrow(true_dat)
```

```{r}
optim_out = optim(c(.01, .01), get_task_nll, data=true_dat, par_names = c("d", "sigma"), model_name="model2b", control = list(maxit=25))
```

Convergence 1 means max it was reached.

```{r}
optim_out
```

```{r}
pars = list(d=0.02, sigma = 0.007)
fit1_data = fit_task(true_dat, model_name = "model2b", pars_ = pars)
```

```{r}
unique(fit1_data$likelihood)
```

```{r}
fit1_data %>%
  ggplot(aes(-log(likelihood+1e-20))) +
  geom_histogram()
```