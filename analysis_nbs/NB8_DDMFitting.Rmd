---
title: "Experience vs. description based decision-making project: DDM fitting to subject data"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: 'hide'
---

# Setup

Set up environment and load in data

```{r include=FALSE}
library(tidyverse)
library(here)
cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
theme_set(theme_classic())
sem <- function(x) {sd(x, na.rm=T) / sqrt(length(x))}
helpers_path = here('helpers/')

set.seed(38573)
```

Adding in parameters from the two systems model for all subjects (i.e. not choosing the best fitting one per subject)

```{r}
 # do this first not to mess with cluster setup
source(paste0(helpers_path,'ddmSims/fit_task.R'))
source(paste0(helpers_path,'ddmSims/sim_task.R'))

source(paste0(helpers_path,'ddmSims/sim_sanity_checks.R'))
source(paste0(helpers_path,'twoSystemsFitting/fit_twoValSystemsWithRL_hierarchical.R'))
source(paste0(helpers_path,'add_inferred_pars.R'))
clean_beh_data = add_inferred_pars(clean_beh_data, par_ests, model_name="original")

rm(fit, g_par_ests, par_ests, get_qvals, organize_stan_output, add_inferred_pars, extract_var_for_stan)
```

Create empty list that will store the trial simulators for the forthcoming models.

```{r}
sim_trial_list = list()
fit_trial_list = list()
```

# Testing parameter recovery

## Model 1: Simplest

I had started with model 2b but was confused about how the prob of states was being calculated in each step 

I'm wondering if there is a problem with implementing the integration of the sampled value when computing the probability of being in the next state

Going back to the simpler model.

```{r}
source(paste0(helpers_path, 'ddmSims/ddm_model1.R'))
```

### Single trial 

Compare likelihoods for four trials generated with different `d` and `sigma` combinations.

```{r}
100 %>% 
  rerun() %>%
  map_df(~data.frame(sim_trial(d = .04, sigma = .02, EVLeft = .7, EVRight = .2, QVLeft = .7, QVRight = .2, probFractalDraw = .3))) %>%
  summarise(meanRT = mean(reactionTime),
            numLeft = sum(choice == "left"))
```

```{r}
100 %>% 
  rerun() %>%
  map_df(~data.frame(sim_trial(d = .02, sigma = .02, EVLeft = .7, EVRight = .2, QVLeft = .7, QVRight = .2, probFractalDraw = .3))) %>%
  summarise(meanRT = mean(reactionTime),
            numLeft = sum(choice == "left"))
```

```{r}
100 %>% 
  rerun() %>%
  map_df(~data.frame(sim_trial(d = .02, sigma = .04, EVLeft = .7, EVRight = .2, QVLeft = .7, QVRight = .2, probFractalDraw = .3))) %>%
  summarise(meanRT = mean(reactionTime),
            numLeft = sum(choice == "left"))
```

```{r}
100 %>% 
  rerun() %>%
  map_df(~data.frame(sim_trial(d = .04, sigma = .04, EVLeft = .7, EVRight = .2, QVLeft = .7, QVRight = .2, probFractalDraw = .3))) %>%
  summarise(meanRT = mean(reactionTime),
            numLeft = sum(choice == "left"))
```

Is the likelihood higher for the correct parameter combination?

Trial1 & Trial 4

```{r}
fit_trial(d = .04, sigma = .02, EVLeft = .7, EVRight = .2, QVLeft = .7, QVRight = .2, probFractalDraw = .3, choice="left", reactionTime = .52)
```

```{r}
fit_trial(d = .02, sigma = .02, EVLeft = .7, EVRight = .2, QVLeft = .7, QVRight = .2, probFractalDraw = .3, choice="left", reactionTime = .52)
```

```{r}
fit_trial(d = .02, sigma = .04, EVLeft = .7, EVRight = .2, QVLeft = .7, QVRight = .2, probFractalDraw = .3, choice="left", reactionTime = .52)
```

```{r}
fit_trial(d = .04, sigma = .04, EVLeft = .7, EVRight = .2, QVLeft = .7, QVRight = .2, probFractalDraw = .3, choice="left", reactionTime = .52)
```

Trial2 & Trial 3

```{r}
fit_trial(d = .04, sigma = .02, EVLeft = .7, EVRight = .2, QVLeft = .7, QVRight = .2, probFractalDraw = .3, choice="left", reactionTime = 1.03)
```

```{r}
fit_trial(d = .02, sigma = .02, EVLeft = .7, EVRight = .2, QVLeft = .7, QVRight = .2, probFractalDraw = .3, choice="left", reactionTime = 1.03)
```

```{r}
fit_trial(d = .02, sigma = .04, EVLeft = .7, EVRight = .2, QVLeft = .7, QVRight = .2, probFractalDraw = .3, choice="left", reactionTime = 1.03)
```

```{r}
fit_trial(d = .04, sigma = .04, EVLeft = .7, EVRight = .2, QVLeft = .7, QVRight = .2, probFractalDraw = .3, choice="left", reactionTime = 1.03)
```
**For all trial types (generated by different parameter combinations) the likelihood is always higher for larger sigma's and larger d's with sigma having a larger effect.**

### Whole task

Is this true at whole task level true?

```{r}
sim_trial_list[['model1']] = sim_trial
fit_trial_list[['model1']] = fit_trial
```

Filter one subject's data to get set of stimuli.

```{r}
sub_data = clean_beh_data %>%
  filter(subnum  %in% c("19")) %>%
  select(leftQValue, rightQValue, leftLotteryEV, rightLotteryEV, probFractalDraw, reactionTime, choiceLeft, subnum) %>%
  rename(EVLeft = leftLotteryEV, EVRight = rightLotteryEV, QVLeft = leftQValue, QVRight = rightQValue)
```

Use the trials from that subject's data to simulate data (not fitting anything to real data yet to see if recovery with known true parameters is possible)

```{r}
stim1 = sim_task(sub_data, model_name = "model1", d = .04, sigma = .02) %>%drop_na()
stim2 = sim_task(sub_data, model_name = "model1", d = .02, sigma = .02) %>%drop_na()
stim3 = sim_task(sub_data, model_name = "model1", d = .02, sigma = .04) %>%drop_na()
stim4 = sim_task(sub_data, model_name = "model1", d = .04, sigma = .04) %>%drop_na()
```

Do these paramters generate reasonable data?

```{r}
sim_sanity_checks(stim4, compare_logits = TRUE)
```

```{r}
stim1
```

Fit using the parameter combinations.

```{r}
stim1_fit1 = fit_task(stim1, model_name = "model1", pars_ = list(d=.04, sigma = .02))
stim1_fit2 = fit_task(stim1, model_name = "model1", pars_ = list(d=.02, sigma = .02))
stim1_fit3 = fit_task(stim1, model_name = "model1", pars_ = list(d=.02, sigma = .04))
stim1_fit4 = fit_task(stim1, model_name = "model1", pars_ = list(d=.04, sigma = .04))
```

What has the smallest neg log likelihood?

```{r}
sum(-log(stim1_fit1$likelihood+1e-200))
sum(-log(stim1_fit2$likelihood+1e-200))
sum(-log(stim1_fit3$likelihood+1e-200))
sum(-log(stim1_fit4$likelihood+1e-200))
```


## Drivers of the tradeoff

What in `fit_trial` is different for these two parameter combinations?

`changeMatrix`, `changeUp`, `changeDown` are the same for both. 

What's different is the mu and sigma when calculating `dnorm(changeMatrix, mu, sigma)` and `pnorm(changeUp[,nextTime], mu, sigma)` which is then reflected in `prStates`, `probUpCrossing` and `probDownCrossing`

```{r}
fit_trial(d = .04, sigma = .02, EVLeft = .7, EVRight = .2, QVLeft = .7, QVRight = .2, probFractalDraw = .3, choice="left", reactionTime = .6, debug=TRUE)
```

```{r}
fit_trial(d = .02, sigma = .02, EVLeft = .7, EVRight = .2, QVLeft = .7, QVRight = .2, probFractalDraw = .3, choice="left", reactionTime = .6, debug=TRUE)
```

```{r}
fit_trial(d = .02, sigma = .04, EVLeft = .7, EVRight = .2, QVLeft = .7, QVRight = .2, probFractalDraw = .3, choice="left", reactionTime = .6, debug=TRUE)
```

`prStates` for the starting position declines a lot faster for the smaller mu.

How do `dnorm(changeMatrix, mu, sigma)` and `pnorm(changeUp[,nextTime], mu, sigma)` change depending on mu and sigma?

```{r}
dnorm(changeMatrix[,11], .02, .02)
```

```{r}
dnorm(changeMatrix[,11], .01, .04)
```

Is the spread of the prob change faster bc of sigma?

```{r}
dnorm(changeMatrix[,11], .01, .02)
```

What do these normal distributions look like?

```{r}
data.frame(vals=rnorm(1000, .02, .02)) %>%
  mutate(dist = 'mu=.02, sigma=.02') %>%
  rbind(data.frame(vals=rnorm(1000, .01, .04)) %>%
  mutate(dist = 'mu=.01, sigma=.04')) %>%
  ggplot(aes(vals, fill=dist))+
  geom_histogram(position="identity", alpha=.5, bins=30)+
  geom_vline(aes(xintercept = .02))+
  geom_vline(aes(xintercept = .01))+
  labs(fill="")
```




## Built-in optimizer

This does work (checked with fewer iterations) but very slowly. **But I currently don't know if it gets to estimates that are any good.**

Still, I could get subject estimates running this with an appropriate number of iterations on a cluster. Note that locally I did this with 7 cores per subject so would need to make sure there are sufficient cores in the AWS instance type. **How long would this take per subject on a single AWS instance with sufficient resources (make sure to try it with data you know the true parameters for)?**

But especially with this number of subjects and given that the research question is not one of individual differences the more important question here will be to do a model comparison. **What do I need for the model comparison?**

Note also that in simulations timeouts are possible. The decisions are not entirely random for these trials but **a large number of timeout trials could influence the optimizers ability to recover parameters.**

```{r}
true_dat = sim_task(sub_data, model_name = "model2b", d=0.02, sigma = 0.007)
sum(true_dat$timeOut)/nrow(true_dat)
```

```{r}
optim_out = optim(c(.01, .01), get_task_nll, data=true_dat, par_names = c("d", "sigma"), model_name="model2b", control = list(maxit=25))
```

Convergence 1 means max it was reached.

```{r}
optim_out
```

```{r}
pars = list(d=0.02, sigma = 0.007)
fit1_data = fit_task(true_dat, model_name = "model2b", pars_ = pars)
```

```{r}
unique(fit1_data$likelihood)
```

```{r}
fit1_data %>%
  ggplot(aes(-log(likelihood+1e-20))) +
  geom_histogram()
```

# Stop clusters

```{r}
parallel::stopCluster(cl = my.fit.cluster)
parallel::stopCluster(cl = my.sim.cluster)
```