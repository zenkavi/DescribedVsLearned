---
title: "Experience vs. description based decision-making project: DDM fitting to subject data"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: 'hide'
---

# Setup

Set up environment and load in data

```{r include=FALSE}
library(tidyverse)
library(here)
cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
theme_set(theme_classic())
sem <- function(x) {sd(x, na.rm=T) / sqrt(length(x))}
helpers_path = here('helpers/')

set.seed(38573)
```

Adding in parameters from the two systems model for all subjects (i.e. not choosing the best fitting one per subject)

```{r}
 # do this first not to mess with cluster setup
source(paste0(helpers_path,'ddmSims/fit_task.R'))
source(paste0(helpers_path,'ddmSims/sim_task.R'))

source(paste0(helpers_path,'ddmSims/sim_sanity_checks.R'))
source(paste0(helpers_path,'twoSystemsFitting/fit_twoValSystemsWithRL_hierarchical.R'))
source(paste0(helpers_path,'add_inferred_pars.R'))
clean_beh_data = add_inferred_pars(clean_beh_data, par_ests, model_name="original")

rm(fit, g_par_ests, par_ests, get_qvals, organize_stan_output, add_inferred_pars, extract_var_for_stan)
```

Create empty list that will store the trial simulators for the forthcoming models.

```{r}
sim_trial_list = list()
fit_trial_list = list()
```

# Testing parameter recovery

## Model 1: Simplest

I had started with model 2b but was confused about how the prob of states was being calculated in each step 

I'm wondering if there is a problem with implementing the integration of the sampled value when computing the probability of being in the next state

Going back to the simpler model.

```{r}
source(paste0(helpers_path, 'ddmSims/ddm_model1.R'))
```

### Single trial 

Compare likelihoods for four trials generated with different `d` and `sigma` combinations.

```{r}
100 %>% 
  rerun() %>%
  map_df(~data.frame(sim_trial(d = .04, sigma = .02, EVLeft = .7, EVRight = .2, QVLeft = .7, QVRight = .2, probFractalDraw = .3))) %>%
  summarise(meanRT = mean(reactionTime),
            numLeft = sum(choice == "left"))
```

```{r}
100 %>% 
  rerun() %>%
  map_df(~data.frame(sim_trial(d = .02, sigma = .02, EVLeft = .7, EVRight = .2, QVLeft = .7, QVRight = .2, probFractalDraw = .3))) %>%
  summarise(meanRT = mean(reactionTime),
            numLeft = sum(choice == "left"))
```

```{r}
100 %>% 
  rerun() %>%
  map_df(~data.frame(sim_trial(d = .02, sigma = .04, EVLeft = .7, EVRight = .2, QVLeft = .7, QVRight = .2, probFractalDraw = .3))) %>%
  summarise(meanRT = mean(reactionTime),
            numLeft = sum(choice == "left"))
```

```{r}
100 %>% 
  rerun() %>%
  map_df(~data.frame(sim_trial(d = .04, sigma = .04, EVLeft = .7, EVRight = .2, QVLeft = .7, QVRight = .2, probFractalDraw = .3))) %>%
  summarise(meanRT = mean(reactionTime),
            numLeft = sum(choice == "left"))
```

Is the likelihood higher for the correct parameter combination?

Trial1 & Trial 4

```{r}
fit_trial(d = .04, sigma = .02, EVLeft = .7, EVRight = .2, QVLeft = .7, QVRight = .2, probFractalDraw = .3, choice="left", reactionTime = .52)
```

```{r}
fit_trial(d = .02, sigma = .02, EVLeft = .7, EVRight = .2, QVLeft = .7, QVRight = .2, probFractalDraw = .3, choice="left", reactionTime = .52)
```

```{r}
fit_trial(d = .02, sigma = .04, EVLeft = .7, EVRight = .2, QVLeft = .7, QVRight = .2, probFractalDraw = .3, choice="left", reactionTime = .52)
```

```{r}
fit_trial(d = .04, sigma = .04, EVLeft = .7, EVRight = .2, QVLeft = .7, QVRight = .2, probFractalDraw = .3, choice="left", reactionTime = .52)
```

Trial2 & Trial 3

```{r}
fit_trial(d = .04, sigma = .02, EVLeft = .7, EVRight = .2, QVLeft = .7, QVRight = .2, probFractalDraw = .3, choice="left", reactionTime = 1.03)
```

```{r}
fit_trial(d = .02, sigma = .02, EVLeft = .7, EVRight = .2, QVLeft = .7, QVRight = .2, probFractalDraw = .3, choice="left", reactionTime = 1.03)
```

```{r}
fit_trial(d = .02, sigma = .04, EVLeft = .7, EVRight = .2, QVLeft = .7, QVRight = .2, probFractalDraw = .3, choice="left", reactionTime = 1.03)
```

```{r}
fit_trial(d = .04, sigma = .04, EVLeft = .7, EVRight = .2, QVLeft = .7, QVRight = .2, probFractalDraw = .3, choice="left", reactionTime = 1.03)
```
**For all trial types (generated by different parameter combinations) the likelihood is always higher for larger sigma's and larger d's with sigma having a larger effect.**

### Whole task

Is this true at whole task level true?

```{r}
sim_trial_list[['model1']] = sim_trial
fit_trial_list[['model1']] = fit_trial
```

Filter one subject's data to get set of stimuli.

```{r}
sub_data = clean_beh_data %>%
  filter(subnum  %in% c("19")) %>%
  select(leftQValue, rightQValue, leftLotteryEV, rightLotteryEV, probFractalDraw, reactionTime, choiceLeft, subnum) %>%
  rename(EVLeft = leftLotteryEV, EVRight = rightLotteryEV, QVLeft = leftQValue, QVRight = rightQValue)
```

Use the trials from that subject's data to simulate data (not fitting anything to real data yet to see if recovery with known true parameters is possible). Note model1 has time outs so simulated data can have different number of rows depending on the parameters.

```{r}
stim1 = sim_task(sub_data, model_name = "model1", d = .04, sigma = .02) %>%drop_na()
stim2 = sim_task(sub_data, model_name = "model1", d = .02, sigma = .02) %>%drop_na()
stim3 = sim_task(sub_data, model_name = "model1", d = .02, sigma = .04) %>%drop_na()
stim4 = sim_task(sub_data, model_name = "model1", d = .04, sigma = .04) %>%drop_na()
```

Do these parameters generate reasonable data?

```{r eval=FALSE}
sim_sanity_checks(stim4, compare_logits = TRUE)
```

True pars d = .04, sigma =.02

```{r}
stim1_fit1 = fit_task(stim1, model_name = "model1", pars_ = list(d=.04, sigma = .02))
stim1_fit2 = fit_task(stim1, model_name = "model1", pars_ = list(d=.02, sigma = .02))
stim1_fit3 = fit_task(stim1, model_name = "model1", pars_ = list(d=.02, sigma = .04))
stim1_fit4 = fit_task(stim1, model_name = "model1", pars_ = list(d=.04, sigma = .04))
```

What has the smallest neg log likelihood?

```{r}
print("True pars d = .04, sigma =.02")
paste0("Fit pars d = .04, sigma =.02, nll = ", round(sum(-log(stim1_fit1$likelihood+1e-200)), 2))
paste0("Fit pars d = .02, sigma =.02, nll = ", round(sum(-log(stim1_fit2$likelihood+1e-200)), 2))
paste0("Fit pars d = .02, sigma =.04, nll = ", round(sum(-log(stim1_fit3$likelihood+1e-200)), 2))
paste0("Fit pars d = .04, sigma =.04, nll = ", round(sum(-log(stim1_fit4$likelihood+1e-200)), 2))
```

True pars d = .02, sigma =.02

```{r}
stim2_fit1 = fit_task(stim2, model_name = "model1", pars_ = list(d=.04, sigma = .02))
stim2_fit2 = fit_task(stim2, model_name = "model1", pars_ = list(d=.02, sigma = .02))
stim2_fit3 = fit_task(stim2, model_name = "model1", pars_ = list(d=.02, sigma = .04))
stim2_fit4 = fit_task(stim2, model_name = "model1", pars_ = list(d=.04, sigma = .04))
```

What has the smallest neg log likelihood?

```{r}
print("True pars d = .02, sigma =.02")
paste0("Fit pars d = .04, sigma =.02, nll = ", round(sum(-log(stim2_fit1$likelihood+1e-200)), 2))
paste0("Fit pars d = .02, sigma =.02, nll = ", round(sum(-log(stim2_fit2$likelihood+1e-200)), 2))
paste0("Fit pars d = .02, sigma =.04, nll = ", round(sum(-log(stim2_fit3$likelihood+1e-200)), 2))
paste0("Fit pars d = .04, sigma =.04, nll = ", round(sum(-log(stim2_fit4$likelihood+1e-200)), 2))
```

True pars d = .02, sigma =.04

```{r}
stim3_fit1 = fit_task(stim3, model_name = "model1", pars_ = list(d=.04, sigma = .02))
stim3_fit2 = fit_task(stim3, model_name = "model1", pars_ = list(d=.02, sigma = .02))
stim3_fit3 = fit_task(stim3, model_name = "model1", pars_ = list(d=.02, sigma = .04))
stim3_fit4 = fit_task(stim3, model_name = "model1", pars_ = list(d=.04, sigma = .04))
```

What has the smallest neg log likelihood?

```{r}
print("True pars d = .02, sigma =.04")
paste0("Fit pars d = .04, sigma =.02, nll = ", round(sum(-log(stim3_fit1$likelihood+1e-200)), 2))
paste0("Fit pars d = .02, sigma =.02, nll = ", round(sum(-log(stim3_fit2$likelihood+1e-200)), 2))
paste0("Fit pars d = .02, sigma =.04, nll = ", round(sum(-log(stim3_fit3$likelihood+1e-200)), 2))
paste0("Fit pars d = .04, sigma =.04, nll = ", round(sum(-log(stim3_fit4$likelihood+1e-200)), 2))
```

True pars d = .04, sigma =.04

```{r}
stim4_fit1 = fit_task(stim4, model_name = "model1", pars_ = list(d=.04, sigma = .02))
stim4_fit2 = fit_task(stim4, model_name = "model1", pars_ = list(d=.02, sigma = .02))
stim4_fit3 = fit_task(stim4, model_name = "model1", pars_ = list(d=.02, sigma = .04))
stim4_fit4 = fit_task(stim4, model_name = "model1", pars_ = list(d=.04, sigma = .04))
```

What has the smallest neg log likelihood?

```{r}
print("True pars d = .02, sigma =.04")
paste0("Fit pars d = .04, sigma =.02, nll = ", round(sum(-log(stim4_fit1$likelihood+1e-200)), 2))
paste0("Fit pars d = .02, sigma =.02, nll = ", round(sum(-log(stim4_fit2$likelihood+1e-200)), 2))
paste0("Fit pars d = .02, sigma =.04, nll = ", round(sum(-log(stim4_fit3$likelihood+1e-200)), 2))
paste0("Fit pars d = .04, sigma =.04, nll = ", round(sum(-log(stim4_fit4$likelihood+1e-200)), 2))
```

## Drivers of the tradeoff

What in `fit_trial` is different for these two parameter combinations?

`changeMatrix`, `changeUp`, `changeDown` are the same for both. 

What's different is the mu and sigma when calculating `dnorm(changeMatrix, mu, sigma)` and `pnorm(changeUp[,nextTime], mu, sigma)` which is then reflected in `prStates`, `probUpCrossing` and `probDownCrossing`

```{r}
fit_trial(d = .04, sigma = .02, EVLeft = .7, EVRight = .2, QVLeft = .7, QVRight = .2, probFractalDraw = .3, choice="left", reactionTime = .6, debug=TRUE)
```

```{r}
fit_trial(d = .02, sigma = .02, EVLeft = .7, EVRight = .2, QVLeft = .7, QVRight = .2, probFractalDraw = .3, choice="left", reactionTime = .6, debug=TRUE)
```

```{r}
fit_trial(d = .02, sigma = .04, EVLeft = .7, EVRight = .2, QVLeft = .7, QVRight = .2, probFractalDraw = .3, choice="left", reactionTime = .6, debug=TRUE)
```

```{r}
fit_trial(d = .04, sigma = .04, EVLeft = .7, EVRight = .2, QVLeft = .7, QVRight = .2, probFractalDraw = .3, choice="left", reactionTime = .6, debug=TRUE)
```

`prStates` for the starting position declines a lot faster when sigma is large.

How do `dnorm(changeMatrix, mu, sigma)` and `pnorm(changeUp[,nextTime], mu, sigma)` change depending on mu and sigma?

```{r}
reactionTime = 600
timeStep = 10
barrier = 1
bias = 0
approxStateStep = .1

numTimeSteps = round(reactionTime / timeStep)
  
initialBarrier = barrier
barrier = rep(initialBarrier, numTimeSteps)

# Obtain correct state step.
halfNumStateBins = round(initialBarrier / approxStateStep)
stateStep = initialBarrier / (halfNumStateBins + 0.5)

# The vertical axis is divided into states.
states = seq(-1*(initialBarrier) + (stateStep / 2), initialBarrier - (stateStep / 2), stateStep)

# Find the state corresponding to the bias parameter.
biasState = which.min(abs(states - bias))

# Initial probability for all states is zero, except the bias state,
# for which the initial probability is one.
prStates = matrix(data = 0, nrow = length(states), ncol = numTimeSteps)
prStates[biasState,1] = 1

# How much change is required from each state to move onto every other state
changeMatrix = matrix(data = states, ncol=length(states), nrow=length(states), byrow=FALSE) - matrix(data = states, ncol=length(states), nrow=length(states), byrow=TRUE)

# How much change is required from each state to cross the up or down barrier at each time point
changeUp = matrix(data = barrier, ncol=numTimeSteps, nrow=length(states), byrow=TRUE) - matrix(data = states, ncol=numTimeSteps, nrow=length(states), byrow=FALSE)
changeDown = matrix(data = -barrier, ncol=numTimeSteps, nrow=length(states), byrow=TRUE) - matrix(data = states, ncol=numTimeSteps, nrow=length(states), byrow=FALSE)
```

What are the RDV states?

```{r}
states
```

How much change in required to get to each state from the starting state of 0.

```{r}
changeMatrix[,11]
```

How much change in required to cross the upper bound from each state at each time step?

```{r}
changeUp[,1]
```

How much change in required to cross the lower bound at each time step?

```{r}
changeDown[,1]
```

What do the normal distributions, of which the PDF and CDF will be used to compute probabilities during fitting, look like?

```{r}
data.frame(vals=rnorm(2500, .02, .02)) %>%
  mutate(dist = 'mu=.02, d= .04, sigma=.02') %>%
  rbind(data.frame(vals=rnorm(2500, .01, .02)) %>%
          mutate(dist = 'mu=.01, d= .02, sigma=.02')) %>%
  rbind(data.frame(vals=rnorm(2500, .01, .04)) %>%
          mutate(dist = 'mu=.01, d= .02, sigma=.04')) %>%
  rbind(data.frame(vals=rnorm(2500, .02, .04)) %>%
          mutate(dist = 'mu=.02, d= .04,, sigma=.04')) %>%
  ggplot(aes(vals))+
  # geom_histogram(position="identity", alpha=.5, bins=30)+
  geom_density()+
  facet_wrap(~dist)
```
Probability of transition from starting state to every other state is given by the height prob density for the distribution from which the RDV increment is sampled (i.e. the density of observing a change of size x)

```{r}
mu = .02
sigma = .02
print("Transition probability densities from the starting state")
round(dnorm(changeMatrix[,11], mu, sigma), 5)
print("1 - AOC to cross the upper bound from each state")
round(1 - pnorm(changeUp[,1], mu, sigma), 5) # all columns of changeUp and changeDown are the same without barrier decay
print("AOC to cross the lower bound from each state")
round(pnorm(changeDown[,1], mu, sigma), 5)
```

```{r}
mu = .01
sigma = .02
print("Transition probability densities from the starting state")
round(dnorm(changeMatrix[,11], mu, sigma), 5)
print("1 - AOC to cross the upper bound from each state")
round(1 - pnorm(changeUp[,1], mu, sigma), 5) # all columns of changeUp and changeDown are the same without barrier decay
print("AOC to cross the lower bound from each state")
round(pnorm(changeDown[,1], mu, sigma), 5)
```

```{r}
mu = .01
sigma = .04
print("Transition probability densities from the starting state")
round(dnorm(changeMatrix[,11], mu, sigma), 5)
print("1 - AOC to cross the upper bound from each state")
round(1 - pnorm(changeUp[,1], mu, sigma), 5) # all columns of changeUp and changeDown are the same without barrier decay
print("AOC to cross the lower bound from each state")
round(pnorm(changeDown[,1], mu, sigma), 5)
```

```{r}
mu = .02
sigma = .04
print("Transition probability densities from the starting state")
round(dnorm(changeMatrix[,11], mu, sigma), 5)
print("1 - AOC to cross the upper bound from each state")
round(1 - pnorm(changeUp[,1], mu, sigma), 5) # all columns of changeUp and changeDown are the same without barrier decay
print("AOC to cross the lower bound from each state")
round(pnorm(changeDown[,1], mu, sigma), 5)
```




## Built-in optimizer

This does work (checked with fewer iterations) but very slowly. **But I currently don't know if it gets to estimates that are any good.**

Still, I could get subject estimates running this with an appropriate number of iterations on a cluster. Note that locally I did this with 7 cores per subject so would need to make sure there are sufficient cores in the AWS instance type. **How long would this take per subject on a single AWS instance with sufficient resources (make sure to try it with data you know the true parameters for)?**

But especially with this number of subjects and given that the research question is not one of individual differences the more important question here will be to do a model comparison. **What do I need for the model comparison?**

Note also that in simulations timeouts are possible. The decisions are not entirely random for these trials but **a large number of timeout trials could influence the optimizers ability to recover parameters.**

```{r}
true_dat = sim_task(sub_data, model_name = "model2b", d=0.02, sigma = 0.007)
sum(true_dat$timeOut)/nrow(true_dat)
```

```{r}
optim_out = optim(c(.01, .01), get_task_nll, data=true_dat, par_names = c("d", "sigma"), model_name="model2b", control = list(maxit=25))
```

Convergence 1 means max it was reached.

```{r}
optim_out
```

```{r}
pars = list(d=0.02, sigma = 0.007)
fit1_data = fit_task(true_dat, model_name = "model2b", pars_ = pars)
```

```{r}
unique(fit1_data$likelihood)
```

```{r}
fit1_data %>%
  ggplot(aes(-log(likelihood+1e-20))) +
  geom_histogram()
```

# Stop clusters

```{r}
parallel::stopCluster(cl = my.fit.cluster)
parallel::stopCluster(cl = my.sim.cluster)
```